Title,Company,Location,Posted Date,Job Type,Description,Qualifications,Responsibilities,Tools/Software
GCP Data Engineer/Architect,"GSPANN Technologies, Inc","GSPANN Technologies, Inc • San Francisco, CA •  via LinkedIn",18 hours ago,Contractor,"ique consultancy with the capabilities of a large IT services firm.GCP Data Architect / EngineerLocation-San Francisco, CA (East Bay Area or Bay Area)Job Type-Long Term ContractWork type: 3 days on-siteSkills required:• Expert level of understanding of Google Cloud Data & associated services• Hands on programming expertise in SQL, Python & Spark• Design and implementation of end-to-end cloud based data platforms• Expert in data warehousing and analytics concepts, architecture and patterns• Good understanding of data modeling techniques and principlesWorking at GSPANNGSPANN is a diverse, prosperous, and rewarding place to work. We provide competitive benefits, educational assistance, and career growth opportunities to our employees. Every employee is valued for their talent and contribution. Working with us will give you an opportunity to work globally with some of the best brands in the industry.The company does and will take affirmative action to employ and advance in the employment of individuals with disabilities and protected veterans and to treat qualified individuals without discrimination based on their physical or mental disability status. GSPANN is an equal opportunity employer for minorities/females/veterans/disabled.","Expert level of understanding of Google Cloud Data & associated services; Hands on programming expertise in SQL, Python & Spark; Design and implementation of end-to-end cloud based data platforms; Expert in data warehousing and analytics concepts, architecture and patterns; Good understanding of data modeling techniques and principles; Working at GSPANN; 3 more items(s)","We provide competitive benefits, educational assistance, and career growth opportunities to our employees","Google Cloud Data, SQL, Python, Spark"
Data Engineer - Databricks,Women Impact Tech,"Women Impact Tech • Sunnyvale, CA •  via LinkedIn",24 hours ago,Full-time,"tems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our data architecture to support our next generation of products and data initiatives.Responsibilities"" Create and maintain optimal data pipeline architecture for data intensive applications."" Assemble large, complex data sets that meet functional / non-functional business requirements."" Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc."" Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure SQL, Cosmo DB, Databricks and other legacy databases."" Build analytics Dashboard/Visualizations utilizing the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics."" Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs."" Keep our data separated and secure across national boundaries through multiple data centers and Azure regions."" Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader."" Work with data and analytics experts to strive for greater functionality in our data systems.Qualifications For Data Engineer"" Strong python programming skills, expert level on using Python to process Big Data;"" Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases."" Extensive Experience on Databricks on Azure Cloud platform, deep understanding on Delta lake, Lake House Architecture."" Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement."" Strong analytic skills related to working with Data Visualization Dashboard, Metrics and etc, experience on Tableau, Power BI or Looker tools;"" Build processes supporting data transformation, data structures, metadata, dependency and workload management."" A successful history of manipulating, processing and extracting value from large disconnected datasets."" Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores."" Familiar with Deployment tool like Docker and building CI/CD pipelines."" Experience supporting and working with cross-functional teams in a dynamic environment."" 8+ years' experience in software development, Data engineering, and"" Bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field. Postgraduate/master's degree is preferred.What is the makeup of the team?Team of 8, supporting Sam's Cash application operations, monitoring and reporting tasks.Team has a mixed of Backend Engineers, Site Reliability Engineers and Data Engineers.Top 3 Skills Needed: (Need a Write-Up)Python, Advanced SQL and DatabricksAdditional Job DetailsPreferred Locations: Sunnyvale, Bentonville, or Dallas.Must be strong and hands-on on Python and Databricks","They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products; The right candidate will be excited by the prospect of optimizing or even re-designing our data architecture to support our next generation of products and data initiatives; Qualifications For Data Engineer; "" Strong python programming skills, expert level on using Python to process Big Data;; "" Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases; "" Extensive Experience on Databricks on Azure Cloud platform, deep understanding on Delta lake, Lake House Architecture; "" Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement; "" Strong analytic skills related to working with Data Visualization Dashboard, Metrics and etc, experience on Tableau, Power BI or Looker tools;; "" Build processes supporting data transformation, data structures, metadata, dependency and workload management; "" A successful history of manipulating, processing and extracting value from large disconnected datasets; "" Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores; "" Familiar with Deployment tool like Docker and building CI/CD pipelines; "" Experience supporting and working with cross-functional teams in a dynamic environment; "" 8+ years' experience in software development, Data engineering, and; "" Bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field; Top 3 Skills Needed: (Need a Write-Up); Python, Advanced SQL and Databricks; Must be strong and hands-on on Python and Databricks; 15 more items(s)","The role will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams; The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up; The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects; "" Create and maintain optimal data pipeline architecture for data intensive applications; "" Assemble large, complex data sets that meet functional / non-functional business requirements; "" Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc; "" Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure SQL, Cosmo DB, Databricks and other legacy databases; "" Build analytics Dashboard/Visualizations utilizing the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics; "" Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs; "" Keep our data separated and secure across national boundaries through multiple data centers and Azure regions; "" Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader; "" Work with data and analytics experts to strive for greater functionality in our data systems; Team of 8, supporting Sam's Cash application operations, monitoring and reporting tasks; Team has a mixed of Backend Engineers, Site Reliability Engineers and Data Engineers; 11 more items(s)","Azure SQL, Cosmo DB, Databricks, Python, SQL, Tableau, Power BI, Looker, Docker"
Data Engineer- Data Bricks- Hybrid CA,MasterClass,"MasterClass • San Francisco, CA •  via Greenhouse",20 days ago,Full-time,"uding Arts & Entertainment, Business, Design & Style, Sports & Gaming, Writing and more. Step into Nas’ recording studio and Gordon Ramsay’s kitchen, and go behind the big screen with James Cameron. Design your career with Elaine Welteroth, get ready to win with Lewis Hamilton, perfect your pitch with Shonda Rhimes and discover your inner negotiator with Chris Voss.It's a pivotal time for MasterClass – and we want you to be a part of the journey. With offices in San Francisco (HQ) and Waterloo, Ontario plus a studio in Los Angeles, we are looking to expand our team to support the business.If you want to help impact our members' lives – we want to hear from you!Snapshot of the Role:At MasterClass, data is pivotal to our decision-making processes, influencing business strategies, product development, content creation, and operational efficiency. Our expanding Data teams are central to the company, collaborating across various departments to drive decisions and steer future growth at MasterClass. Our Data engineering team tackles challenging problems across many technical disciplines, including data warehouse, batch compute infrastructure, data orchestration systems, critical integrations (and more.)We seek an exceptional Data Engineer to help design, build, and operate our data platform to scale the business and enable the Data organization and teams to solve challenges.This role is a remote position with periodic in-person touchpoints (HQ in San Francisco, CA)What You Will Do:• Design, build, and manage our data warehouse and data ingestion solutions.• Understand and translate business needs into data models to support long-term, scalable, and reliable data pipelines.• Enhance and maintain Data Infrastructure using best practices and latest features to ensure high data quality.• Define and manage SLAs for data sets and processes running in production• Continuously improve our data infrastructure and empower teams with the best data tooling and systems.• Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers, and Software Engineers to understand and deliver on data needs.• Be part of a data engineering team that is also responsible for the reliability of the data systems that are built and be available to respond to critical incidents as needed.Requirements:4 years of experience in Data Engineering and Data Warehousing• Bachelor's degree in a quantitative field, e.g., Computer Science, Math, Physics• Experience scaling data environments with distributed processing technologies• Advanced proficiency with SQL, Python, Postgres, and integrations via APIs• Experience working in AWS cloud environment in designing and implementing a cloud data warehouse, other types of storage, and developing ETL/ELT pipelines• Experience integrating and building data platforms in support of BI, Analytics, and Data Science• Strong communication skills, with the ability to initiate and drive projects proactively and accurately• Eligible to work in the United States legallyAt MasterClass, we believe we put our best work forward when our employees bring together ideas that are diverse in thought. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law. In addition, MasterClass will provide reasonable accommodations for qualified individuals with disabilities. If you have a disability or special need, we would like to know how we can better accommodate you. #LI-LM1","4 years of experience in Data Engineering and Data Warehousing; Bachelor's degree in a quantitative field, e.g., Computer Science, Math, Physics; Experience scaling data environments with distributed processing technologies; Advanced proficiency with SQL, Python, Postgres, and integrations via APIs; Experience working in AWS cloud environment in designing and implementing a cloud data warehouse, other types of storage, and developing ETL/ELT pipelines; Experience integrating and building data platforms in support of BI, Analytics, and Data Science; Strong communication skills, with the ability to initiate and drive projects proactively and accurately; Eligible to work in the United States legally; 5 more items(s)","At MasterClass, data is pivotal to our decision-making processes, influencing business strategies, product development, content creation, and operational efficiency; Design, build, and manage our data warehouse and data ingestion solutions; Understand and translate business needs into data models to support long-term, scalable, and reliable data pipelines; Enhance and maintain Data Infrastructure using best practices and latest features to ensure high data quality; Define and manage SLAs for data sets and processes running in production; Continuously improve our data infrastructure and empower teams with the best data tooling and systems; Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers, and Software Engineers to understand and deliver on data needs; Be part of a data engineering team that is also responsible for the reliability of the data systems that are built and be available to respond to critical incidents as needed; 5 more items(s)","SQL, Python, Postgres, AWS"
Sr. Data Engineer - Remote,Zortech Solutions,"Zortech Solutions • Mountain View, CA •  via ZipRecruiter",19 hours ago,Full-time,Role: Data Engineer-Data BrickLocation: Mountain View CA (Hybrid 2-3 days form officeDuration: 6+ MonthsJob Description:• Data engineer with following tech stack• Experience on Data Brick• Experience on building Data pipeline• Knowledge on AWS Lambda (Must have)• Knowledge on Kafka ( Must have)• understanding on Apache Spark or Hadoop (Nice to have)• Basic of Python(nice to have),Data engineer with following tech stack; Experience on Data Brick; Experience on building Data pipeline; Knowledge on AWS Lambda (Must have); Knowledge on Kafka ( Must have); understanding on Apache Spark or Hadoop (Nice to have); 3 more items(s),,"Data Brick, AWS Lambda, Kafka, Apache Spark, Hadoop, Python"
"Software Engineer, Data Acquisition",First American Financial Corporation,"First American Financial Corporation • San Francisco, CA •  via First American Financial Corporation - First American Title",17 days ago,Full-time,"n environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For® list for nine consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com.What We DoThe Data Engineer is an expert in data technologies and plays a key role in the development and deployment of innovative data and analytics solutions. Directly supports the Business Intelligence & Analytics team with regard to data architecture, data pipelines, and data modeling. Works with functional teams and supports data analysts, report developers, and data scientists on data initiatives. Ensures that optimal data delivery architecture is consistent throughout ongoing projects.HOW YOU’LL CONTRIBUTE• Viewed as a data expert; drives innovation and plays a key role in the department. Participates in highly visible initiatives that have broad impact.• Identify, design, and implement internal process improvements: automate manual processes, optimize data delivery, re-design architecture for greater scalability.• Design, update, and maintain data architecture including data warehouses, lakehouses, and semantic models.• Create data pipelines to acquire, move, and transform data to support analytics initiatives.• Serve as a subject matter expert on data architecture and make innovative recommendations including best practices and how to improve data reliability, efficiency, and quality.• Serve as a mentor for other members of the team with regard to the data engineering domain.• Troubleshoot and resolve a wide range of data issues.• Required to perform duties outside of normal work hours based on business needs.WHAT YOU’LL BRING• Bachelor's degree in Computer Science, Information Systems, Informatics, or related field or equivalent combination of education and experience.• 5-8 years of directly related experience.• Working experience building data pipelines (ETL & ELT) and data warehouses.• Demonstrated expertise in data modeling, database maintenance, monitoring and performance tuning on SQL Server and NoSQL database.• Working knowledge/proficient with Microsoft data and analytics stack – ideally Fabric as a whole with a focus on SQL Server, Data Factory, Synapse, and Analysis Services / semantic models.• Exceptional analytical skills analyzing large and complex data sets.• Perform thorough testing and data validation to ensure the accuracy and efficiency of data transformations.• Strong written and verbal communication skills, with precise documentation.• Self-driven team player with ability to work independently and multi-task.• Analytical, creative thinker and innovative problem solver.• Experience working in Agile SDLC methodology• Ideally, experience with the process of deploying machine learning models in a production environment.• Proficient with SQL and T-SQL, DAX or python (specifically PySpark)SALARY RANGE:$99,675 - $166,100This hiring range is a reasonable estimate of the base pay range for this position at the time of posting. Pay is based on a number of factors which may include job-related knowledge, skills, experience, business requirements and geographic location.What We OfferBy choice, we don’t simply accept individuality – we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it’s the right thing to do, but also because it’s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term.Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.","Bachelor's degree in Computer Science, Information Systems, Informatics, or related field or equivalent combination of education and experience; 5-8 years of directly related experience; Working experience building data pipelines (ETL & ELT) and data warehouses; Demonstrated expertise in data modeling, database maintenance, monitoring and performance tuning on SQL Server and NoSQL database; Working knowledge/proficient with Microsoft data and analytics stack – ideally Fabric as a whole with a focus on SQL Server, Data Factory, Synapse, and Analysis Services / semantic models; Exceptional analytical skills analyzing large and complex data sets; Perform thorough testing and data validation to ensure the accuracy and efficiency of data transformations; Strong written and verbal communication skills, with precise documentation; Self-driven team player with ability to work independently and multi-task; Analytical, creative thinker and innovative problem solver; Experience working in Agile SDLC methodology; Ideally, experience with the process of deploying machine learning models in a production environment; Proficient with SQL and T-SQL, DAX or python (specifically PySpark); 10 more items(s)","$99,675 - $166,100; This hiring range is a reasonable estimate of the base pay range for this position at the time of posting; Pay is based on a number of factors which may include job-related knowledge, skills, experience, business requirements and geographic location; Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan; 1 more items(s)","SQL Server, NoSQL database, Microsoft Data Factory, Synapse, Analysis Services, SQL, T-SQL, DAX, Python, PySpark"
Principal Data Software Engineer,OpenAI,"OpenAI • San Francisco, CA •  via ZipRecruiter",2 days ago,310K–385K a year,"ities:• Own and lead engineering projects in the area of data acquisition including web crawling, data ingestion, and search.• Collaborate with other sub-teams, such as Data Processing, Architecture, and Scaling, to ensure smooth data flow and system operability.• Work closely with the legal team to handle any compliance or data privacy-related matters.• Develop and deploy highly scalable distributed systems capable of handling petabytes of data.• Architect and implement algorithms for data indexing and search capabilities.• Build and maintain backend services for data storage, including work with key-value databases and synchronization.• Deploy solutions in a Kubernetes Infrastructure-as-Code environment and perform routine system checks.• Conduct and analyze experiments on data to provide insights into system performance.Qualifications:• BS/MS/PhD in Computer Science or a related field.• 5+ years of industry experience in software development.• Experience with large web crawlers a plus• Strong expertise in large stateful distributed systems and data processing.• Proficiency in Kubernetes, and Infrastructure-as-Code concepts.• Willingness and enthusiasm for trying new approaches and technologies.• Ability to handle multiple tasks and adapt to changing priorities.• Strong communication skills, both written and verbal.About OpenAIOpenAI is an AI research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. We push the boundaries of the capabilities of AI systems and seek to safely deploy them to the world through our products. AI is an extremely powerful tool that must be created with safety and human needs at its core, and to achieve our mission, we must encompass and value the many different perspectives, voices, and experiences that form the full spectrum of humanity.We are an equal opportunity employer and do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, veteran status, disability or any other legally protected status.OpenAI Affirmative Action and Equal Employment Opportunity Policy StatementFor US Based Candidates: Pursuant to the San Francisco Fair Chance Ordinance, we will consider qualified applicants with arrest and conviction records.We are committed to providing reasonable accommodations to applicants with disabilities, and requests can be made via this link.OpenAI Global Applicant Privacy PolicyAt OpenAI, we believe artificial intelligence has the potential to help people solve immense global challenges, and we want the upside of AI to be widely shared. Join us in shaping the future of technology.","BS/MS/PhD in Computer Science or a related field; 5+ years of industry experience in software development; Strong expertise in large stateful distributed systems and data processing; Proficiency in Kubernetes, and Infrastructure-as-Code concepts; Willingness and enthusiasm for trying new approaches and technologies; Ability to handle multiple tasks and adapt to changing priorities; Strong communication skills, both written and verbal; 4 more items(s)","Own and lead engineering projects in the area of data acquisition including web crawling, data ingestion, and search; Collaborate with other sub-teams, such as Data Processing, Architecture, and Scaling, to ensure smooth data flow and system operability; Work closely with the legal team to handle any compliance or data privacy-related matters; Develop and deploy highly scalable distributed systems capable of handling petabytes of data; Architect and implement algorithms for data indexing and search capabilities; Build and maintain backend services for data storage, including work with key-value databases and synchronization; Deploy solutions in a Kubernetes Infrastructure-as-Code environment and perform routine system checks; Conduct and analyze experiments on data to provide insights into system performance; 5 more items(s)","Kubernetes, Infrastructure-as-Code"
Senior Staff Software Engineer (Data)  - Activision Blizzard Media,Match Group,"Match Group • San Francisco, CA •  via Taro",Full-time,No Degree Mentioned,"Principal Data Software Engineer position at Tinder, a Match Group company, for experienced professionals in data engineering.",,,The job description provided does not mention any specific tools or software names.
DELTA DENTAL: Data Engineer,King,"King • San Francisco, CA •  via Careers At King",18 hours ago,Full-time and Part-time,"Overwatch® Leagues, and some of the top PC and console gaming such as World of Warcraft®, Call of Duty®, and StarCraft®. The idea is simple: great game experiences offer great marketing experiences.We are seeking a Senior Staff Data Engineer in our Ads Engineering team, crafting ground breaking mobile advertising technologies to build the next generation of our Ads Platform.Responsibilities• Design and build Data Pipelines to collect, transform, store, analyze, explore and visualize.• Craft and build cloud-based data lakes and data warehouses.• Work with the product team to understand data sources, use cases and data models.• Choose the right technology stack to align with those use cases with scalability.• Collaborate with the other team members across different teams.• Develop alert & monitoring systems and procedures for faster disaster recovery.• Build and contribute to data products with focus on data Quality and data Governance.Leadership and Communication• Proven ability to lead and mentor engineers in a collaborative environment.• Excellent at articulating complex technical concepts to diverse stakeholders.• Experienced in driving technical strategy and making impactful decisions.Qualifications• BA/BS degree in Computer Science, similar technical field of study or equivalent practical experience.• Strong experience in Software Development in Java or Python.• Working experience in building data warehouses and data lakes.• 8+ years of demonstrated ability working with relational databases such as MySQL, Postgres etc.• 4+ years of experience in NoSQL databases like Bigtable, Cassandra, HBase etc.• Expert level SQL skills and database performance concepts.• Extensive experience in relational and dimensional data modeling.• Experience with developing extract-transform-load (ETL).• Understanding of distributed computing frameworks like Apache Spark and Flink.• Should be able to communicate and articulate their thought process optimally with team members.Preferred Qualifications• Experience building data warehouse, data lake and data pipeline using Google Cloud Platform (GCP).• Experience with large scale distributed systems with large datasets.• Experience in distributed computing framework – Hadoop, Spark, Flink or Storm.• Experience with messaging systems like Kafka and RabbitMQ.• Knowledge of advertising platforms.About Activision Blizzard MediaActivision Blizzard Media is the gateway for brands to the #1 cross-platform gaming company in the western world, with hundreds of millions of players across over 190 countries. Our legendary portfolio includes iconic mobile game franchises such as Candy Crush™, esports opportunities like the Call of Duty® and Overwatch® Leagues, and some of the top PC and console gaming franchises such as World of Warcraft®, Call of Duty®, and StarCraft®. The idea is simple: great game experiences offer great marketing experiences.A Great Saga Needs All Sorts of HeroesKing strives to be a place where everyone can be their most authentic self. We recognize that diversity, equity and inclusion is a vital and continuous conversation, and that change only happens when we all come together. It’s our mission to build a diverse and inclusive Kingdom for our people, players, and community.Making the World PlayfulMaking the World Playful is our mission – it’s the thread that connects our people, our players, and our passion for our games. Let’s face it, who doesn’t love a bit of fun?Kingsters are seriously playful: creative thinkers who balance art and science to bring moments of magic to millions daily. But our players aren’t the only ones that can level-up. We’re always looking for ways to champion each other and make what’s already great, even better.So, if this feels like a fun way to spend your days, and you share our passion, our values, and our hunger to shape the future, join us in Making the World Playful.Applications needs to be in English.Discover King at careers.king.comRewardsWe provide a suite of benefits that promote physical, emotional and financial well-being for ‘Every World’ - we’ve got our employees covered! Subject to eligibility requirements, the Company offers comprehensive benefits including:• Medical, dental, vision, health savings account or health reimbursement account, healthcare spending accounts, dependent care spending accounts, life and AD&D insurance, disability insurance;• 401(k) with Company match, tuition reimbursement, charitable donation matching;• Paid holidays and vacation, paid sick time, floating holidays, compassion and bereavement leaves, parental leave;• Mental health & wellbeing programs, fitness programs, free and discounted games, and a variety of other voluntary benefit programs like supplemental life & disability, legal service, ID protection, rental insurance, and others;• If the Company requires that you move geographic locations for the job, then you may also be eligible for relocation assistance.Eligibility to participate in these benefits may vary for part time and temporary full-time employees and interns with the Company. You can learn more by visiting https://www.benefitsforeveryworld.com/.In the U.S., the standard base pay range for this role is $155,500.00 - $287,876.00 Annual. These values reflect the expected base pay range of new hires across all U.S. locations. Ultimately, your specific range and offer will be based on several factors, including relevant experience, performance, and work location. Your Talent Professional can share this role’s range details for your local geography during the hiring process. In addition to a competitive base pay, employees in this role may be eligible for incentive compensation. Incentive compensation is not guaranteed. While we strive to provide competitive offers to successful candidates, new hire compensation is negotiable.","Excellent at articulating complex technical concepts to diverse stakeholders; Experienced in driving technical strategy and making impactful decisions; BA/BS degree in Computer Science, similar technical field of study or equivalent practical experience; Strong experience in Software Development in Java or Python; Working experience in building data warehouses and data lakes; 8+ years of demonstrated ability working with relational databases such as MySQL, Postgres etc; 4+ years of experience in NoSQL databases like Bigtable, Cassandra, HBase etc; Expert level SQL skills and database performance concepts; Extensive experience in relational and dimensional data modeling; Experience with developing extract-transform-load (ETL); Understanding of distributed computing frameworks like Apache Spark and Flink; Should be able to communicate and articulate their thought process optimally with team members; A Great Saga Needs All Sorts of Heroes; 10 more items(s)","We provide a suite of benefits that promote physical, emotional and financial well-being for ‘Every World’ - we’ve got our employees covered!; Medical, dental, vision, health savings account or health reimbursement account, healthcare spending accounts, dependent care spending accounts, life and AD&D insurance, disability insurance;; 401(k) with Company match, tuition reimbursement, charitable donation matching;; Paid holidays and vacation, paid sick time, floating holidays, compassion and bereavement leaves, parental leave;; Mental health & wellbeing programs, fitness programs, free and discounted games, and a variety of other voluntary benefit programs like supplemental life & disability, legal service, ID protection, rental insurance, and others;; If the Company requires that you move geographic locations for the job, then you may also be eligible for relocation assistance; In the U.S., the standard base pay range for this role is $155,500.00 - $287,876.00 Annual; These values reflect the expected base pay range of new hires across all U.S. locations; In addition to a competitive base pay, employees in this role may be eligible for incentive compensation; Incentive compensation is not guaranteed; 7 more items(s)","Java, Python, MySQL, Postgres, Bigtable, Cassandra, HBase, SQL, Apache Spark, Flink, Google Cloud Platform (GCP), Hadoop, Kafka, RabbitMQ"
Sr data engineer,Elevated Resources,"Elevated Resources • San Francisco, CA •  via Adzuna",Full-time,No Degree Mentioned,"governance COE, the Senior Data Engineer will design and develop interfaces between applications, databases, data assets, external partners, and third-party systems in a combination of cloud and on-premise platforms.• Develops and maintains scalable data pipelines and builds out new integrations to support continuing increases in data volume and complexity• Designs and develops scalable ETL packages for point to point integration of data between source systems, extraction and integration of data into various data assets, including data warehouse and fit for purpose data repositories, both on prem and cloud• Designs and develops scalable data APIs to provide data as a service to microservices, applications, and analytical tools• Designs and develops data migrations in support of enterprise application and system implementations from legacy systems• Writes functional specifications for data pipelines and APIs and writes and performs unit/integration tests• Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.• Assists in planning, coordinating, and executing engineering projects• Supports and collaborates with other Engineers through evaluation, design analysis, and development phases• Maintains knowledge, ensures competency and compliance with policies and procedures, in order to be the technical expert while collaborating with cross-functional teams• This list is not all-inclusive and you are expected to perform other duties as requested or assigned","The Senior Data Engineer will design, build, and implement data integration solutions, including data pipelines, data API's, and ETL jobs, to meet the data needs of applications, services, micro services, data assets, and business intelligence and analytical tools; Working with data architects, application development teams, data analytics teams, business analytics, product managers, and the data governance COE, the Senior Data Engineer will design and develop interfaces between applications, databases, data assets, external partners, and third-party systems in a combination of cloud and on-premise platforms; Develops and maintains scalable data pipelines and builds out new integrations to support continuing increases in data volume and complexity; Designs and develops scalable ETL packages for point to point integration of data between source systems, extraction and integration of data into various data assets, including data warehouse and fit for purpose data repositories, both on prem and cloud; Designs and develops scalable data APIs to provide data as a service to microservices, applications, and analytical tools; Designs and develops data migrations in support of enterprise application and system implementations from legacy systems; Writes functional specifications for data pipelines and APIs and writes and performs unit/integration tests; Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues; Assists in planning, coordinating, and executing engineering projects; Supports and collaborates with other Engineers through evaluation, design analysis, and development phases; Maintains knowledge, ensures competency and compliance with policies and procedures, in order to be the technical expert while collaborating with cross-functional teams; This list is not all-inclusive and you are expected to perform other duties as requested or assigned; 9 more items(s)",,"ETL, APIs"
Data Engineering Manager,Apex Systems,"Apex Systems • San Francisco, CA •  via Talent.com",18 hours ago,Full-time,"S Redshift and BigQuery should have cloud data warehouse experience (Snowflake also okay)• Need experience in building data pipelines for ML use cases, data plumbing work for building training data sets, evaluation, etc.• Need strong Python experience, using PySpark for ML pipelines as well as a couple other ETL tools (NOT Glue, moving away from AWS toward Google, Dataproc?)• Need experience building ETL pipelines from NoSQL databases, MongoDB acceptable, but if they have used a less mature NoSQL such as Google Filebase that would be preferred• Experience in Vector DB preferred• Advanced SQL skills• Need experience with at least 1 BI tool : Tableau, Looker, etc. PowerBI less preferredMust Haves :• Data warehousing fundamentals• At least 1 cloud-based warehouse (Redshift, BigQuery, Snowflake)• ETL (Python)• Experience in ML Pipeline engineering building data solutions for ML / AI use cases• NoSQL database experience• Advanced SQL• BI toolingEEO EmployerApex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law.Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law.If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at or 844-463-6178.Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning.We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package.Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.Last updated : 2024-09-07","Temperament is important, need to understand the messiness of data and be able to solve problems will driving the business forward; Using Postgres as a data warehouse, AWS Redshift and BigQuery should have cloud data warehouse experience (Snowflake also okay); Need experience in building data pipelines for ML use cases, data plumbing work for building training data sets, evaluation, etc; Need strong Python experience, using PySpark for ML pipelines as well as a couple other ETL tools (NOT Glue, moving away from AWS toward Google, Dataproc?); Advanced SQL skills; Need experience with at least 1 BI tool : Tableau, Looker, etc; Data warehousing fundamentals; At least 1 cloud-based warehouse (Redshift, BigQuery, Snowflake); ETL (Python); Experience in ML Pipeline engineering building data solutions for ML / AI use cases; NoSQL database experience; Advanced SQL; BI tooling; 10 more items(s)","We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package","Redshift, BigQuery, Snowflake, Python, PySpark, MongoDB, Google Filebase, Vector DB, SQL, Tableau, Looker, PowerBI, Dataproc, Postgres"
Data Scientist/Data Engineer (1042),Revinate,"Revinate • San Francisco, CA •  via Indeed",130K–220K a year,Full-time,"ze direct booking revenue. This combination maximizes the lifetime value of each guest through personalized and targeted campaigns across the guest journey. Revinate Marketing has won 1st place for Hotel CRM & Email Marketing in the HotelTechAwards five years in a row!About RevinateWe support full remote work but also maintain offices in Amsterdam, Singapore and Bend Oregon, Revinate seeks to build specialized and easy-to-adopt technology to solve these challenges. Revinate enables hoteliers to transform their guest data into revenue. With Revinate Marketing and Revinate Guest Feedback, hoteliers are empowered to make smarter decisions, resulting in increased direct revenue and guest engagement. Much like the industry we serve, we are a team of hard-working and passionate individuals who love our customers and are committed to surprising and delighting them with every new innovation and disruption.The company is backed by leading Silicon Valley investors, including Serent Capital, Benchmark Capital, Tenaya Capital, and Sozo Ventures. Headquartered in San Francisco with regional offices in Amsterdam and Singapore, Revinate counts tens of thousands of the world’s leading hotels as customers.To learn more, please visit www.revinate.comWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability statusWhat We BelieveWe believe that hoteliers deserve better. The global hotel sector is a booming $500B+ industry, yet hotels are facing many complex challenges, including increased pressure from online travel agencies and intense competition from ever-growing room inventory and the shared economy. That coupled with aging, cumbersome technology is making the job of the hotelier more difficult than ever. At Revinate, we use cutting edge technology to build powerful software for hotels to take back control and drive direct revenue. The simplicity and beautiful UX of our solutions are a breath of fresh air in an industry of old technology.Role OverviewRevinate is seeking an experienced Data Engineering Manager to lead our Pipeline team. This team is responsible for building and maintaining reactive microservices to enable a highly-available data pipeline that processes millions of events, in real-time, each day, from thousands of hotels across the globe. A passion for building a high-volume, cloud-native, distributed stream-processing pipeline is a must. Additionally, the ability to collaborate with others, regardless of title, to drive cross-team technical and process improvements is a requirement. If you enjoy coming up with thoughtful solutions to challenging problems then we may be a match.What You'll Do• Lead and coach a team of data engineers, providing them with challenges, growth opportunities, and performance feedback• Build, scale, and maintain a high-volume, cloud-native, distributed stream-processing pipeline that runs 24x7• Establish a culture of rapid iteration and frequent production deployments, without sacrificing quality• Play a hands-on role investigating issues, asking questions, and prioritizing tech debt• Promote quality and best practices through active participation in code reviews, knowledge sharing, and delivery of learning and development initiatives• Collaborate with product managers, project managers, and other engineers to understand, document, and deliver solution requirementsMonitor system performance and ensure timely recovery from incidents• Work from home most days, but meet in person (in our downtown San Francisco office) once every two weeksWhat You'll Bring• Experience as a Data Engineering Manager, growing and developing amazing engineers• Exceptional written and verbal communication skills• Ability to build relationships and influence peers and non-direct reports• Deep understanding of distributed systems, microservice architecture, and reactive design patterns• Unwavering commitment to engineering excellence and craftsmanship, setting high expectations for systems and teams• Experience with data engineering technologies (Java, Kafka, Kafka Streams, Cassandra, Spark, Elasticsearch, S3, Protocol Buffers) and supporting technologies (Docker, Kubernetes, Helm, Gradle, GitLab, AWS)• Systems thinking mindset with a strong desire to understand the entire end-to-end architecture and data flow• Ability to challenge existing designs and propose solutions for highly-scalable distributed systems while minimizing technology costs and maintenance obligationsIntuitive sense for spotting, investigating, and prioritizing data processing anomaliesBenefits• Health insurance-employee premium paid 100% by Revinate• Dental insurance-employee and dependents’ premium paid 100% by Revinate• Vision insurance-employee and dependents’ premium paid 100% by Revinate• 401(k) with employer match• Short & Long Term Disability insurance• Life insurance• Paid time off• Monthly work from home stipend• Telehealth access• Employee Assistance Program (EAP)This salary range may be inclusive of several career levels at Revinate and will be narrowed during the interview process based on a number of factors, including (but not limited to) the candidate’s experience, qualifications and location.Revinate values the flexibility of a remote workforce and the benefits of localized hiring. We focus on specific cities to foster local communities and enhance team cohesion, allowing employees to collaborate, attend local events, and build a strong sense of community and company culture.Candidates must be located in the city listed in the job application. Thank you!Revinate is not open to third party solicitation or resumes for our posted FTE positions. Resumes received from third party agencies that are unsolicited will be considered complementary.Important Security AlertWe have been made aware of fraudulent activities involving individuals impersonating our HR team and offering fake job opportunities. Please be vigilant and ensure your safety by verifying all job offers.For Authentic Opportunities: Only refer to our official careers page on our company website. Your security is our priority. If you encounter any suspicious activity, please report it immediately. Stay safe and secure! You can confirm or inquire with any questions by reaching out to recruiting@revinate.com#LI-Remote#LI-AE1Excited?! Want to learn more? Apply Now!Our Core Values:One Revinate - United & Strong, on a single mission togetherBuilt on Trust - It’s the foundation of everything we doExpect Amazing - We think, dream & deliver bigCustomer Love - When the customer wins, we winMake it Simpler - Apply it to everything we doHungerness - Feel it, follow it, be relentless about our successGrounded in Gratitude - We’re glad to be here & make the most of every dayRevinate Inc. provides Equal Employment Opportunity to all employees and applicants for employment without regard to race, color, religion, gender identity or expression, sex, sexual orientation, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. Revinate complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.Revinate is not open to third party solicitation or resumes for our posted FTE positions. Resumes received from third party agencies that are unsolicited will be considered complementary.If you are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to recruiting@revinate.com.By submitting your application you acknowledge that you have read Revinate's Privacy Policy (https://www.revinate.com/privacy/)","A passion for building a high-volume, cloud-native, distributed stream-processing pipeline is a must; Additionally, the ability to collaborate with others, regardless of title, to drive cross-team technical and process improvements is a requirement; Experience as a Data Engineering Manager, growing and developing amazing engineers; Exceptional written and verbal communication skills; Ability to build relationships and influence peers and non-direct reports; Deep understanding of distributed systems, microservice architecture, and reactive design patterns; Unwavering commitment to engineering excellence and craftsmanship, setting high expectations for systems and teams; Experience with data engineering technologies (Java, Kafka, Kafka Streams, Cassandra, Spark, Elasticsearch, S3, Protocol Buffers) and supporting technologies (Docker, Kubernetes, Helm, Gradle, GitLab, AWS); Systems thinking mindset with a strong desire to understand the entire end-to-end architecture and data flow; Ability to challenge existing designs and propose solutions for highly-scalable distributed systems while minimizing technology costs and maintenance obligations; Intuitive sense for spotting, investigating, and prioritizing data processing anomalies; 8 more items(s)","Health insurance-employee premium paid 100% by Revinate; Dental insurance-employee and dependents’ premium paid 100% by Revinate; Vision insurance-employee and dependents’ premium paid 100% by Revinate; 401(k) with employer match; Short & Long Term Disability insurance; Life insurance; Paid time off; Monthly work from home stipend; Telehealth access; Employee Assistance Program (EAP); This salary range may be inclusive of several career levels at Revinate and will be narrowed during the interview process based on a number of factors, including (but not limited to) the candidate’s experience, qualifications and location; Revinate values the flexibility of a remote workforce and the benefits of localized hiring; 9 more items(s)","Java, Kafka, Kafka Streams, Cassandra, Spark, Elasticsearch, S3, Protocol Buffers, Docker, Kubernetes, Helm, Gradle, GitLab, AWS"
Senior Data Infrastructure Engineer,City and County of San Francisco,"City and County of San Francisco • San Francisco, CA •  via SF Careers",Full-time,,"n and future growth of San Francisco’s built environment.This role will play a key role in Data Analytics Group’s effort for modernizing data and analytics systems, workflows, products, and tools that support long-range land-use planning. This effort includes developing an integrated City Information Model that improves access to and quality of data on housing production and pipeline, land use, commercial and industrial activity, socio-demographic characteristic, and other information that support planning analytics.The Data Scientist/Engineer will apply novel geospatial, statistical and machine learning methods to derive insights from a wide range of data sources including digitized records, permit systems, geospatial databases, the City’s Open Data Portal, surveys, and third-party data services. This role will develop a wide range of analytics tools for a variety of planning applications including differential policy diagnostics, recovery strategy assessments, and operational efficiency modelling. As a systems thinker and innovator, this position will work cross-functionally with data analysts, GIS analysts, data engineers, planners, urban designers, architects, and interagency stakeholders to create a tighter integration between policy and operational goals and evidence-based outcomes.RESPONSIBILITIESUnder the direction of the Data and Analytics Manager, the Data Scientist/Engineer will support planning/policy development, implementation, and monitoring, and operations, and business process improvements in the Planning Department, and will be responsible for the following:• Collaborate with other divisions/teams in the Department to identify and understand key questions and challenges in plan and policy development, implementation, and monitoring, as well as in the Departments’ operations, and business processes.• Formulate analytical solutions and products that can answer and/or provide insight about key questions and challenges and communicate those analytical methods with respective divisions/teams.• Implement and maintain analytical solutions and products:• Build data pipelines, identify data gaps and collaborate across the Department and other agencies to create/improve data systems• Build interactive maps, data visualization and dashboards that convey key insights to business audiences and help them improve the performance of policies, processes and workflows• Develop performance measurement frameworks and monitoring key metrics during policy implementation• Implement and use machine learning models and perform statistical analysis to derive data-driven actionable recommendations that can improve the performance of policies, processes, and workflows• Clearly convey analysis and modeling results to relevant stakeholders through reports and presentations• Identify new data sources and methods to enhance the Department’s core data products, analytical tools and reports","Under the direction of the Data & Analytics Manager, the Data Scientist/Engineer will develop predictive models, scenario planning analyses, data visualizations and their required data pipelines to provide decision support for key areas including public policy development, implementation, monitoring, operations, community engagement, and business process improvements that together inform the design and future growth of San Francisco’s built environment; This role will play a key role in Data Analytics Group’s effort for modernizing data and analytics systems, workflows, products, and tools that support long-range land-use planning; This effort includes developing an integrated City Information Model that improves access to and quality of data on housing production and pipeline, land use, commercial and industrial activity, socio-demographic characteristic, and other information that support planning analytics; The Data Scientist/Engineer will apply novel geospatial, statistical and machine learning methods to derive insights from a wide range of data sources including digitized records, permit systems, geospatial databases, the City’s Open Data Portal, surveys, and third-party data services; This role will develop a wide range of analytics tools for a variety of planning applications including differential policy diagnostics, recovery strategy assessments, and operational efficiency modelling; As a systems thinker and innovator, this position will work cross-functionally with data analysts, GIS analysts, data engineers, planners, urban designers, architects, and interagency stakeholders to create a tighter integration between policy and operational goals and evidence-based outcomes; Under the direction of the Data and Analytics Manager, the Data Scientist/Engineer will support planning/policy development, implementation, and monitoring, and operations, and business process improvements in the Planning Department, and will be responsible for the following:; Collaborate with other divisions/teams in the Department to identify and understand key questions and challenges in plan and policy development, implementation, and monitoring, as well as in the Departments’ operations, and business processes; Formulate analytical solutions and products that can answer and/or provide insight about key questions and challenges and communicate those analytical methods with respective divisions/teams; Implement and maintain analytical solutions and products:; Build data pipelines, identify data gaps and collaborate across the Department and other agencies to create/improve data systems; Build interactive maps, data visualization and dashboards that convey key insights to business audiences and help them improve the performance of policies, processes and workflows; Develop performance measurement frameworks and monitoring key metrics during policy implementation; Implement and use machine learning models and perform statistical analysis to derive data-driven actionable recommendations that can improve the performance of policies, processes, and workflows; Clearly convey analysis and modeling results to relevant stakeholders through reports and presentations; Identify new data sources and methods to enhance the Department’s core data products, analytical tools and reports; 13 more items(s)",,"GIS, machine learning models, data visualization, dashboards"
Software Engineer - Database Engineering,Webflow,"Webflow • San Francisco, CA •  via Greenhouse",Full-time,No Degree Mentioned,"ners and creative agencies to Fortune 500 companies, millions worldwide use Webflow to be more nimble, creative, and collaborative. It’s the web, made better.We’re excited for a Senior Data Infrastructure Engineer to join our Data Platform team. In this role, you’ll play a key part in building robust, secure, and scalable infrastructure that powers our data operations. You will have the opportunity to optimize the performance of our data services and automate infrastructure management ensuring everything runs smoothly and reliably. Your expertise will be crucial in integrating and managing essential components like Kafka, Spark, and Airflow, providing a solid foundation for our data-driven products. If you are passionate about leveraging cutting-edge technologies to make a real impact, we’d love to connect with you!About the role• Location: Remote-first (United States; BC & ON, Canada)• Full-time• Permanent• Exempt• The cash compensation for this role is tailored to align with the cost of labor in different geographic markets. We've structured the base pay ranges for this role into zones for our geographic markets, and the specific base pay within the range will be determined by the candidate’s geographic location, job-related experience, knowledge, qualifications, and skills.• United States (all figures cited below in USD and pertain to workers in the United States)• Zone A: $158,000 - $218,000• Zone B: $149,000 - $205,000• Zone C: $139,000 - $192,000• Canada (All figures cited below in CAD and pertain to workers in ON & BC, Canada)• CAD 180,000 - CAD 248,000• Please visit our Careers page for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.• Reporting to the Senior Engineering ManagerAs a Senior Data Infrastructure Engineer, you’ll …• Provision and deploy infrastructure using Pulumi for Kafka, Spark, Airflow, Athena, and other critical systems on AWS.• Manage and maintain clusters, ensuring optimal performance and reliability, including implementing auto-scaling and right-sizing instances.• Configure and manage VPCs, load balancers, and VPC endpoints for secure communication between internal and external services.• Manage IAM roles, apply security patches, plan and execute version upgrades, and ensure compliance with regulations such as GDPR.• Design and implement high-availability solutions across multiple zones and regions, including backups, multi-region replication, and disaster recovery plans.• Oversee S3 data lake management, including file size management, compaction, encryption, and compression to maximize storage efficiency.• Implement caching strategies, indexing, and query optimization to ensure efficient data retrieval and processing.• Spearhead initiatives for optimizing performance, capacity planning, ensuring fault tolerance, and implementing failure recovery across all infrastructure components.• Implement monitoring and logging using tools like Datadog, CloudWatch and OpenSearch.• Develop services, tools and automation to simplify infrastructure complexity for other engineering teams, enabling them to focus on building great products.• Participate in all engineering activities including incident response, interviewing, designing and reviewing technical specifications, code review, and releasing new functionality.• Mentor, coach, and inspire a team of engineers of various levels.In addition to the responsibilities outlined above, at Webflow we will support you in identifying where your interests and development opportunities lie and we'll help you incorporate them into your role.About youYou’ll thrive as a Senior Data Infrastructure Engineer if you have:• 5+ years of experience as a Data Infrastructure Engineer or in related roles like Platform Engineer, SRE, DevOps or Backend Engineer.• Strong experience with provisioning and managing data infrastructure components like Kafka, Spark, and Airflow.• Proficiency with cloud services and environments (compute, storage, networking, identity management, infrastructure as code, etc.).• Experience with containerization technologies like Docker and Kubernetes.• Expertise in infrastructure as code tools like Terraform and Pulumi.• Solid understanding of networking concepts and configurations, including VPCs, load balancers, and endpoints.• Experience with monitoring and logging tools.• Strong problem-solving skills and attention to detail.• Excellent communication and collaboration skills.Bonus points if you have:• AWS certifications (e.g., AWS Certified Solutions Architect, AWS Certified DevOps Engineer).• Familiarity with multi-zone and multi-region high availability and disaster recovery strategies.• Knowledge of compliance standards (GDPR, CCPA) and security best practices.Our Core Behaviors:• Obsess over customer experience. We deeply understand what we’re building and who we’re building for and serving. We define the leading edge of what’s possible in our industry and deliver the future for our customers.• Move with heartfelt urgency. We have a healthy relationship with impatience, channeling it thoughtfully to show up better and faster for our customers and for each other. Time is the most limited thing we have, and we make the most of every moment.• Say the hard thing with care. Our best work often comes from intelligent debate, critique, and even difficult conversations. We speak our minds and don’t sugarcoat things — and we do so with respect, maturity, and care.• Make your mark. We seek out new and unique ways to create meaningful impact, and we champion the same from our colleagues. We work as a team to get the job done, and we go out of our way to celebrate and reward those going above and beyond for our customers and our teammates.Benefits & wellness• Equity ownership (RSUs) in a growing, privately-owned company• 100% employer-paid healthcare, vision, and dental insurance coverage for employees and dependents (US; full-time Canadian workers working 30+ hours per week), as well as Health Savings Account/Health Reimbursement Account, dependent on insurance plan selection. Employees also have voluntary insurance options, such as life, disability, hospital protection, accident, and critical illness• 12 weeks of paid parental leave for both birthing and non-birthing caregivers, as well as an additional 6-8 weeks of pregnancy disability for birthing parents to be used before child bonding leave. Employees also have access to family planning care and reimbursement.• Flexible PTO with an mandatory annual minimum of 10 days paid time off, and sabbatical program• Access to mental wellness coaching, therapy, and Employee Assistance Program• Monthly stipends to support health and wellness, as well as smart work, and annual stipends to support professional growth• Professional career coaching, internal learning & development programs• 401k plan and financial wellness benefits, like CPA or financial advisor coverage• Commuter benefits for in-office workersTemporary employees are not eligible for paid holiday time off, accrued paid time off, paid leaves of absence, or company-sponsored perks.Be you, with usAt Webflow, equality is a core tenet of our culture. We are committed to building an inclusive global team that represents a variety of backgrounds, perspectives, beliefs, and experiences. Employment decisions are made on the basis of job-related criteria without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other classification protected by applicable law.Stay connectedNot ready to apply, but want to be part of the Webflow community? Consider following our story on our Webflow Blog, LinkedIn, Twitter, and/or Glassdoor.Please note:To join Webflow, you'll need valid U.S. or Canadian work authorization depending on the country of employment.If you are extended an offer, that offer may be contingent upon your successful completion of a background check, which will be conducted in accordance with applicable laws. We may obtain one or more background screening reports about you, solely for employment purposes.Webflow Applicant Privacy Notice","You’ll thrive as a Senior Data Infrastructure Engineer if you have:; 5+ years of experience as a Data Infrastructure Engineer or in related roles like Platform Engineer, SRE, DevOps or Backend Engineer; Strong experience with provisioning and managing data infrastructure components like Kafka, Spark, and Airflow; Proficiency with cloud services and environments (compute, storage, networking, identity management, infrastructure as code, etc.); Experience with containerization technologies like Docker and Kubernetes; Expertise in infrastructure as code tools like Terraform and Pulumi; Solid understanding of networking concepts and configurations, including VPCs, load balancers, and endpoints; Experience with monitoring and logging tools; Strong problem-solving skills and attention to detail; Excellent communication and collaboration skills; AWS certifications (e.g., AWS Certified Solutions Architect, AWS Certified DevOps Engineer); Familiarity with multi-zone and multi-region high availability and disaster recovery strategies; Knowledge of compliance standards (GDPR, CCPA) and security best practices; Our best work often comes from intelligent debate, critique, and even difficult conversations; 11 more items(s)","The cash compensation for this role is tailored to align with the cost of labor in different geographic markets; We've structured the base pay ranges for this role into zones for our geographic markets, and the specific base pay within the range will be determined by the candidate’s geographic location, job-related experience, knowledge, qualifications, and skills; Zone A: $158,000 - $218,000; Zone B: $149,000 - $205,000; CAD 180,000 - CAD 248,000; Equity ownership (RSUs) in a growing, privately-owned company; 100% employer-paid healthcare, vision, and dental insurance coverage for employees and dependents (US; full-time Canadian workers working 30+ hours per week), as well as Health Savings Account/Health Reimbursement Account, dependent on insurance plan selection; Employees also have voluntary insurance options, such as life, disability, hospital protection, accident, and critical illness; 12 weeks of paid parental leave for both birthing and non-birthing caregivers, as well as an additional 6-8 weeks of pregnancy disability for birthing parents to be used before child bonding leave; Employees also have access to family planning care and reimbursement; Flexible PTO with an mandatory annual minimum of 10 days paid time off, and sabbatical program; Access to mental wellness coaching, therapy, and Employee Assistance Program; Monthly stipends to support health and wellness, as well as smart work, and annual stipends to support professional growth; Professional career coaching, internal learning & development programs; 401k plan and financial wellness benefits, like CPA or financial advisor coverage; Commuter benefits for in-office workers; Temporary employees are not eligible for paid holiday time off, accrued paid time off, paid leaves of absence, or company-sponsored perks; 14 more items(s)","Kafka, Spark, Airflow, Athena, AWS, Pulumi, Docker, Kubernetes, Terraform, Datadog, CloudWatch, OpenSearch"
"Sr. Big Data Engineer in San Francisco, CA / McLean, VA",Snowflake,"Snowflake • San Mateo, CA •  via Snowflake Careers",9 days ago,Full-time,"is a world with unlimited access to governed data, so every organization can tackle the challenges and opportunities of today and reveal the possibilities of tomorrow.We’re hiring talented Software Engineers to join the Snowflake Database Engineering group and help build the world’s leading AI Data Cloud platform! Our group spans across three key areas -• Database Query Processing: This is the core data processing engine, powering the world's best data platform. This includes building SQL language features and developing novel query optimization and execution techniques for industry-leading performance. We also build features that automatically optimize workloads for performance and cost-efficiency.• Foundation Database: It is our large-scale distributed transactional KV store - internally called FDB - which powers all of Snowflake’s products and services and is rapidly evolving to meet Snowflake’s future needs. FDB houses Snowflake's metadata, allowing the service to be elastic, making the AI Data Cloud possible. FDB is also used to store user data for Unistore (and provides transactional and analytical optimized access paths).• Unistore: Unistore unites analytics with transactional data processing by removing OLAP data silos and providing real-time data produced by our customers' OLTP-based transactional systems. Unistore can tell you NOW using the superpower of HTAP (Hybrid Transaction and Analytics Processing) provided by Snowflake’s Hybrid Tables.Learn more about the Snowflake Database Engineering group at https://careers.snowflake.com/us/en/database-engineeringAS A SOFTWARE ENGINEER AT SNOWFLAKE, YOU WILL:• Design, develop, and support a petabyte-scale cloud database that is highly parallel and fault-tolerant.• Build high-quality and highly reliable software to meet the needs of some of the largest companies on the planet.• Analyze and understand performance and scalability bottlenecks in the system and solve them.• Pinpoint problems, instrument relevant components as needed, and ultimately implement solutions.• Design and implement novel query optimization or distributed data processing algorithms which allow Snowflake to provide industry leading data warehousing capabilities.• Design and implement the new service architecture required to enable the Snowflake AI Data Cloud• Develop tools for improving our customers' insights into their workloads.OUR IDEAL SOFTWARE ENGINEER WILL HAVE:• 2+ years industry experience working on commercial or open-source software.• Systems programming skills including multi-threading, concurrency, etc. Fluency in C++, C, or Java is preferred.• Familiarity with development in a Linux environment.• Excellent problem solving skills, and strong CS fundamentals including data structures, algorithms, and distributed systems.• Systems programming skills including multi-threading, concurrency, etc.• Experience with implementation testing, debugging and documentation.• Bachelor’s degree or foreign equivalent in Computer Science, Software Engineering or related field; Masters or PhD preferred.• Ability to work on-site in our San Mateo / Bellevue / Berlin office.BONUS POINTS FOR EXPERIENCE WITH THE FOLLOWING:• SQL or other database technologies including internal design and implementation.• Hands-on experience designing/implementing database security technologies, including encryption algorithms, cryptographic key management systems, and secure authentication mechanisms.• Query optimization, query execution, compiler design and implementation.• Experience with internals of distributed key value stores like FoundationDB and storage engines like RocksDB, InnoDB, BerkeleyDB etc.• Experience with MySQL, PostgreSQL internals• Data warehouse design, database systems, and large-scale data processing solutions like Hadoop and Spark.• Large scale distributed systems, transactions and consistency models.• Experience in database replication technology• Big data storage technologies and their applications, e.g., HDFS, Cassandra, Columnar Databases, etc.","2+ years industry experience working on commercial or open-source software; Systems programming skills including multi-threading, concurrency, etc; SQL or other database technologies including internal design and implementation; Hands-on experience designing/implementing database security technologies, including encryption algorithms, cryptographic key management systems, and secure authentication mechanisms; Query optimization, query execution, compiler design and implementation; Experience with internals of distributed key value stores like FoundationDB and storage engines like RocksDB, InnoDB, BerkeleyDB etc; Experience with MySQL, PostgreSQL internals; Data warehouse design, database systems, and large-scale data processing solutions like Hadoop and Spark; Large scale distributed systems, transactions and consistency models; Experience in database replication technology; Big data storage technologies and their applications, e.g., HDFS, Cassandra, Columnar Databases, etc; 8 more items(s)","This includes building SQL language features and developing novel query optimization and execution techniques for industry-leading performance; FDB is also used to store user data for Unistore (and provides transactional and analytical optimized access paths); Unistore: Unistore unites analytics with transactional data processing by removing OLAP data silos and providing real-time data produced by our customers' OLTP-based transactional systems; Design, develop, and support a petabyte-scale cloud database that is highly parallel and fault-tolerant; Build high-quality and highly reliable software to meet the needs of some of the largest companies on the planet; Analyze and understand performance and scalability bottlenecks in the system and solve them; Pinpoint problems, instrument relevant components as needed, and ultimately implement solutions; Design and implement novel query optimization or distributed data processing algorithms which allow Snowflake to provide industry leading data warehousing capabilities; Design and implement the new service architecture required to enable the Snowflake AI Data Cloud; Develop tools for improving our customers' insights into their workloads; 7 more items(s)","Snowflake, FDB, Unistore, C++, C, Java, Linux, SQL, FoundationDB, RocksDB, InnoDB, BerkeleyDB, MySQL, PostgreSQL, Hadoop, Spark, HDFS, Cassandra, Columnar Databases"
Data Engineer (Offshore),"Next Level Business Services, Inc.","Next Level Business Services, Inc. • San Francisco, CA •  via ZipRecruiter",6 days ago,Contractor,ast 2 year of experience building and deploying applications to cloud providers like AWSImpetus is a one stop technology vendor in the Big Data space with an interesting combination of full life cycle implementation services and cutting edge products/solutions. Some of products/solutions are listed below:StreamAnalytix (industry's only real time streaming platform based on open source)KyvosInsights (industry's most scalable technology for OLAP on Hadoop)EDW workload migration to Hadoop solution (a solution that can automatically offload 70-95 percent of your EDW workload to Hadoop)I hope this will give you some idea of our capabilities in Big Data Space.Additional InformationAll your information will be kept confidential according to EEO guidelines.,"At least 8 years of experience in Java or Scala; At least 8 years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python; At least 2 years of experience with leading big and fast data technologies like Spark, Scala, Akka, Cassandra, Accumulo, Hbase, Hadoop, HDFS, AVRO, MongoDB, Mesos, ElasticSearch; At least 2 year of experience building and deploying applications to cloud providers like AWS; EDW workload migration to Hadoop solution (a solution that can automatically offload 70-95 percent of your EDW workload to Hadoop); 2 more items(s)",,"AWS, StreamAnalytix, KyvosInsights, Hadoop, Java, Scala, Unix, Linux, Shell, Perl, Python, Spark, Akka, Cassandra, Accumulo, Hbase, HDFS, AVRO, MongoDB, Mesos, ElasticSearch"
Senior Data Engineering Manager,BayOne,"BayOne • San Francisco, CA •  via Indeed",Full-time,No Degree Mentioned,"ons and data models• Providing users access to datasets using REST and Python APIs• Communicating with business users and technology stakeholders• Improving internal processes by identifying, planning, and putting them into practice","Design, develop, and maintain data pipelines to collect, process, and store large volumes of structured and unstructured data; Use programming languages such as Python, SQL, Java, or Scala to develop data pipelines and ETL (Extract, Transform, Load) processes; Collaborating with cross-functional teams to understand data requirements and translate them into technical specifications and data models; Providing users access to datasets using REST and Python APIs; Communicating with business users and technology stakeholders; Improving internal processes by identifying, planning, and putting them into practice; 3 more items(s)",,"Python, SQL, Java, Scala, REST APIs"
Staff Data Engineer,Atlassian,"Atlassian • San Francisco, CA •  via LinkedIn",2 days ago,Full-time,"bination. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually as part of being a distributed-first company.About This RoleData is a big deal at Atlassian. We ingest billions of events each month into our analytical platforms and have dozens of teams across the company that depend on data platforms to drive their decisions.We seek a Senior Data Engineering Manager to join the Data Engineering team that supports the Customer Support and Services (CSS) organization in building foundational analytical and operational data products to provide the best customer support experience for Atlassian Customers.As a Senior Data Engineering Manager, you will be a technical leader and a people manager. Your role encompasses guiding the team in making crucial technical decisions to develop production-grade data products. Additionally, you will prioritize and organize the team's workload, ensuring optimal efficiency. Beyond that, you'll foster career growth by providing mentorship opportunities and empowering your team to thrive professionally.CompensationAt Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:Zone A: $203,300 - $271,000Zone B: $183,000 - $243,900Zone C: $168,700 - $225,000This role may also be eligible for benefits, bonuses, commissions, and equity.Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.Responsibilities• Build and lead a team of data engineers through hiring, coaching, mentoring, and hands-on career development.• Provide deep technical guidance and support the team in driving large projects within complex dependencies and multiple stakeholders.• Collaborate with various stakeholders, gather deep domain expertise, and own delivery of projects across multiple streams.• Collaborate in technical/architectural discussions, provide direction, and drive decision-making.• Inspire innovation and operational excellence within the team and influence other Data Engineering teams.QualificationsOn your first day, we'll expect you to have:• You have a Bachelor’s or Master’s degree in computer science or related field of study and at least 6-8 years of experience as an individual contributor in a Data Engineering or related Software Engineering role.• You have 4+ years of experience managing data or related software teams.• You have a track record of driving and delivering large (multi-person or multi-team) and complex projects.• You are a great communicator and maintain many of the essential cross-team and cross-functional relationships necessary for the team's success.• Industry experience working with large-scale, high-performance data processing systems (batch and streaming) with a ""Streaming First"" mindset to drive Atlassian's business growth and improve the product experience.• You have stellar people management skills and experience leading agile data or related software teams.• You are passionate about grooming talent and helping them reach their highest potential.• You can drive technical excellence, pushing for innovation and quality.• Experience working with public cloud offerings such as Amazon Web Services, DynamoDB, ElasticSearch, S3, Databricks, Spark/Spark-Streaming, and Databricks.We'd be super excited if you have• Experience with at least one high-level programming language, such as Python• Understanding and experience building RESTful APIs and microservices• Experience with Tableau or other reporting tools.• Basic understanding of Experimentation.","You have a Bachelor’s or Master’s degree in computer science or related field of study and at least 6-8 years of experience as an individual contributor in a Data Engineering or related Software Engineering role; You have 4+ years of experience managing data or related software teams; You have a track record of driving and delivering large (multi-person or multi-team) and complex projects; You are a great communicator and maintain many of the essential cross-team and cross-functional relationships necessary for the team's success; Industry experience working with large-scale, high-performance data processing systems (batch and streaming) with a ""Streaming First"" mindset to drive Atlassian's business growth and improve the product experience; You have stellar people management skills and experience leading agile data or related software teams; You are passionate about grooming talent and helping them reach their highest potential; You can drive technical excellence, pushing for innovation and quality; Experience working with public cloud offerings such as Amazon Web Services, DynamoDB, ElasticSearch, S3, Databricks, Spark/Spark-Streaming, and Databricks; Experience with at least one high-level programming language, such as Python; Understanding and experience building RESTful APIs and microservices; Experience with Tableau or other reporting tools; Basic understanding of Experimentation; 10 more items(s)","Beyond that, you'll foster career growth by providing mentorship opportunities and empowering your team to thrive professionally; At Atlassian, we strive to design equitable, explainable, and competitive compensation programs; Zone A: $203,300 - $271,000; Zone B: $183,000 - $243,900; Zone C: $168,700 - $225,000; This role may also be eligible for benefits, bonuses, commissions, and equity; Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones; 4 more items(s)","Amazon Web Services, DynamoDB, ElasticSearch, S3, Databricks, Spark/Spark-Streaming, Python, Tableau"
Data Engineer (W2/Benefits Provided) - Freelance [Remote],hackerone,"hackerone • San Francisco, CA •  via CareerBuilder",12 days ago,Full-time,"including bug bounty, pentesting, code security audits, spot checks, and AI red teaming, ensure continuous vulnerability discovery and management throughout the software development lifecycle. Trusted by industry leaders such as Coinbase, General Motors, GitHub, Goldman Sachs, Hyatt, PayPal, and the U.S. Department of Defense, HackerOne was named a Best Workplace for Innovators by Fast Company in 2023 and a Most Loved Workplace for Young Professionals in 2024.Position SummaryHackerOne is seeking a Staff Data Engineer, DataOne, to lead the discovery, architecture, and development of high-performance, scalable data products and solutions. Joining our growing, distributed organization, you'll be instrumental in building the foundation that powers HackerOne's one source of truth.As a Staff Data Engineer, you'll have the autonomy to lead challenging projects and foster collaboration across the company. Leveraging your extensive technological expertise, domain knowledge, and dedication to business objectives, you'll drive innovation to propel HackerOne forward.DataOne's Mission & VisionDataOne democratizes source of truth information and insights to enable all Hackeronies to ask the right questions, tell cohesive stories and make rigorous decisions, so that HackerOne can delight our Customers and empower the world to build a safer internet.The future is one where every Hackeronie is a catalyst for positive change, driving data-informed innovation while fostering our culture of transparency, collaboration, integrity, excellence, and respect for all.Your Journey at HackerOneYour first day will start with a warm welcome and technical onboarding!• Your first 30 days will focus on getting to know HackerOne. You will join your new squad and begin onboarding - learn our technology stack (Python, Airflow, Snowflake, DBT, Meltano, Fivetran, Looker, AWS), and meet our Hackeronies.• Within 60 days, you will deliver impact on a department level with consistent contribution of high-quality, high-impact code.• Within 90 days, you will drive the continuous evolution and innovation of data at HackerOne, identifying and leading new initiatives. Additionally, you foster cross-departmental collaboration to enhance these efforts.Your journey doesn't stop here: your manager will work with you on your growth path toward the future of your career so you become the best version of yourself!After 90 days, you will:• Deliver impact by consistently contributing high-quality, high-impact code, and by leading and delivering cross-functional product and technical initiatives.• Manage highly complex design processes, balance competing priorities, and adapt to shifting business objectives.• Drive continuous evolution and innovation within the DataOne codebase and engineering processes.• Champion a higher bar for discoverability, usability, reliability, timeliness, consistency, validity, uniqueness, simplicity, completeness, integrity, security, and compliance of information and insights across the company.• Provide technical leadership and mentorship, fostering a culture of continuous learning and growth.• Delegate tasks effectively within your team and across the organization.By establishing HackerOne's one source of truth and developing data products and solutions on this foundation, we help our Hackers, Customers, and Hackeronies gain insights and make better decisions, thereby empowering the world to build a safer internet.Minimum Qualifications• 8+ years experience developing data frameworks (preferably using Python).• 8+ years of using SQL for data manipulation in a fast-paced work environment.• Strong programming skills, knowledge of algorithms, and data structures.• Extensive experience building and optimizing data pipelines, products, and solutions.• Extensive experience working with various data technologies and tools such as Airflow, Snowflake, Meltano, Fivetran, DBT, AWS, and Looker.• Proven track record of having substantial impact across the company, demonstrating your ability to drive positive change and achieve significant results.• HackerOne is a digital-first company. This model offers our employees flexibility in time and location. All employees must be able to work and excel in a remote environment.Preferred Qualifications• US EST and CST time zones.• Champion of new initiatives focused on architectural enhancements.• Experience working with Kubernetes.• Proven track record of driving innovation, adopting emerging technologies and implementing industry best practices.• Passion for working backwards from the Customer and empathy for business stakeholders.• Excellent communication skills, and can present data-driven narratives in verbal, presentation, and written formats.• Experience working with Agile and iterative development processes.• US EST and CST time zones preferred.• HackerOne is a digital first company, and all employees must be able to work and excel in a remote environment.Employment at HackerOne is contingent on a background check.HackerOne is an Equal Opportunity Employer in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, pregnancy, disability or veteran status, or any other protected characteristic as outlined by international, federal, state, or local laws.This policy applies to all HackerOne employment practices, including hiring, recruiting, promotion, termination, layoff, recall, leave of absence, compensation, benefits, training, and apprenticeship. HackerOne makes hiring decisions based solely on qualifications, merit, and business needs at the time.Pursuant to the San Francisco Fair Chance Ordinance, all qualified applicants with arrest and conviction records will be considered for the position.HackerOne ValuesHackerOne commits to maintaining a strong, inclusive culture built for our employees and our community of hackers. We are driven by our five core values. We recognize that our mission is bigger than us, and therefore act with integrity at all times. As a team, we believe that transparency builds trust so we default to disclosure in our communications. Each individual executes with excellence, creating an environment of greater alignment and greater autonomy. We win as a team and respect all people to empower everyone to learn from each other, innovate, and grow.What We DoHackerOne is the global leader in human-powered security. We leverage human ingenuity to pinpoint the most critical security flaws across your attack surface to outmatch cybercriminals. HackerOne's Attack Resistance Platform combines the most creative human intelligence with the latest artificial intelligence to reduce threat exposure at all stages of the software development lifecycle. From meeting compliance requirements with pentesting to finding novel and elusive vulnerabilities through bug bounty, HackerOne's elite community of ethical hackers helps organizations transform their businesses with confidence. HackerOne has helped find and fix more vulnerabilities than any other vendor for brands including Coinbase, General Motors, GitHub, Goldman Sachs, Hyatt, PayPal, and the U.S Department of Defense. In 2023, HackerOne was named a Best Workplace for Innovators by Fast Company.HackerOne is Digital FirstOur work is optimized for asynchronous collaboration, knowledge management, and decision-making. HackerOne is creating an industry, and to do that, we must employ the most creative, forward-thinking distributed talent in the market. Our remote model allows employees to contribute to our mission while providing time and location flexibility which are core elements to a healthy relationship between professional and personal pursuits.Compensation Range:$169,200 - 253,800 USD#LI-Remote#LI-SM1We're committed to building a global team! For certain roles outside the United States, U.K., and the Netherlands, we partner with Remote.com as our Employer of Record (EOR).Employment at HackerOne is contingent on a background check.HackerOne is an Equal Opportunity Employer in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, pregnancy, disability or veteran status, or any other protected characteristic as outlined by international, federal, state, or local laws.This policy applies to all HackerOne employment practices, including hiring, recruiting, promotion, termination, layoff, recall, leave of absence, compensation, benefits, training, and apprenticeship. HackerOne makes hiring decisions based solely on qualifications, merit, and business needs at the time.For US based roles only: Pursuant to the San Francisco Fair Chance Ordinance, all qualified applicants with arrest and conviction records will be considered for the position.HackerOne ValuesHackerOne commits to maintaining a strong, inclusive culture built for our employees and our community of hackers. We are driven by our five core values. We recognize that our mission is bigger than us, and therefore act with integrity at all times. As a team, we believe that transparency builds trust so we default to disclosure in our communications. Each individual executes with excellence, creating an environment of greater alignment and greater autonomy. We win as a team and respect all people to empower everyone to learn from each other, innovate, and grow.Compensation Range: $160K - $253.8K","8+ years experience developing data frameworks (preferably using Python); 8+ years of using SQL for data manipulation in a fast-paced work environment; Strong programming skills, knowledge of algorithms, and data structures; Extensive experience building and optimizing data pipelines, products, and solutions; Extensive experience working with various data technologies and tools such as Airflow, Snowflake, Meltano, Fivetran, DBT, AWS, and Looker; Proven track record of having substantial impact across the company, demonstrating your ability to drive positive change and achieve significant results; All employees must be able to work and excel in a remote environment; 4 more items(s)","$169,200 - 253,800 USD; Compensation Range: $160K - $253.8K","Python, Airflow, Snowflake, DBT, Meltano, Fivetran, Looker, AWS, SQL, Kubernetes"
Data Engineer with Data Visualization Exp,Braintrust,"Braintrust • San Francisco, CA •  via The Org",Contractor,No Degree Mentioned,"g your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.• JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)• LOCATION: Remote - United States only• HOURLY RANGE: Our client is looking to pay $145 – $155 /hr• ESTIMATED DURATION: 40h/week - Short term• EXPERIENCE: 5-10 years• BRAINTRUST JOB ID: 9737THE OPPORTUNITYRequirementsAs a member of the Finance Operations & Analytics team, you’ll focus on implementing scalable systems and processes that provide metrics visibility for our senior leaders, enable business analysts to generate insights, and power our mission-critical reporting as a public company.We are looking for a seasoned Data Engineer to build multiple analytics datasets for us. The successful candidate will have experience working with enterprise data warehouses, be able to ramp up quickly and adapt in new environments, and have strong critical thinking skills.What you’ll do :• Build three detailed diagnostic datasets to allow our Business Operations partners to analyze metrics quickly and all in one place• User Growth & Engagement Diagnostics• User Impressions & Conversions Diagnostics• Regional Revenue Diagnostics• Collaborate with both central technical teams and business stakeholders to ramp quickly• Analyze existing data sources independently in Hive using Presto/Spark SQL and be able to create design recommendations• Build an ETL pipeline using Airflow with queries optimized for scale and large datasetsWhat you’ll be working onWhat we're looking for :• Hands-on experience in principled data warehouse design, data visualization, and data pipeline design and development• 4+ years of hands-on experience with data platforms (Hadoop, Hive, Presto, Spark and Snowflake)• Expert-level proficiency in writing complex, highly-optimized SQL queries across large datasets• Experience with Airflow and associated Python data libraries (Pandas, NumPy, Matplotlib)• Experience working with Business TeamsApply Now!Notes:Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.","EXPERIENCE: 5-10 years; We are looking for a seasoned Data Engineer to build multiple analytics datasets for us; The successful candidate will have experience working with enterprise data warehouses, be able to ramp up quickly and adapt in new environments, and have strong critical thinking skills; Hands-on experience in principled data warehouse design, data visualization, and data pipeline design and development; 4+ years of hands-on experience with data platforms (Hadoop, Hive, Presto, Spark and Snowflake); Expert-level proficiency in writing complex, highly-optimized SQL queries across large datasets; Experience with Airflow and associated Python data libraries (Pandas, NumPy, Matplotlib); Experience working with Business Teams; 5 more items(s)",HOURLY RANGE: Our client is looking to pay $145 – $155 /hr; ESTIMATED DURATION: 40h/week - Short term,"Hadoop, Hive, Presto, Spark, Snowflake, SQL, Airflow, Python, Pandas, NumPy, Matplotlib"
"Manager, Data Engineering",Infojini Inc,"Infojini Inc • San Francisco, CA •  via ZipRecruiter",9 days ago,Full-time,"commercial customer base including fortune 100 companies and most state and federal agencies such as State of North Carolina, State of South Carolina, State of Maryland, State of California, State of Pennsylvania, State of Virginia, State of Washington and many others.Infojini Consulting is an equal opportunity employer and considers all qualified individuals for employment irrespective of their race, gender, age, color, sexual orientation. We offer an excellent compensation packageJob DescriptionWe are looking for Data Engineer in San Francisco, CA for Fulltime position.Please refer someone else if you are not available at this time or you are not right match for this job opportunity. We have great Referral Bonus up to $2500!!! Please don't miss to refer someone who are looking for projects.Job details mentioned below:Job Title: Data EngineerLocation: San Francisco, CADuration: Full TimeClient: Direct ClientExperience:u CS fundamentalsu Experience with real-world data, large and smallu A talent for communicating through datau A passion for empowering consumers through data visualizationEXTRA CREDIT:u Probabilistic classifiers (e.g. Naive Bayes) and statistical analysisu Full-text search engines (e.g., Elasticsearch, Lucene)u Experience with real-estate or economics data sets (e.g., MLS, US Census)u An interest in economics, construction, topographyOUR TECH STACK INCLUDES:u AngularJSu Ruby on Railsu MySQL and Elasticsearchu Ubuntu Linux, Nginx, Varnish, AWS, Cloudfront, S3u Github, Bugsnag, New Relic, Optimizely, MixpanelAdditional Information","u CS fundamentals; u Experience with real-world data, large and small; u A talent for communicating through data; u A passion for empowering consumers through data visualization; u Probabilistic classifiers (e.g; Naive Bayes) and statistical analysis; u Full-text search engines (e.g., Elasticsearch, Lucene); u Experience with real-estate or economics data sets (e.g., MLS, US Census); u An interest in economics, construction, topography; u Ruby on Rails; u MySQL and Elasticsearch; u Ubuntu Linux, Nginx, Varnish, AWS, Cloudfront, S3; 9 more items(s)",We offer an excellent compensation package; We have great Referral Bonus up to $2500!!!,"AngularJS, Ruby on Rails, MySQL, Elasticsearch, Ubuntu Linux, Nginx, Varnish, AWS, Cloudfront, S3, Github, Bugsnag, New Relic, Optimizely, Mixpanel"
Senior Data Engineer 🏆,Chegg Inc.,"Chegg Inc. • Santa Clara, CA •  via LinkedIn",1 month ago,Full-time,"and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc• Collaborate with business users, data scientists, and engineering teams across the company and various time zone to strive for greater functionality in our data• Create and manage BI dashboards and reports using tools like Tableau• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.• Provide technical leadership and coaching to junior team membersYou Have• 5 + Years of experience in data engineering, software engineering, or other related roles• Proficiency in SQL, Python, and at least one cloud platform (AWS,GCP)• Experience with Databricks' lakehouse architecture and implementing medallion architecture• Experience supporting and working with cross-functional teams in a dynamic environment• Experience with AWS cloud services such as EMR, Redshift, S3, Glue, EC2• Experience with large scale distributed real-time tools such as Kafka, Flink, or Spark.• Experience with BI tools and data visualization techniques• Experience designing various consumption patterns on top of data lake/lakehouse to cater to different personas within organization.• Excellent written and oral communication skills• Knowledge of Gen AI is a plusThe pay range for this position is $100,192 to $248,094. The actual pay will vary based on geographic location, job requirements, professional experience, and other factors. In addition, Chegg offers a comprehensive benefits plan for eligible employees, including medical, dental, vision, life and supplemental life insurance, short-and long-term disability, mental health support, parental leave, paid time off, volunteer time off, paid holidays, 401(k) with matching contributions, Flexible Spending Account (FSA) and Health Savings Account (H.S.A.) options, an Employee Stock Purchase Plan, an Employee Referral Program, Tuition Reimbursement, and other benefits found at: https://www.chegg.com/about/working-at-chegg/benefits/.Why do we exist?Students are working harder than ever before to stabilize their future. Our recent research study called State of the Student shows that nearly 3 out of 4 students are working to support themselves through college and 1 in 3 students feel pressure to spend more than they can afford. We founded our business on provided affordable textbook rental options to address these issues. Since then, we’ve expanded our offerings to supplement many facets of higher educational learning through Chegg Study, Chegg Math, Chegg Writing, Chegg Internships, Thinkful Online Learning, and more to support students beyond their college experience. These offerings lower financial concerns for students by modernizing their learning experience. We exist so students everywhere have a smarter, faster, more affordable way to student.Video ShortsLife at Chegg: http://youtu.be/Fwf90zgaOLACertified Great Place to Work!: http://reviews.greatplacetowork.com/cheggChegg Corporate Career Page: https://jobs.chegg.com/Chegg India: http://www.cheggindia.com/Chegg Israel: http://www.chegg.com/about/working-at-chegg/israel/Thinkful (a Chegg Online Learning Service): https://www.thinkful.com/about/#careersChegg out our culture and benefits!http://www.chegg.com/about/working-at-chegg/benefits/http://techblog.chegg.com/Chegg is an equal opportunity employer","5 + Years of experience in data engineering, software engineering, or other related roles; Proficiency in SQL, Python, and at least one cloud platform (AWS,GCP); Experience with Databricks' lakehouse architecture and implementing medallion architecture; Experience supporting and working with cross-functional teams in a dynamic environment; Experience with AWS cloud services such as EMR, Redshift, S3, Glue, EC2; Experience with large scale distributed real-time tools such as Kafka, Flink, or Spark; Experience with BI tools and data visualization techniques; Experience designing various consumption patterns on top of data lake/lakehouse to cater to different personas within organization; Excellent written and oral communication skills; 6 more items(s)","The pay range for this position is $100,192 to $248,094; The actual pay will vary based on geographic location, job requirements, professional experience, and other factors; In addition, Chegg offers a comprehensive benefits plan for eligible employees, including medical, dental, vision, life and supplemental life insurance, short-and long-term disability, mental health support, parental leave, paid time off, volunteer time off, paid holidays, 401(k) with matching contributions, Flexible Spending Account (FSA) and Health Savings Account (H.S.A.) options, an Employee Stock Purchase Plan, an Employee Referral Program, Tuition Reimbursement, and other benefits found at: https://www.chegg.com/about/working-at-chegg/benefits/; Since then, we’ve expanded our offerings to supplement many facets of higher educational learning through Chegg Study, Chegg Math, Chegg Writing, Chegg Internships, Thinkful Online Learning, and more to support students beyond their college experience; Chegg out our culture and benefits!; 2 more items(s)","Tableau, SQL, Python, AWS, GCP, Databricks, EMR, Redshift, S3, Glue, EC2, Kafka, Flink, Spark"
"Principal, Data Engineer",Digital Janet,"Digital Janet • Atherton, CA •  via LinkedIn",Full-time,No Degree Mentioned,"Role: Data EngineerLocation: Menlo Park, CA (100% Work From Office)Type: Full-timeExperience: 10+ yearsSkillsCandidate should have a good knowledge of Influxdb, Hive and has experience working in Lab Environment","Experience: 10+ years; Candidate should have a good knowledge of Influxdb, Hive and has experience working in Lab Environment",,"Influxdb, Hive"
"Financial Data, Engineering Manager",Walmart,"Walmart • Hayward, CA •  via ZipRecruiter",5 days ago,Full-time and Part-time,"experienced tech lead to spearhead advertising data analytics related to deman d, supply, and overall marketplace healt h. You will be responsible for extracting meaningful insights about campaign performance, marketplace efficiency, and gaps in systems and products , surfacing marketplace health metrics via dashboards , and working with cross-functiona l partners to move the needle in display advertising.What you will do:• Building data pipelines and dashboards to monitor display ads serving funnel and marketplace health.• Leading analytics for display ads marketplace and presenting findings and recommendations to engineering teams and cross functional partners.• Collaborating with engineering and data science leads to improve data quality, build ing data warehouse, and delivering predictive models for market place insights and anomaly detection.• Bringing data driven culture to the engineering team to assure product and system quality.What you'll bring :• Bachelor's degree in data science , computer science, statistics, operation research, or related fields• 6 years' experience of building data pipelines, extracting signals from noisy data, and establishing metrics for monitoring.• Proficiency in data analysis tools (e.g., as Python, R, SQL) and data visualization tools (e.g., Tableau, Superset, Looker ) .• Analytical thinking and detail-oriented mindset. Familiar with S QL and N on- SQL database s.• String communication skills. Ability to convey complex ideas and findings to non-technical stakeholders .• Knowledge of A/B testing.Preferred qualifications:• Advanced degree in data science, computer science, statistics, operation research, or related fields• 6 + years' experience in programmatic advertising, on-line content distribution, or e-commerce.• Knowledge of optimization, auction, and ML applications.• People management experience is a big plus.About Walmart Global Tech :Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That is what we do at Walmart Global Tech. We are a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity and those looking for the opportunity to define their career. Here, you can kickstart a distinguished career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.Flexible, hybrid work:We use a hybrid way of working that is primarily in the office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.Benefits:Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO , multiple health plans, and much more.Equal Opportunity Employer:Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas, and opinions - while being inclusive of all people.The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.‎‎‎You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable.‎For information about PTO, see https://one.walmart.com/notices .‎‎Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.‎Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.‎For information about benefits and eligibility, see One.Walmart .‎Bellevue, Washington US-11075:The annual salary range for this position is $132,000.00-$264,000.00‎SUNNYVALE, California US-04396:The annual salary range for this position is $143,000.00-$286,000.00‎‎‎‎‎‎‎‎‎‎Additional compensation includes annual or quarterly performance bonuses.‎Additional compensation for certain positions may also include:‎‎- Stock‎‎Minimum Qualifications...Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.Option 1: Bachelor's degree in Computer Science and 5 years' experience in software engineering or related field. Option 2: 7 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.4 years' experience in data engineering, database engineering, business intelligence, or business analytics.Preferred Qualifications...Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture.Primary Location...840 W CALIFORNIA AVE, SUNNYVALE, CA 94086-4828, United States of America","Bachelor's degree in data science , computer science, statistics, operation research, or related fields; 6 years' experience of building data pipelines, extracting signals from noisy data, and establishing metrics for monitoring; Proficiency in data analysis tools (e.g., as Python, R, SQL) and data visualization tools (e.g., Tableau, Superset, Looker ) ; Analytical thinking and detail-oriented mindset; Familiar with S QL and N on- SQL database s; String communication skills; Ability to convey complex ideas and findings to non-technical stakeholders ; Knowledge of A/B testing; Option 1: Bachelor's degree in Computer Science and 5 years' experience in software engineering or related field; Option 2: 7 years' experience in software engineering or related field; Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field; 4 years' experience in data engineering, database engineering, business intelligence, or business analytics; Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly; The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture; 11 more items(s)","Benefits: Beyond our great compensation package, you can receive incentive awards for your performance; Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO , multiple health plans, and much more; At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet; Health benefits include medical, vision and dental coverage; Financial benefits include 401(k), stock purchase and company-paid life insurance; Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting; Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more; You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes; The amount you receive depends on your job classification and length of employment; It will meet or exceed the requirements of paid sick leave laws, where applicable; Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities; Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates; Tuition, books, and fees are completely paid for by Walmart; Benefits are subject to change and may be subject to a specific plan or program terms; Bellevue, Washington US-11075:The annual salary range for this position is $132,000.00-$264,000.00; SUNNYVALE, California US-04396:The annual salary range for this position is $143,000.00-$286,000.00; Additional compensation includes annual or quarterly performance bonuses; 14 more items(s)","Python, R, SQL, Tableau, Superset, Looker"
Data Engineer - Messaging Data Platform,Stripe,"Stripe • San Francisco, CA •  via No Whiteboard",Full-time,No Degree Mentioned,"edented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.About the teamThe Financial Data team is the single source of truth of financial activity at Stripe. Our mission is to ensure the billions of monetary transactions happening on the platform clear and move between customers as expected. Internal partner team use our systems and datasets to create new products, close accounting books, generate financial statements, produce compliance reports, and much more. We measure and ensure the quality of money movement by using ML models and predictive alerts. Our financial events are predictable, complete, accurate, timely and reliably delivered.If you are a motivated leader that is excited to work on some of the largest financial datasets in the world, the Financial Data team will provide opportunities to deliver significant impact while growing the GDP of the internet. Our team is composed of a wide range of experience from new graduates to experienced developers with strong finance backgrounds. We want a leader that can be hands-on and have the ability to create the future.What you’ll do• Lead a team of engineers to solve the challenges of financial data ingestion, analysis, monitoring, and repair.• Work with Stripe leaders across the company to drive the vision and direction of Stripe’s Financial Data systems.• Build the next generation of Financial Data platforms.• Define and grow your team’s charter with Stripe’s business growth.• Understand user needs and user pain points to prioritize engineering work and deliver high quality solutions to meet user needs.• Drive the roadmap and priorities for your team, and resolve dependencies across the company.• Be deeply involved in understanding and operationalizing revenue and pricing models for new product and market launches.• Support the engineering team in achieving a high level of technical excellence and stability.• Manage processes to help the team do its best work.• Develop engineers on the team and align customer needs with opportunities for them to advance in their careers.• Recruit great engineers, in collaboration with Stripe’s recruiting team.• Contribute to engineering-wide initiatives as a member of Stripe’s engineering management team.Who you are• 3+ years of engineering management experience• Managed team that have shipped and operated critical infrastructure and/or products.• Outstanding technical and people management skills to manage and grow a set of highly talented engineers.• Experienced in navigating high-stake situations. You’ll have significant responsibility in creating Stripe’s success.• Work effectively cross-team and cross-function, and are able to think rigorously and make hard decisions and tradeoffs.• Hands-on experience in solving large scale and difficult problems.• The desire to encourage a healthy work environment that’s both supportive and challenging.Preferred qualifications• Managed team that have shipped and operated data pipelines.• The ability to thrive on a high level of ambiguity, autonomy and responsibility.• A genuine enjoyment of learning and diving into the nuts-and-bolts of how things work — in this role, you’ll learn a ton about product pricing and management, currency conversion, intercompany flows, finance and accounting, and many more topics.","We want a leader that can be hands-on and have the ability to create the future; 3+ years of engineering management experience; Managed team that have shipped and operated critical infrastructure and/or products; Outstanding technical and people management skills to manage and grow a set of highly talented engineers; Experienced in navigating high-stake situations; You’ll have significant responsibility in creating Stripe’s success; Work effectively cross-team and cross-function, and are able to think rigorously and make hard decisions and tradeoffs; Hands-on experience in solving large scale and difficult problems; The desire to encourage a healthy work environment that’s both supportive and challenging; 6 more items(s)","Our mission is to ensure the billions of monetary transactions happening on the platform clear and move between customers as expected; Internal partner team use our systems and datasets to create new products, close accounting books, generate financial statements, produce compliance reports, and much more; We measure and ensure the quality of money movement by using ML models and predictive alerts; Lead a team of engineers to solve the challenges of financial data ingestion, analysis, monitoring, and repair; Work with Stripe leaders across the company to drive the vision and direction of Stripe’s Financial Data systems; Build the next generation of Financial Data platforms; Define and grow your team’s charter with Stripe’s business growth; Understand user needs and user pain points to prioritize engineering work and deliver high quality solutions to meet user needs; Drive the roadmap and priorities for your team, and resolve dependencies across the company; Be deeply involved in understanding and operationalizing revenue and pricing models for new product and market launches; Support the engineering team in achieving a high level of technical excellence and stability; Manage processes to help the team do its best work; Develop engineers on the team and align customer needs with opportunities for them to advance in their careers; Recruit great engineers, in collaboration with Stripe’s recruiting team; Contribute to engineering-wide initiatives as a member of Stripe’s engineering management team; 12 more items(s)","ML models, predictive alerts"
Senior Data Engineer - BPR Insight,Twilio,"Twilio • San Francisco, CA •  via JobzMall",5 days ago,120K–150K a year,"our customers. If you are passionate about data and have a strong background in data engineering, we encourage you to apply for this exciting opportunity.Design and develop data pipelines to efficiently process and transform large volumes of messaging data.Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business needs.Implement data quality checks and monitoring processes to ensure the accuracy and reliability of messaging data.Optimize and maintain existing data infrastructure to improve data processing performance and reduce costs.Troubleshoot and resolve data-related issues in a timely manner to minimize impact on business operations.Continuously monitor and improve data architecture, ensuring scalability and efficiency of the messaging data platform.Stay updated with the latest data engineering technologies and trends to propose and implement new solutions.Work closely with data analysts and data scientists to support their data needs and enable them to extract insights from messaging data.Document data engineering processes and procedures to maintain a centralized knowledge base for the team.Participate in code reviews and provide constructive feedback to ensure high-quality, maintainable data pipelines.Collaborate with DevOps and security teams to ensure data privacy and compliance with industry standards.Communicate effectively with team members and stakeholders to provide updates on data engineering projects and address any concerns.Take ownership of data engineering projects from conception to deployment, ensuring timely delivery and meeting project objectives.Mentor and coach junior members of the data engineering team, fostering a culture of continuous learning and growth.Twilio is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.","As a Data Engineer, you will play a crucial role in ensuring the accuracy, reliability, and efficiency of our messaging data, enabling us to provide top-notch services to our customers; Design and develop data pipelines to efficiently process and transform large volumes of messaging data; Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business needs; Implement data quality checks and monitoring processes to ensure the accuracy and reliability of messaging data; Optimize and maintain existing data infrastructure to improve data processing performance and reduce costs; Troubleshoot and resolve data-related issues in a timely manner to minimize impact on business operations; Continuously monitor and improve data architecture, ensuring scalability and efficiency of the messaging data platform; Stay updated with the latest data engineering technologies and trends to propose and implement new solutions; Work closely with data analysts and data scientists to support their data needs and enable them to extract insights from messaging data; Document data engineering processes and procedures to maintain a centralized knowledge base for the team; Participate in code reviews and provide constructive feedback to ensure high-quality, maintainable data pipelines; Collaborate with DevOps and security teams to ensure data privacy and compliance with industry standards; Communicate effectively with team members and stakeholders to provide updates on data engineering projects and address any concerns; Take ownership of data engineering projects from conception to deployment, ensuring timely delivery and meeting project objectives; Mentor and coach junior members of the data engineering team, fostering a culture of continuous learning and growth; 12 more items(s)",,Twilio
"Data Engineer, Data Platform (Contract)",iTvorks Inc,"iTvorks Inc • San Francisco, CA •  via OPTnation",60K–65K a year,Contractor,by Perl Bash) Experience with workflow management tools (Airflow and Oozie) Comfortable working directly with data analytics to bridge business requirements with data engineering.,Data Engineer (8+ years) Location SFO CA Duration Long Term; Experience & Skills Extensive experience with Hadoop (or similar) Ecosystem (Map Reduce HDFS Hive Spark Pig HBase) Proficient in at least one of the SQL languages (MySQL PostgreSQL SqlServer Oracle) Good understanding of SQL Engine and able to conduct advanced performance tuning Strong skills in the scripting language (Python Ruby Perl Bash) Experience with workflow management tools (Airflow and Oozie) Comfortable working directly with data analytics to bridge business requirements with data engineering,,"Perl, Bash, Airflow, Oozie, Hadoop, Map Reduce, HDFS, Hive, Spark, Pig, HBase, MySQL, PostgreSQL, SqlServer, Oracle, Python, Ruby"
Bioinformatics Data Engineer,Pagoda,"Pagoda • San Francisco, CA •  via Karkidi","12,395–14,583 a month",Full-time,"ists to create data marts for various data products;• Creating data extraction tools using Python, JavaScript, SQL, and Rust;• Collaborate with internal and external engineers and product managers.What We're Looking For• Experience building and managing data lakes aggregating dozens of data sources and providing insights to multiple different stakeholders based on terabytes of data;• Experience in GCP and/or AWS data infrastructure products;• Fluency in writing complex analytical SQL queries;• Strong communication and remote friendly working skills;• Bachelor’s Degree in Computer Science, Applied Mathematics or related field is a must.We'd Love If You Have• Deep understanding of DataBricks and BigQuery technologies;• Knowledge of product analytics tools such as Segment, FullStory, MixPanel or Amplitude;• Familiarity with crypto or blockchain technologies;• Experience working at a startup.Here’s What Our Interview Process Looks LikeOur interviews take place via Zoom and typically consists of the following stages:• Internal Recruiter Call (30 to 45 minutes)• Technical Interviews (2 x 60 minutes)• Pagoda Values Interview (30 to 45 minutes)Compensation• *This role is a 6 month contract with an opportunity for full time conversion depending on performance and business needs. Contractors will not receive any of the full time benefits shown below.Pay Rate: $12,395/month - $14,583/month, which represents the cash payment range per month applicable to US locations only.The actual pay rate within the range is dependent upon many factors, including: leveling, relevant skills, and work location. If you are based outside of the US, we there are other geographic considerations that may impact your final compensation. Your recruiter can share more about the compensation and benefits applicable to your preferred location during the hiring process.Benefits & Perks• Flexible Annual Leave / PTO with an encouraged 20 day per year minimum• Paid Holiday Week: the last week of the year• Paid Wellness Week: the first week of July• $2,000 Yearly Continued Education Reimbursement• $2,000 Home Office Setup Reimbursement• Co-working Space Reimbursement• Company Retreats (2023 in Spain!) & Team Offsites• Mental Health Support and access to licensed therapists through Spill, 100% paid by PagodaOur Values at PagodaOur values express our company culture. Learn more on our careers page.Pagoda is an Equal Employment Opportunity (EEO) employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.","Experience building and managing data lakes aggregating dozens of data sources and providing insights to multiple different stakeholders based on terabytes of data;; Experience in GCP and/or AWS data infrastructure products;; Fluency in writing complex analytical SQL queries;; Strong communication and remote friendly working skills;; Bachelor’s Degree in Computer Science, Applied Mathematics or related field is a must; Deep understanding of DataBricks and BigQuery technologies;; Knowledge of product analytics tools such as Segment, FullStory, MixPanel or Amplitude;; Familiarity with crypto or blockchain technologies;; Experience working at a startup; Technical Interviews (2 x 60 minutes); 7 more items(s)","*This role is a 6 month contract with an opportunity for full time conversion depending on performance and business needs; Pay Rate: $12,395/month - $14,583/month, which represents the cash payment range per month applicable to US locations only; The actual pay rate within the range is dependent upon many factors, including: leveling, relevant skills, and work location; If you are based outside of the US, we there are other geographic considerations that may impact your final compensation; Your recruiter can share more about the compensation and benefits applicable to your preferred location during the hiring process; Benefits & Perks; Flexible Annual Leave / PTO with an encouraged 20 day per year minimum; Paid Holiday Week: the last week of the year; Paid Wellness Week: the first week of July; $2,000 Yearly Continued Education Reimbursement; $2,000 Home Office Setup Reimbursement; Co-working Space Reimbursement; Company Retreats (2023 in Spain!); Mental Health Support and access to licensed therapists through Spill, 100% paid by Pagoda; 11 more items(s)","Python, JavaScript, SQL, Rust, GCP, AWS, DataBricks, BigQuery, Segment, FullStory, MixPanel, Amplitude"
Software Engineer - Data Platform - New College Grad,Revolution Medicines,"Revolution Medicines • Redwood City, CA •  via VentureLoop",11 days ago,Full-time,"icines team, you will join other outstanding professionals in a tireless commitment to patients with cancers harboring mutations in the RAS signaling pathway.The Opportunity:In this role, you will work within our Translational Medicine group transferring, organizing, curating and disseminating biomarker data from clinical trial samples. This incoming data consists of a diverse mix of next-generation sequencing, downstream variant calls, imaging, and other molecular assay outputs. The bioinformatics data engineer will work on various projects in different phases of drug development and contribute to the cross-functional bioinformatics team which includes other bioinformaticians and data scientists. As a Bioinformatics Data Engineer in Translational Medicine, you will:• Act as a main liaison between internal scientists and external molecular diagnostic providers.• Be the primary point of contact for the data transfer process, establishing connections between our cloud storage and external file shares.• Help develop scripts and processes to clean, upload and accession biomarker data from clinical trails into internal systems for further analysis.• Carry out data modeling and curation to ensure data quality and relevance.• Collaborate with other departments to share and disseminate data in a controlled and documented manner.• Develop web-based data visualization dashboards using python and R frameworks.• Use and modify data analysis pipelines using nextflow and WDL.Required Experience, Skills, and Education:• Master’s or bachelor’s degree with 3+ years’ experience in biology/computer science related field.• Expertise using linux clusters, cloud computing and file systems.• Proficient in designing and querying databases using SQL and object-relational mapping techniques.• Skilled in utilizing Python and R for data manipulation and analysis.• Strong familiarity with bioinformatics data types and outputs.• Excellent communication skills, enabling effective collaboration and conveying technical information clearly to diverse audiences.Preferred Experience, Skills, and Education:• Experience using visualization packages such as R Shiny, PowerBI or similar systems.• Experience in biomarker data analysis and bioinformatics.• Familiarity with large genomic data sets (GDC, TCGA, etc.).• Experience in tools to analyze cancer genomics datasets (GATK, Salmon, OncoKB, VEP, etc.).• Development experience in groups using IDE’s, a ticketing system and software control systems (GIT).The expected salary range for this role is $120,000 to $145,000. An individual’s position within the range may be influenced by multiple factors, including skills and experience in role, overall performance, individual impact and contributions, tenure, and market dynamics. Base salary is one part of the overall total rewards program at RevMed, which includes competitive cash compensation, robust equity awards, strong benefits, and significant learning and development opportunities.Revolution Medicines is an equal opportunity employer and prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity/expression, national origin/ancestry, age, disability, marital status, medical condition, and veteran status.Revolution Medicines takes protection and security of personal data very seriously and respects your right to privacy while using our website and when contacting us by email or phone. We will only collect, process and use any personal data that you provide to us in accordance with our CCPA Notice and Privacy Policy. For additional information, please contact privacy@revmed.com.","Master’s or bachelor’s degree with 3+ years’ experience in biology/computer science related field; Expertise using linux clusters, cloud computing and file systems; Proficient in designing and querying databases using SQL and object-relational mapping techniques; Skilled in utilizing Python and R for data manipulation and analysis; Strong familiarity with bioinformatics data types and outputs; Excellent communication skills, enabling effective collaboration and conveying technical information clearly to diverse audiences; 3 more items(s)","The expected salary range for this role is $120,000 to $145,000; Base salary is one part of the overall total rewards program at RevMed, which includes competitive cash compensation, robust equity awards, strong benefits, and significant learning and development opportunities","Python, R, SQL, Linux clusters, cloud computing, nextflow, WDL, R Shiny, PowerBI, GATK, Salmon, OncoKB, VEP, GIT"
"Data Engineer, Product Analytics",Wealthfront,"Wealthfront • Palo Alto, CA •  via Powderkeg",Full-time,,"lthfront Data Engineer, you will be challenged to help build a cloud-native Data Analytics Platform that securely manages our clients’ financial data. Most data platforms use data for reporting and ad hoc analysis. We push the boundaries of real-time decisioning using ML and optimization within our product experiences. Optimized cost to serve, high throughput, low latency insights are new challenges to most data engineers today. As part of an agile team in a startup, you will have input, impact and ownership in the entire process rather than be part of a large company assembly line.Responsibilities:• Help implement data platform capabilities and patterns• Understand and anticipate stakeholder needs and translate needs into deliverable data platform components• Design and develop reusable components and frameworks for ingestion, cleansing, data quality, model building and execution• Learn and master an agile methodology in the software development process• Be an integral part of our scrum team to deliver on commitments on time and with good qualityRequirements:• Pursuing a Bachelor's degree in Computer Science, Engineering or a related field• Knowledge in object-oriented and/or functional programming languages (e.g. Java/Scala, Python)• Familiarity with Distributed data technologies like Hadoop, Kafka, NoSql etc• Excellent written and verbal communication and social skills; able to effectively collaborate with partners• Detail oriented. Strong prioritization skills and sense of urgency• Willing to constantly learn and grow• Love to learn and experiment with new technologies and shares findings with the team• Effective team player. Honest and respectful of others• Graduating in Spring 2022Everyone across the financial spectrum deserves to live secure and rewarding lives. In order to successfully serve clients across the United States, the Wealthfront team is focused on hiring team members with a diverse range of backgrounds, experiences and perspectives. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.About WealthfrontWealthfront started with the ambition to transform the investment advisory business, with the goal to unlock access to high quality investment advice for millions who were underserved by the traditional institutions. We built the first automated investment product that allows you to invest in a personalized portfolio of thousands of companies in seconds for a remarkably low fee; we then expanded into banking which made it remarkably easy for people to automate their finances end-to-end and eliminated the hassle of money management, all of which resulted in attracting more than $27 billion of our client’s hard earned savings, created the robo-advisor category and transformed the broader industry. And yet, we have a long way to go to achieve our mission to build a financial system that favors people, not institutions.Wealthfront’s vision is to make it delightfully easy to build long-term wealth on your own terms. This vision is more relevant than ever because millions more people are getting into the market early and investing their hard earned savings in a handful of stocks. While this is a great way to start, it is inconsistent with building long-term wealth. We want to empower young investors to expand their horizon, and easily explore and execute on a wider range of investing strategies, make informed investment decisions that are consistent with their values and beliefs while also making it effortless to grow and compound their savings exponentially, that’s transformational to their lives and their long-term future.","Pursuing a Bachelor's degree in Computer Science, Engineering or a related field; Knowledge in object-oriented and/or functional programming languages (e.g. Java/Scala, Python); Familiarity with Distributed data technologies like Hadoop, Kafka, NoSql etc; Excellent written and verbal communication and social skills; able to effectively collaborate with partners; Detail oriented; Strong prioritization skills and sense of urgency; Willing to constantly learn and grow; Love to learn and experiment with new technologies and shares findings with the team; Effective team player; Honest and respectful of others; Graduating in Spring 2022; 8 more items(s)","As a Wealthfront Data Engineer, you will be challenged to help build a cloud-native Data Analytics Platform that securely manages our clients’ financial data; Most data platforms use data for reporting and ad hoc analysis; Optimized cost to serve, high throughput, low latency insights are new challenges to most data engineers today; As part of an agile team in a startup, you will have input, impact and ownership in the entire process rather than be part of a large company assembly line; Help implement data platform capabilities and patterns; Understand and anticipate stakeholder needs and translate needs into deliverable data platform components; Design and develop reusable components and frameworks for ingestion, cleansing, data quality, model building and execution; Learn and master an agile methodology in the software development process; Be an integral part of our scrum team to deliver on commitments on time and with good quality; 6 more items(s)","Java, Scala, Python, Hadoop, Kafka, NoSql"
Sr./Staff Data Engineer,Meta,"Meta • Sunnyvale, CA •  via The Muse",Full-time,Health insurance,"ith some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few companies can match.Data Engineer, Product Analytics Responsibilities:• Manage and execute data warehouse plans for a product or a group of products to solve well-scoped problems• Identify the data needed for a business problem and implement logging required to ensure availability of data, while working with data infrastructure to triage issues and resolve• Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights in a meaningful way• Build data expertise and leverage data controls to ensure privacy, security, compliance, data quality, and operations for allocated areas of ownership• Design, build and launch new data models and visualizations in production, leveraging common development toolkits• Independently design, build and launch new data extraction, transformation and loading processes in production, mentoring others around efficient queries• Support existing processes running in production and implement optimized solutions with limited guidance• Define and manage SLA for data sets in allocated areas of ownershipMinimum Qualifications:• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.• 2+ years of work experience in data engineering• Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)Preferred Qualifications:• Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy• Experience working with terabyte to petabyte scale dataAbout Meta:Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today-beyond the constraints of screens, the limits of distance, and even the rules of physics.Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.$109,000/year to $171,000/year + bonus + equity + benefitsIndividual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about benefits at Meta.","Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience; 2+ years of work experience in data engineering; Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.); Meta participates in the E-Verify program in certain locations, as required by law; 1 more items(s)","$109,000/year to $171,000/year + bonus + equity + benefits; Individual compensation is determined by skills, qualifications, experience, and location; Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable; In addition to base compensation, Meta offers benefits; 1 more items(s)","SQL, ETL, Python, C++, C#, Scala"
Data Engineer II,Machinify,"Machinify • Palo Alto, CA •  via Jobs",Full-time,No Degree Mentioned,"loy, at light speed, industry-specific products that increase the speed and accuracy of claims processing by orders of magnitude.We are seeking a Sr./Staff Data Engineer to build and own critical data pipelines.What you’ll do:• Independently understand all aspects of a business problem including those unrelated to their area of expertise, weigh pros and cons of different approaches and suggest ones likely to succeed• Work with a cross-functional organization including engineering, delivering, subject-matter experts, product managers, as well as platform engineers to deliver a scalable framework.• Map the customer data into Machinify canonical form. Identify and ingest non canonical fields and generalize the process to a minimal level of customization.• Proactively design and adapt the canonical form to suit changing query patterns and needs.• Ultimately own data availability and quality for the Data Science organization.What You Bring:• Deep experience as a hands-on Data Engineer building production data pipelines• Experience managing the delivery of complex data• Experience in ETL orchestration and workflow management tools with a strong preference for Apache Airflow• Experience in Spark or other distributed computing frameworks• SQL and Python• Advanced SQL performance tuning• Kubernetes and building Docker images• AWS & GCP• Experience working with APIs to collect or ingest data• Manage SLA for all pipelines in allocated areas of ownership• Streaming technologies like kafka , spark streaming etc• ELK stack , Grafana etcThe base salary for this position will vary based on an array of factors unique to each candidate such as qualifications, years and depth of experience, skill set, certifications, etc. The base salary range for this role is $200k-250k. We are hiring for different seniorities, and our Recruiting team will let you know if you qualify for a different role/range. Salary is one component of the total compensation package, which includes meaningful equity, excellent healthcare, flexible time off, and other benefits and perks.Equal Employment Opportunity at MachinifyMachinify is committed to hiring talented and qualified individuals with diverse backgrounds for all of its positions. Machinify believes that the gathering and celebration of unique backgrounds, qualities, and cultures enriches the workplace.",Deep experience as a hands-on Data Engineer building production data pipelines; Experience managing the delivery of complex data; Experience in ETL orchestration and workflow management tools with a strong preference for Apache Airflow; Experience in Spark or other distributed computing frameworks; SQL and Python; Advanced SQL performance tuning; Kubernetes and building Docker images; AWS & GCP; Experience working with APIs to collect or ingest data; 6 more items(s),"The base salary for this position will vary based on an array of factors unique to each candidate such as qualifications, years and depth of experience, skill set, certifications, etc; The base salary range for this role is $200k-250k; Salary is one component of the total compensation package, which includes meaningful equity, excellent healthcare, flexible time off, and other benefits and perks","Apache Airflow, Spark, SQL, Python, Kubernetes, Docker, AWS, GCP, Kafka, Spark Streaming, ELK stack, Grafana"
Senior Analytics Engineer,ZoomInfo,"ZoomInfo • San Mateo, CA •  via Ladders",95.2K–131K a year,Full-time,"lidated data lake or data warehouse (Snowflake)• Consolidate/join datasets to create easily consumable, performant, and consistent information• Look for ways to improve processes and take initiative to implement them• Evaluate new technology and advise on our data lake ecosystem• Collaborate with cross-functional teams to design and implement impactful solutions to department and business problems• Creates and maintains documentation for business end users and other data analysts.• Determine where in our infrastructure we should house our data based on the use case and data model.What you'll bring:• Degree (Masters preferred) in Computer Science, Information Systems, Data Science, or related field and 4+ years of experience in data engineering, or an equivalent combination of education and experience.• Expert and Hands-On development in distributed computing environment using Hadoop, Scala/Java, Spark and Apache Airflow• SQL knowledge and understanding• Experience architecting solutions in collaboration with engineering development and data analysts team.• Experience working with third party APIs for data collection• Self-motivated; able to work independently to complete tasks and collaborate with others to identify and implement solutions.• Expertise and Hand-On working knowledge of AWS• Experience with Git/Github or other version control systems• Familiarity with Snowflake or DataBricks a plus.#LI-SK#LI-HybridActual compensation offered will be based on factors such as the candidate's work location, qualifications, skills, experience and/or training. Your recruiter can share more information about the specific salary range for your desired work location during the hiring process. We want our employees and their families to thrive.In addition to comprehensive benefits we offer holistic mind, body and lifestyle programs designed for overall well-being. Learn more about ZoomInfo benefits here.Below is the US base salary for this position. Additional compensation such as Bonus, Commission, Equity and other benefits may also apply.$95,200-$130,900 USDAbout us:ZoomInfo (NASDAQ: ZI) is the trusted go-to-market platform for businesses to find, acquire, and grow their customers. It delivers accurate, real-time data, insights, and technology to more than 35,000 companies worldwide. Businesses use ZoomInfo to increase efficiency, consolidate technology stacks, and align their sales and marketing teams - all in one platform.ZoomInfo may use a software-based assessment as part of the recruitment process. More information about this tool, including the results of the most recent bias audit, is available here.ZoomInfo is proud to be an Equal Opportunity Employer. We are committed to equal employment opportunities for applicants and employees regardless of sex, race, age, color, national origin, sexual orientation, gender identity, marital status, disability status, religion, protected military or veteran status, medical condition, or any other characteristic or status protected by applicable law. At ZoomInfo, we also consider qualified candidates with criminal histories, consistent with legal requirements.","Expert and Hands-On development in distributed computing environment using Hadoop, Scala/Java, Spark and Apache Airflow; SQL knowledge and understanding; Experience architecting solutions in collaboration with engineering development and data analysts team; Experience working with third party APIs for data collection; Self-motivated; able to work independently to complete tasks and collaborate with others to identify and implement solutions; Expertise and Hand-On working knowledge of AWS; Experience with Git/Github or other version control systems; 4 more items(s)","Your recruiter can share more information about the specific salary range for your desired work location during the hiring process; In addition to comprehensive benefits we offer holistic mind, body and lifestyle programs designed for overall well-being; Below is the US base salary for this position; Additional compensation such as Bonus, Commission, Equity and other benefits may also apply; $95,200-$130,900 USD; 2 more items(s)","Snowflake, Hadoop, Scala, Java, Spark, Apache Airflow, SQL, AWS, Git, Github, DataBricks"
"Senior Director, Data Engineering",Upstart,"Upstart • San Mateo, CA •  via Upstart Careers",Full-time,No Degree Mentioned,"demand. More than two-thirds of Upstart loans are approved instantly and are fully automated.Upstart is a digital-first company, which means that most Upstarters live and work anywhere in the United States. However, we also have offices in San Mateo, California; Columbus, Ohio; and Austin, Texas.Most Upstarters join us because they connect with our mission of enabling access to effortless credit based on true risk. If you are energized by the impact you can make at Upstart, we’d love to hear from you!The TeamAs a Senior Analytics Engineer at Upstart, you will design and develop robust and scalable data models for analytical stakeholders to empower them to retrieve meaningful insights. Our analytics engineering team is centralized within the company and partners closely with data engineering, data platform, software engineers, ML engineers and cross functional analytics squads to stitch together datasets to create data models that are ready for consumption.In addition to architecting data models, we implement Analytics Engineering best practices around Data Governance, Data Quality, Data Orchestration and pipeline optimization. We enable analysts to think like software engineers by defining, documenting and ensuring adoption of best practices when making contributions to the analytical code-base.Position Location - This role is available in the following locations: Remote, San Mateo, ColumbusTime Zone Requirements - The team operates in all US time zones.Travel Requirements - This team has regular on-site collaboration sessions. These occur 3 days per quarter at an Upstart office. If you need to travel to make these meetups, Upstart will cover all travel related expenses.How you’ll make an impact:• Understand how data is produced and consumed at a deep level - you will need to be extremely collaborative with the teams that produce and consume the data to create an end-to-end solution that maximizes the value of our data• Lead efforts to design data models and build curated data sets that are the foundation of reporting and analytics at Upstart• Collaborate with data engineers and data platform teams to create analytics pipelines• Meet consumers of data where they are by wearing an educator’s hat and training them on how to use BI tools and dashboards• Be a close strategic partner to analytics squads to participate in decision making on the analytics road map for UpstartWhat we’re looking for:• Minimum requirements:• 5+ year(s) of experience as a Data Engineer / Analytics Engineer / BI Engineer• Strong understanding of data modeling concepts in both transactional and analytical databases• Proven ability to lead cross-team projects to create data models for analytics/reporting purposes• Excellent communication and collaboration skills, particularly when explaining technical or complex matters to less technical co-workers• Expertise in SQL, Python, and ETL optimization techniques• Preferred qualifications:• Familiarity with business intelligence visualization tools such as Mode, Looker, Tableau, Power BI, etc.• Experience creating curated data models using a data mesh architecture• Experience with cloud computing platforms such as Amazon Web Services (AWS), Azure and/or Google Cloud• Experience with building Looker data models (LookML), analytics engineering frameworks like dbt, orchestration tools like Airflow, schema design, and dimensional data modeling• Thorough understanding of data lake/warehouse architectures (BigQuery, Databricks, Redshift)What you'll love:• Competitive Compensation (base + bonus & equity)• Comprehensive medical, dental, and vision coverage with Health Savings Account contributions from Upstart• 401(k) with 100% company match up to $4,500 and immediate vesting and after-tax savings• Employee Stock Purchase Plan (ESPP)• Life and disability insurance• Generous holiday, vacation, sick and safety leave• Supportive parental, family care, and military leave programs• Annual wellness, technology & ergonomic reimbursement programs• Social activities including team events and onsites, all-company updates, employee resource groups (ERGs), and other interest groups such as book clubs, fitness, investing, and volunteering• Catered lunches + snacks & drinks when working in offices#LI-REMOTE#LI-MidSeniorAt Upstart, your base pay is one part of your total compensation package. The anticipated base salary for this position is expected to be within the below range. Your actual base pay will depend on your geographic location–with our “digital first” philosophy, Upstart uses compensation regions that vary depending on location. Individual pay is also determined by job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.In addition, Upstart provides employees with target bonuses, equity compensation, and generous benefits packages (including medical, dental, vision, and 401k).United States | Remote - Anticipated Base Salary Range$160,400—$222,000 USDUpstart is a proud Equal Opportunity Employer. We are dedicated to ensuring that underrepresented classes receive better access to affordable credit, and are just as committed to embracing diversity and inclusion in our hiring practices. We celebrate all cultures, backgrounds, perspectives, and experiences, and know that we can only become better together.If you require reasonable accommodation in completing an application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please email candidate_accommodations@upstart.comhttps://www.upstart.com/candidate_privacy_policy","5+ year(s) of experience as a Data Engineer / Analytics Engineer / BI Engineer; Strong understanding of data modeling concepts in both transactional and analytical databases; Proven ability to lead cross-team projects to create data models for analytics/reporting purposes; Excellent communication and collaboration skills, particularly when explaining technical or complex matters to less technical co-workers; Expertise in SQL, Python, and ETL optimization techniques; Familiarity with business intelligence visualization tools such as Mode, Looker, Tableau, Power BI, etc; Experience creating curated data models using a data mesh architecture; Experience with cloud computing platforms such as Amazon Web Services (AWS), Azure and/or Google Cloud; Experience with building Looker data models (LookML), analytics engineering frameworks like dbt, orchestration tools like Airflow, schema design, and dimensional data modeling; Thorough understanding of data lake/warehouse architectures (BigQuery, Databricks, Redshift); 7 more items(s)","Competitive Compensation (base + bonus & equity); Comprehensive medical, dental, and vision coverage with Health Savings Account contributions from Upstart; 401(k) with 100% company match up to $4,500 and immediate vesting and after-tax savings; Employee Stock Purchase Plan (ESPP); Life and disability insurance; Generous holiday, vacation, sick and safety leave; Supportive parental, family care, and military leave programs; Annual wellness, technology & ergonomic reimbursement programs; Social activities including team events and onsites, all-company updates, employee resource groups (ERGs), and other interest groups such as book clubs, fitness, investing, and volunteering; Catered lunches + snacks & drinks when working in offices; At Upstart, your base pay is one part of your total compensation package; The anticipated base salary for this position is expected to be within the below range; Your actual base pay will depend on your geographic location–with our “digital first” philosophy, Upstart uses compensation regions that vary depending on location; Individual pay is also determined by job-related skills, experience, and relevant education or training; Your recruiter can share more about the specific salary range for your preferred location during the hiring process; In addition, Upstart provides employees with target bonuses, equity compensation, and generous benefits packages (including medical, dental, vision, and 401k); United States | Remote - Anticipated Base Salary Range; $160,400—$222,000 USD; 15 more items(s)","SQL, Python, Mode, Looker, Tableau, Power BI, Amazon Web Services (AWS), Azure, Google Cloud, LookML, dbt, Airflow, BigQuery, Databricks, Redshift"
"Data Engineer, Fleet Analytics",GoFundMe,"GoFundMe • San Francisco, CA •  via ZipRecruiter",Full-time,Dental insurance,"software company that enables nonprofits to connect supporters with the causes they care about. Together, GoFundMe and Classy have empowered people and organizations to raise more than $25 billion since 2010. Our vision is to become the most helpful place in the world.Join us in this pivotal role and be a key player in driving data-driven decision-making and innovation within the GoFundMe/Classy organization.We are looking for an experienced and technically skilled individual to lead the team that provides data engineering and analytics functionality for the entire GoFundMe/Classy organization.The ideal candidate will be an analytical, results-oriented, self-motivated, and customer-focused data engineering leader who will play a key role in continuous improvement for the critical systems that enable our entire business. The role will also include surfacing key product metrics as well as building critical data pipelines and analytics tools to improve the efficiency and quality of strategic business decisions.Candidates considered for this role will be located in the San Francisco Bay Area.The Job...• Collaborate with stakeholders across the organization to understand their data needs and deliver actionable insights.• Translate product requirements into engineering work for your team, partnering closely with peers in Product and other disciplines - to make our vision a reality• Have direct ownership to grow a strategic and emerging part of GoFundMe business to address new and expanding data and AI product opportunities.• Lead and manage a team of data engineers and analysts to deliver high-quality data solutions.• Oversee the design, development, and maintenance of data pipelines and data warehousing solutions.• Develop and maintain key product metrics to support business objectives.• Implement best practices for data management, including data governance, quality, and security.• Drive continuous improvement in data engineering practices and processes.• Mentor and develop team members, fostering a culture of innovation and excellence.You...• 7+ years industry experience designing and building distributed data systems,• 5+ years of experience supporting or managing a business intelligence or analytics team, including employee development and performance management.• A track record of recruiting, leading, and growing high performing technical teams in a demanding talent market• Strong proficiency in SQL and ETL processes.• Modern data architectures including Data Lake, Data Mesh• Experience with data visualization tools such as Tableau, Looker, Quicksight, or similar.• Proficiency in statistical/machine learning software such as R, Python, SAS, or Matlab.• Completed bachelor's degree in Computer Science, Data Science, Statistics, or a related field.Preferred Qualifications:• 5+ years of experience working with very large data warehousing environments, ideally within a large technology company.• 10+ years of experience in data warehouse technical architectures, data modeling, infrastructure components, ETL/ELT processes, and reporting/analytic tools.• Hands-on experience with data structures and SQL coding.• Marketing tech/ad tech related experience is a strong plus• Advanced degree (Master's or Ph.D.) in a relevant field.• Strong understanding of cloud-based data solutions (e.g., Snowflake, AWS, Azure, Google Cloud).• Experience with big data technologies such as Hadoop, Spark, and Kafka.• Proven ability to translate complex data into actionable insights for non-technical stakeholders.• Excellent communication skills, both written and verbal, with the ability to present complex information clearly and concisely.• Demonstrated ability to drive projects from concept to completion with minimal guidance.Why you'll love it here...• Market competitive pay.• Rich healthcare benefits including employer paid premiums for medical/dental/vision (100% for employee-only plans and 85% for employee + dependent plans) and employer HSA contributions.• 401(k) retirement plan with company matching.• Hybrid workplace with fully remote flexibility for many roles.• Monetary support for new hire setup, hybrid work & wellbeing, family planning, and commuting expenses.• A variety of mental and wellness programs to support employees.• Generous paid parental leave and family planning stipend.• Supportive time off policies including vacation, sick/mental health days, volunteer days, company holidays, and a floating holiday.• Learning & development and recognition programs.• ""Gives Back"" Program where employees can nominate a fundraiser every week for a donation from the company.• Inclusion, diversity, equity, and belonging are vital to our priorities and we continue to evolve our strategy to ensure DEI is embedded in all processes and programs at GoFundMe. Our Diversity, Equity, and Inclusion team is always finding new ways for our company to uphold and represent the experiences of all of the people in our organization.• Employee resource groups.• Your work has a real purpose and will help change lives on a global scale.• You'll be a part of a fun, supportive team that works hard and celebrates accomplishments together.• We live by our core values: impatient to be great, find a way, earn trust every day, fueled by purpose.• We are a certified Great Place to Work, are growing fast and have incredible opportunities ahead!GoFundMe is proud to be an equal opportunity employer that actively pursues candidates of diverse backgrounds and experiences. We are committed to providing diversity, equity, and inclusion training to all employees, and we do not discriminate on the basis of race, color, religion, ethnicity, nationality or national origin, sex, sexual orientation, gender, gender identity or expression, pregnancy status, marital status, age, medical condition, mental or physical disability, or military or veteran status.The total annual salary for this full-time position is $300,000-$410,000 + equity + benefits. The salary range was determined by role, level, and location requirement the US. Individual pay is determined by work location and additional factors including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range based on your location during the hiring process.If you require a reasonable accommodation to complete a job application or a job interview or to otherwise participate in the hiring process, please contact us at accommodationrequests@gofundme.com.Global Data Privacy Notice for Job Candidates and Applicants:Depending on your location, the General Data Protection Regulation (GDPR) or certain US privacy laws may regulate the way we manage the data of job applicants. Our full notice outlining how data will be processed as part of the application procedure for applicable locations is available here. By submitting your application, you are agreeing to our use and processing of your data as required.Dedication to DiversityGoFundMe and Classy are committed to leveraging Diversity, Equity, Inclusion, and Belonging to cultivate a culture that embraces and supports the unique identities, experiences, and perspectives of our people and customers.Our diversity recruiting priority is recognized under our first DEIB Driver: Opportunity Foster Diversity - we identify, recruit, and invest in top talent- ensure our people reflect the unique identities, experiences, and perspectives of the communities we serve and are all given the chance to grow.Learn more about GoFundMe:We're proud to partner with GoFundMe.org, an independent public charity, to extend the reach and impact of our generous community, while helping drive critical social change. You can learn more about GoFundMe.org's activities and impact in their FY '23 annual report.Our annual ""Year in Help"" report reflects our community's impact in advancing our mission of helping people help each other.For recent company news and announcements, visit our Newsroom.","We are looking for an experienced and technically skilled individual to lead the team that provides data engineering and analytics functionality for the entire GoFundMe/Classy organization; 7+ years industry experience designing and building distributed data systems,; 5+ years of experience supporting or managing a business intelligence or analytics team, including employee development and performance management; A track record of recruiting, leading, and growing high performing technical teams in a demanding talent market; Strong proficiency in SQL and ETL processes; Modern data architectures including Data Lake, Data Mesh; Experience with data visualization tools such as Tableau, Looker, Quicksight, or similar; Proficiency in statistical/machine learning software such as R, Python, SAS, or Matlab; Completed bachelor's degree in Computer Science, Data Science, Statistics, or a related field; 6 more items(s)","Market competitive pay; Rich healthcare benefits including employer paid premiums for medical/dental/vision (100% for employee-only plans and 85% for employee + dependent plans) and employer HSA contributions; 401(k) retirement plan with company matching; Hybrid workplace with fully remote flexibility for many roles; Monetary support for new hire setup, hybrid work & wellbeing, family planning, and commuting expenses; A variety of mental and wellness programs to support employees; Generous paid parental leave and family planning stipend; Supportive time off policies including vacation, sick/mental health days, volunteer days, company holidays, and a floating holiday; Learning & development and recognition programs; ""Gives Back"" Program where employees can nominate a fundraiser every week for a donation from the company; Inclusion, diversity, equity, and belonging are vital to our priorities and we continue to evolve our strategy to ensure DEI is embedded in all processes and programs at GoFundMe; The total annual salary for this full-time position is $300,000-$410,000 + equity + benefits; The salary range was determined by role, level, and location requirement the US; Individual pay is determined by work location and additional factors including job-related skills, experience, and relevant education or training; Your recruiter can share more about the specific salary range based on your location during the hiring process; 12 more items(s)","SQL, ETL, Data Lake, Data Mesh, Tableau, Looker, Quicksight, R, Python, SAS, Matlab, Snowflake, AWS, Azure, Google Cloud, Hadoop, Spark, Kafka"
"Senior Data Engineer, Square Banking",Elevano,"Elevano • Atherton, CA •  via Adzuna",150K–170K a year,Full-time,o is an embedded talent consultancy. We help startups and technology companies scale their teams by deeply partnering and embeddign with them. Elevano is an equal opportunity employer.ElevanoWeb: http://www.elevano.com,"Hadoop, Machine learning & Spark; AWS; NoSQL; Kafka, Storm, MapReduce; Scala, Python, Go; 2 more items(s)","Generous Salary + Equity Package; 401K; Close to CalTrain station; Unlimited Vacation Policy; Medical, Dental, Vision; 2 more items(s)","Hadoop, Machine Learning, Spark, AWS, NoSQL, Kafka, Storm, MapReduce, Scala, Python, Go"
Senior Machine Learning Engineer- Data,"Block, Inc.","Block, Inc. • San Francisco, CA •  via Karkidi",202K–248K a year,Full-time,"rategic move that not only empowers us to use the distinctive value of data but also extends its positive impact to our customers, our Sellers, and users of the Banking platform.As an Engineer focused on Data for Square Banking, you will help us build our own Square Banking Financial Data Mesh Platform, using real-time Big Data technologies and Medallion architecture. You will work directly with product, engineering, data science and machine learning teams to understand their use-case, develop reliable, trusted datasets that accelerate the decision-making process of important products.You will:• You'll design large-scale, distributed data processing systems and pipelines to ensure efficient and reliable data ingestion, storage, transformation, and analysis• Promote high-quality software engineering practices towards building data infrastructure and pipelines at scale• You'll build core datasets to serve as unique sources of truth for product and departments (product, marketing, sales, finance, customer experience, data science, business operations, IT, engineering)• You'll partner with data scientists and other cross-functional partners to understand their needs and build pipelines to scale.• Identify and address data quality and integrity issues through data validation, cleansing, and data modeling techniques. You'll implement automated workflows that lower manual/operational cost for team members, define and uphold SLAs for timely delivery of data, move us closer to democratizing data and a self-serve model (query exploration, dashboards, data catalog, data discovery)• Learn about Big Data architecture via technologies such as AWS, DataBricks and Kafka.• Stay up to date with emerging technologies, best practices, and industry trends in data engineering and software development• Mentor and provide guidance to junior data engineers fostering inclusivity and growth.• Work remotely with a team of distributed colleagues #LI-Remote• Report to the Engineering Manager of Banking - Data EngineeringQualificationsYou Have:• 8+ years as a data engineer or software engineer, with a focus on large-scale data processing and analytics• You've spent 4+ years as a data engineer building core datasets.• You are passionate about analytics use cases, data models and solving complex data problems.• You have hands-on experience shipping scalable data solutions in the cloud (e.g AWS, GCP, Azure), across multiple data stores (e.g Databricks, Snowflake, Redshift, Hive, SQL/NoSQL, columnar storage formats) and methodologies (e.g dimensional modeling, data marts, star/snowflake schemas)• You have hands-on experience with highly scalable and reliable data pipelines using BigData (e.g Airflow, DBT, Spark, Hive, Parquet/ORC, Protobuf/Thrift, etc)• Optimized and tuned data pipelines to enhance overall system performance, reliability, and scalability• Knowledge of programming languages (e.g. Go, Ruby, Java, Python)• Willingness to participate in professional development activities to stay current on industry knowledge and passion for trying new things.Additional InformationBlock takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.Zone A: USD $202,500 - USD $247,500Zone B: USD $192,400 - USD $235,200Zone C: USD $182,300 - USD $222,800Zone D: USD $172,200 - USD $210,400To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.Full-time employee benefits include the following:• Healthcare coverage (Medical, Vision and Dental insurance)• Health Savings Account and Flexible Spending Account• Retirement Plans including company match• Employee Stock Purchase Program• Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance• Paid parental and caregiving leave• Paid time off (including 12 paid holidays)• Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)Learning and Development resourcesPaid Life insurance, AD&D, and disability benefitsThese benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, veteran status, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible. Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our I+D page.Additionally, we consider qualified applicants with criminal histories for employment on our team, assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.We’ve noticed a rise in recruiting impersonations across the industry, where individuals are sending fake job offer emails. Contact from any of our recruiters or employees will always come from an email address ending with @block.xyz, @squareup.com, @tidal.com, or @afterpay.com, @clearpay.co.uk.","8+ years as a data engineer or software engineer, with a focus on large-scale data processing and analytics; You've spent 4+ years as a data engineer building core datasets; You are passionate about analytics use cases, data models and solving complex data problems; You have hands-on experience shipping scalable data solutions in the cloud (e.g AWS, GCP, Azure), across multiple data stores (e.g Databricks, Snowflake, Redshift, Hive, SQL/NoSQL, columnar storage formats) and methodologies (e.g dimensional modeling, data marts, star/snowflake schemas); You have hands-on experience with highly scalable and reliable data pipelines using BigData (e.g Airflow, DBT, Spark, Hive, Parquet/ORC, Protobuf/Thrift, etc); Optimized and tuned data pipelines to enhance overall system performance, reliability, and scalability; Knowledge of programming languages (e.g. Go, Ruby, Java, Python); Willingness to participate in professional development activities to stay current on industry knowledge and passion for trying new things; 5 more items(s)","Zone B: USD $192,400 - USD $235,200; Zone C: USD $182,300 - USD $222,800; USD $172,200 - USD $210,400; Healthcare coverage (Medical, Vision and Dental insurance); Health Savings Account and Flexible Spending Account; Retirement Plans including company match; Employee Stock Purchase Program; Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance; Paid parental and caregiving leave; Paid time off (including 12 paid holidays); Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees); Learning and Development resources; Paid Life insurance, AD&D, and disability benefits; These benefits are further detailed in Block's policies; This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus; Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies; Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans; 14 more items(s)","AWS, GCP, Azure, Databricks, Snowflake, Redshift, Hive, SQL/NoSQL, Airflow, DBT, Spark, Parquet/ORC, Protobuf/Thrift, Go, Ruby, Java, Python"
AWS Data Engineer with Snowflake,Luma AI,"Luma AI • Palo Alto, CA •  via Lever",Full-time,Health insurance,"ML experience, Data experience, and passion for building AI products.Responsibilities• Design data pipelines, including finding appropriate data sources, scraping, filtering, post-processing, de-duplicating, and versioning. The system should be robust and scalable for production use.• Design and implement frameworks to evaluate the effectiveness of our models and data. For example, set up the standards for an automated evaluation pipeline to run before any new model gets deployed into the API.• Work closely with others who might be data contributors or consumers or both to incorporate their data usage needs on a variety of tasks and domains.• Work with human labeling vendors to refine the procedure and guidelines to collect high-quality human annotation data.• Conduct open-ended research to improve the quality of collected data, including but not limited to, semi-supervised learning, human-in-the-loop machine learning and fine-tuning with human feedback.Experience• 5+ years of relevant experience or demonstration of high impact projects as a Data Engineer, Machine Learning Engineer, or Data Scientist, dealing with large amounts of data on a daily basis.• Have a strong belief in the criticality of high-quality data and are highly motivated to work with the associated challenges.• Have experience working in large distributed systems.• Strong generalist python and pytorch skills• Experience using SQL, Spark, or other tools for processing large amounts of data.• Please note this role is not meant for recent grads.$180,000 - $250,000 a yearThe pay range for this position in California is $180,000 - $250,000/yr; however, base pay offered may vary depending on job-related knowledge, skills, candidate location, and experience. We also offer competitive equity packages in the form of stock options and a comprehensive benefits plan.Your application is reviewed by real people.","Good candidates should have exceptional general python engineering skills alongside a combination of industry ML experience, Data experience, and passion for building AI products; 5+ years of relevant experience or demonstration of high impact projects as a Data Engineer, Machine Learning Engineer, or Data Scientist, dealing with large amounts of data on a daily basis; Have a strong belief in the criticality of high-quality data and are highly motivated to work with the associated challenges; Have experience working in large distributed systems; Strong generalist python and pytorch skills; Experience using SQL, Spark, or other tools for processing large amounts of data; 3 more items(s)","$180,000 - $250,000 a year; The pay range for this position in California is $180,000 - $250,000/yr; however, base pay offered may vary depending on job-related knowledge, skills, candidate location, and experience; We also offer competitive equity packages in the form of stock options and a comprehensive benefits plan","Python, PyTorch, SQL, Spark"
Staff Data Platform Engineer,ClifyX,"ClifyX • San Francisco, CA •  via ZipRecruiter",No Degree Mentioned,,"Position Title: AWS Data Engineer with Snowflake expLocation: SFO, CA (3 days a week onsite)Duration: 6 Months plusJob Description:Minimum 9 of total IT experienceShould have 3 to 4 yrs of experience in AWS Data EngineeringShould have experience in SnowflakeStrong Communication Skills",Minimum 9 of total IT experience; Should have 3 to 4 yrs of experience in AWS Data Engineering; Should have experience in Snowflake; Strong Communication Skills; 1 more items(s),,"AWS, Snowflake"
Data Engineer - w2 Contract,Adobe,"Adobe • San Francisco, CA •  via Karkidi",146K–275K a year,Full-time,"o hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!The challengeThis is an exciting new opportunity for an experienced Senior/Staff data platform engineer! You will build and maintain scalable, rock solid data pipelines for analysts and machine learning engineers. You will collaborate closely with cross-functional teams across Adobe to enable efficiencies in various stages of data lifecycle from storage, processing to analysis and extraction of actionable insights. You will use your expertise in data engineering, machine learning solutions, and cloud platforms to deliver innovative solutions.The role will be located in San Jose Bay Area, but our team is distributed across the United States and Europe.The ideal candidate will have a passion for simplifying and automating workflows, for researching unexplored domains and for being efficient and helping the team do more with less.What you will do• Build and maintain data infrastructure on cloud platforms using AWS or Azure services.• Ensure data quality and integrity by implementing data validation and cleansing process.• Optimize data and processing pipelines for scalability, reliability and fault-tolerance by using data volume, velocity and variety metrics.• Collaborate with data scientists and analysts to understand data requirements and translate them into scalable, high performant data ETL solutions.• Mentor and provide technical guidance to junior data engineers promoting knowledge sharing across the team.• Work with external partners to facilitate data access.• Lead sophisticated, large-scale projects in an Agile environment using Git, Jenkins, Docker, Marathon, Mesos.Required skills• Master's degree in Computer science, Engineering or a related field.• 8+ years of experience in data engineering, machine learning, and/or cloud tech.• Strong analytical and problem solving skills. Think strategically and make data driven decisions.• Proficient in Python, scripting, strong SQL and query optimization techniques.• Hands-on experience with cloud platforms(AWS, Azure, GCP) and related services(S3, blob store, Databricks, ADLS Lake storage, Azure Data Explorer)• Experience in a DevOps role, configuring and launching Docker containers, Jenkins for CI/CD, airflow for scheduling and orchestration• Knowledge of machine learning pipelines and frameworks like MLFlow• Strong leadership skills with the ability to mentor and inspire junior team members.• Passion for continuous betterment (i.e. better workflows, automation, efficiency)• Excellent in teamwork and communication skillsPreferred skills• Experience and knowledge of Digital Analytics or Digital Marketing products• Knowledge of statistics or experience with pandas, pyspark or R• Experience in a B2B environment is a plus• Staying up-to-date with industry trends and emerging technologies to drive continuous improvement.Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $146,400 -- $275,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.","The ideal candidate will have a passion for simplifying and automating workflows, for researching unexplored domains and for being efficient and helping the team do more with less; Master's degree in Computer science, Engineering or a related field; 8+ years of experience in data engineering, machine learning, and/or cloud tech; Strong analytical and problem solving skills; Think strategically and make data driven decisions; Proficient in Python, scripting, strong SQL and query optimization techniques; Hands-on experience with cloud platforms(AWS, Azure, GCP) and related services(S3, blob store, Databricks, ADLS Lake storage, Azure Data Explorer); Experience in a DevOps role, configuring and launching Docker containers, Jenkins for CI/CD, airflow for scheduling and orchestration; Knowledge of machine learning pipelines and frameworks like MLFlow; Strong leadership skills with the ability to mentor and inspire junior team members; Passion for continuous betterment (i.e; better workflows, automation, efficiency); Excellent in teamwork and communication skills; 10 more items(s)","The U.S. pay range for this position is $146,400 -- $275,000 annually; At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans; Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP); In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award; 1 more items(s)","AWS, Azure, Git, Jenkins, Docker, Marathon, Mesos, S3, blob store, Databricks, ADLS Lake storage, Azure Data Explorer, airflow, MLFlow, Python, SQL, pandas, pyspark, R"
Jr. Data Engineer,"MM International, LLC","MM International, LLC • Sunnyvale, CA •  via LinkedIn",17 days ago,Contractor,"0 percent reporting and 40 percent data engineering. Will be building large scale data pipelines. Need to be proficiaent in SQL, data, analysis, and reportingNeed to have experience with (looker, Google data studio, or adobe). The resource will be pulling data from structured and unstructured data sources. Then ETL the data using Scala, Python, Java, utilize Spark . Everything is done in a GCP/ Azure environment with usage of Hive tables","SCALA, JAVA, AND PYTHON; Virtualization: Looker, Adobe, Or Google Data Studio; Need to be proficiaent in SQL, data, analysis, and reporting; Need to have experience with (looker, Google data studio, or adobe); The resource will be pulling data from structured and unstructured data sources; Then ETL the data using Scala, Python, Java, utilize Spark ; 3 more items(s)",Will be doing Data Engineering work for a Marketing Technology Platform; Position is 60 percent reporting and 40 percent data engineering; Will be building large scale data pipelines,"SQL, Looker, Google Data Studio, Adobe, Scala, Python, Java, Spark, GCP, Azure, Hive"
"Data Engineer at CodePath.org San Francisco, CA",Blumensystems,"Blumensystems • San Francisco, CA •  via Learn4Good",100K–130K a year,Full-time,"onsibilitiesClean, standardize, explore, and catalogue public and proprietary datasets before they enter an automated ingestion pipelineBuild on a proprietary join system by creating unique keys for disparate datasetsRefine and improve the data ingestion and analysis pipeline for vector, raster, and real-time geospatial dataQualifications1-3 years of experience building and shipping complex data pipelines in an enterprise settingDemonstrated experience working with messy SQL querying, data standardization, and schema designPersonable and excited to work closely with customers in natural resources industriesStack: GCP, Big Query, PostGIS, Python,Preferred QualificationsDemonstrated interest in energy development, land use, mineral exploration or similar topicsPast experience with government data sources (BLM, EPA, USFS, DOE, NOAA, BOEM, etc.)Pay$100K-130K annual salaryEquity grantAbout UsBlumen is building software to streamline site selection and permitting for project developers and EPC companies powering the energy transition across solar, wind, BESS, carbon sequestration, geothermal, hydrogen, and RNG projects.How to ApplyDoes this role sound like a good fit? Submit this form .#J-18808-Ljbffr","1-3 years of experience building and shipping complex data pipelines in an enterprise setting; Demonstrated experience working with messy SQL querying, data standardization, and schema design; Personable and excited to work closely with customers in natural resources industries; Stack: GCP, Big Query, PostGIS, Python,; Demonstrated interest in energy development, land use, mineral exploration or similar topics; Past experience with government data sources (BLM, EPA, USFS, DOE, NOAA, BOEM, etc.); 3 more items(s)",$100K-130K annual salary; Equity grant,"GCP, Big Query, PostGIS, Python"
"Computational Biologist & Engineer in Genomics (Engreitz Lab, Research Data Analyst 1)",CodePath.org,"CodePath.org • San Francisco, CA •  via Trustek.org",2 days ago,Full-time,"mpanies, and rise together to become the tech leaders of tomorrow. As of 2023, we've served 10,000 students a year nationwide, and we plan to scale up to ~100,000 students a year by 2028.Founded in 2017, CodePath has taught over 26,000 students and delivered courses across over 110 universities. We are supported by some of the largest and most well-respected organizations, including Amazon, Andreessen Horowitz, Blue Meridian Partners, Comcast, Google, JP Morgan Chase, Knight Foundation, Meta, New Profit, and Salesforce, among others. In 2024, CodePath was recognized as one of the Most Innovative Companies in Education by Fast Company .• * We will never ask for bank information or for you to download any programs as part of our job application process and initial communications to applicants will be sent directly from our HR department.About the roleLocation: Remote, USDuration: FTEReports To: VP of Product EngineeringCompensation: $120,000-$160,000 per yearWere at a critical moment in our growth as an organization. 2024 is a year of incredible change at CodePath and were looking for the right data engineer to lead the charge on our small, skilled, and growing team in the next phase of our exponential growth and impact as we transform computer science education. Our platform overview is a must watch in order to understand the significance that our Data Engineering team will play in the actualization of our mission.As an early hire in data engineering, you will collaborate with stakeholders across the organization to help build and maintain our next generation data infrastructure. In order to build out our data warehousing and ETL/ELT infrastructure the right way, we are looking for somebody resourceful with a problem solving mindset who is self-directed and able to work closely with data scientists, software developers, and business stakeholders. We expect this infrastructure, once built out, to be self-serve, accurate and secure/compliant as it enables all sorts of data science projects, ML modeling, and BI analysis across everything we do from the delivery of our courses, student marketing, sales and fundraising which makes this a challenging, creative, and exciting technical position for an experienced data engineer.The ideal candidate is a true early stage startup Data Engineer and is inspired by CodePaths ambitious vision to transform education starting with computer science, and to train engineers who will generate over $1.5T in wealth for low-income communities within the next 20 years.Key Activities• Architect and develop data platforms including data warehouses, analytic products, data lakes, ETL/ELT workflows, or data pipelines with minimal oversight• Design effective data models for analytics data use cases• Profile data sources and develop ETL/ELT processes with knowledge of data modeling fundamentals, using both SQL and supporting ETL/ELT tools (dbt, Airbyte, and others)• Develop and maintain data ingestion connectors to third-party services (Google Sheets, SurveyMonkey, HubSpot, Airtable)• Develop and maintain reporting dashboards and self-serve analytics tools in Tableau or other BI tools• Ensure architecture will support the requirements of the business by designing, implementing and maintaining a data quality assurance framework• Develop and maintain documentation for data collection processes and standards• Work with stakeholders across the organization to assist with data-related technical issues and support their data infrastructure needs.Qualifications• 2 to 5 years of relevant professional experience• Good understanding of data modeling principles including dimensional modeling, data normalization principles.• Substantial experience with SQL and building efficient data pipelines in SQL and tuning query performance• Experience managing cloud data warehouse such as Google BigQuery, Snowflake, Databricks, or Amazon Redshift• Experience with the tools we use, including Postgres, Airbyte, dbt, and Tableau• Thoughtful about making technical decisions and recommendations that serve the best interests of the organization, factoring in cost, complexity, maintenance burden, and outcomes• Has strong presentation skills: takes analysis and makes it understandable (and interesting!) for stakeholdersAdditional Skills and Experience• Proven ability to drive projects from conception to completion.• Aptitude for problem-solving and troubleshooting.• Excellent communication and collaboration skills.• Ability to work in a fast-paced, startup environment.• Proactive, independent, responsible attitude with the ability to learn quickly.Pay range$120,000$160,000 USDBenefitsWe offer a comprehensive benefits package for full-time employees that includes:• Medical, dental, and vision premiums paid at 90% for FT positions and their dependents• Flexible vacation and sick time policy with 12 company paid holidays plus a week long ""winter break"" office closure from Christmas to New Years. Employees take the time when they need it• Flexible workplace and work schedule• CodePath provides a laptop, monitor, and ergonomic office setup• Annual professional development stipend• Ability to voluntarily contribute pre and post-tax earnings to our 401k plan• Employer Provided Short Term Disability and 10 weeks paid leave to support our employees in growing their family• A commitment to developing leaders from within the organization• Frequent opportunities to connect with students, universities, and communities we serve• Opportunities to engage, collaborate and partner with top technology companies, venture capitalists, and engineering leadersAbout the Current TeamWe are individuals from a multitude of backgrounds, experiences, and unlikely stories, all connected by a single dream: a world in which regardless of background, socioeconomic status, gender, or race all people have pathways to reach their full potential.With a staff and board that cares deeply about diversity and equity, we believe that diverse perspectives and backgrounds create a richer work environment and enhance our ability to pursue our mission.Note: Research suggests that women and BIPOC individuals may self-select out of opportunities if they dont meet 100% of the job requirements. We encourage individuals who believe they have the skills necessary to thrive at CodePath to apply for this role.","2 to 5 years of relevant professional experience; Good understanding of data modeling principles including dimensional modeling, data normalization principles; Substantial experience with SQL and building efficient data pipelines in SQL and tuning query performance; Experience managing cloud data warehouse such as Google BigQuery, Snowflake, Databricks, or Amazon Redshift; Experience with the tools we use, including Postgres, Airbyte, dbt, and Tableau; Thoughtful about making technical decisions and recommendations that serve the best interests of the organization, factoring in cost, complexity, maintenance burden, and outcomes; Has strong presentation skills: takes analysis and makes it understandable (and interesting!) for stakeholders; Additional Skills and Experience; Proven ability to drive projects from conception to completion; Aptitude for problem-solving and troubleshooting; Excellent communication and collaboration skills; Ability to work in a fast-paced, startup environment; Proactive, independent, responsible attitude with the ability to learn quickly; Frequent opportunities to connect with students, universities, and communities we serve; Opportunities to engage, collaborate and partner with top technology companies, venture capitalists, and engineering leaders; 12 more items(s)","Compensation: $120,000-$160,000 per year; Pay range; $120,000$160,000 USD; We offer a comprehensive benefits package for full-time employees that includes:; Medical, dental, and vision premiums paid at 90% for FT positions and their dependents; Flexible vacation and sick time policy with 12 company paid holidays plus a week long ""winter break"" office closure from Christmas to New Years; Flexible workplace and work schedule; CodePath provides a laptop, monitor, and ergonomic office setup; Annual professional development stipend; Ability to voluntarily contribute pre and post-tax earnings to our 401k plan; Employer Provided Short Term Disability and 10 weeks paid leave to support our employees in growing their family; A commitment to developing leaders from within the organization; 9 more items(s)","SQL, dbt, Airbyte, Tableau, Google Sheets, SurveyMonkey, HubSpot, Airtable, Google BigQuery, Snowflake, Databricks, Amazon Redshift, Postgres"
"Senior Data Engineer, Data Ingestion",School of Medicine,"School of Medicine • Stanford, CA •  via Stanford Medicine - Stanford University",Full-time,,"nter. The position is open, and a successful candidate could join immediately.Lab overview: DNA regulatory elements in the human genome, which harbor thousands of genetic risk variants for common and rare diseases and could reveal targets for therapeutics that aim to precisely tune cellular functions — if only we could map the complex regulatory wiring that connects 2 million regulatory elements with 21,000 genes in thousands of cell types in the human body. We have recently developed new approaches that could enable mapping this regulatory wiring at massive scale (see Fulco et al. Science 2016, Fulco et al. Nature Genetics 2019, Nasser et al. Nature 2021, Bergman et al. Nature 2022). We invent new tools combining experimental and computational genomics, biochemistry, molecular biology, and human genetics to assemble regulatory maps of the human genome and uncover biological mechanisms of heart disease.We are looking for creative and passionate people at any stage in their careers, including computational biologists and software engineers. Candidates will train to lead and design computational projects that push the boundaries of genomic technology and reveal the functions of genetic variants associated with human diseases.Specific projects include: to develop and apply predictive models for enhancer-gene regulation, including leveraging the ABC and ENCODE-rE2G framework to capture enhancer synergies and genetic variant effect sizes; to develop and apply methods to systematically benchmark the performance of predictive models; to design and analyze large-scale single-cell CRISPR screens to chart gene regulatory connections and decipher molecular mechanisms of gene regulation. For more information and recent work, see www.engreitzlab.org.The Engreitz Laboratory is a dynamic, interdisciplinary workplace that will provide unique access to cutting edge technologies and scientific thought, with the potential for widespread recognition of scientific contributions. We value a diversity of values, backgrounds, and approaches to solving problems.The successful candidate for this position should have expertise in data analysis and software engineering, preferably with applications to high-throughput sequencing or other biological assays; enthusiasm for developing and applying computational approaches to learn fundamental mechanisms of gene regulation, and understand human disease; fluency in Unix, programming, and bioinformatics tools (Python, R, or equivalent); excellent communication, organization, and time management skills; and be a creative, organized, motivated team player.Duties include:• Collaborate with experimentalists and computational biologists to develop and apply functional genomics techniques to understand gene regulation and the genetic basis of heart disease• Design and implement generalizable algorithms and tools for analysis of biological data, including high-throughput functional genomics assays• Evaluate and recommend new emerging technologies, approaches, and problems• Create scientifically rigorous visualizations, communications, and presentations of results• Contribute to generation of protocols, publications, and intellectual property• Maintain and organize computational infrastructure and resources• - Other duties may also be assigned.DESIRED QUALIFICATIONS:• Suggested: B.S. in computational biology, computer science, physics, statistics, math, molecular biology, or related field. Motivated applicants of all levels are encouraged to apply.• Experience in collaborative software development and relevant platforms• Demonstrated expertise in software engineering and/or statistical methods in data analysis, preferably with applications to high-throughput sequencing or other biological assays• Enthusiasm for using computational approaches to learn fundamental mechanisms of gene regulation and understand human disease• Excellent communication, organization, and time management skills• Creative, organized, motivated, team playerEDUCATION & EXPERIENCE (REQUIRED):Bachelor's degree or a combination of education and relevant experience. Experience in a quantitative discipline such as economics, finance, statistics or engineering.KNOWLEDGE, SKILLS AND ABILITIES (REQUIRED):• Knowledge of Unix, programming, and data analysis tools (Python, R, or equivalent).• Substantial experience with MS Office and analytical programs.• Strong writing and analytical skills.• Ability to prioritize workload.CERTIFICATIONS & LICENSES:None.PHYSICAL REQUIREMENTS*:• Sitting in place at computer for long periods of time with extensive keyboarding/dexterity.• Occasionally use a telephone.• Rarely writing by hand.• - Consistent with its obligations under the law, the University will provide reasonable accommodation to any employee with a disability who requires accommodation to perform the essential functions of his or her job.WORKING CONDITIONS:Some work may be performed in a laboratory or field setting.The expected pay range for this position is $66,560 to $97,000 per annum. Stanford University provides pay ranges representing its good faith estimate of what the university reasonably expects to pay for a position. The pay offered to a selected candidate will be determined based on factors such as (but not limited to) the scope and responsibilities of the position, the qualifications of the selected candidate, departmental budget availability, internal equity, geographic location and external market pay for comparable jobs.At Stanford University, base pay represents only one aspect of the comprehensive rewards package. The Cardinal at Work website (https://cardinalatwork.stanford.edu/benefits-rewards) provides detailed information on Stanford’s extensive range of benefits and rewards offered to employees. Specifics about the rewards package for this position may be discussed during the hiring process.Consistent with its obligations under the law, the University will provide reasonable accommodations to applicants and employees with disabilities. Applicants requiring a reasonable accommodation for any part of the application or hiring process should contact Stanford University Human Resources by submitting a contact form.Stanford is an equal employment opportunity and affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic protected by law.The job duties listed are typical examples of work performed by positions in this job classification and are not designed to contain or be interpreted as a comprehensive inventory of all duties, tasks, and responsibilities. Specific duties and responsibilities may vary depending on department or program needs without changing the general nature and scope of the job or level of responsibility. Employees may also perform other duties as assigned.WORK STANDARDS (from JDL):• Interpersonal Skills: Demonstrates the ability to work well with Stanford colleagues and clients and with external organizations.• Promote Culture of Safety: Demonstrates commitment to personal responsibility and value for safety; communicates safety concerns; uses and promotes safe behaviors based on training and lessons learned.• Subject to and expected to comply with all applicable University policies and procedures, including but not limited to the personnel policies and other policies found in the University's Administrative Guide, http://adminguide.stanford.edu.","The successful candidate for this position should have expertise in data analysis and software engineering, preferably with applications to high-throughput sequencing or other biological assays; enthusiasm for developing and applying computational approaches to learn fundamental mechanisms of gene regulation, and understand human disease; fluency in Unix, programming, and bioinformatics tools (Python, R, or equivalent); excellent communication, organization, and time management skills; and be a creative, organized, motivated team player; Motivated applicants of all levels are encouraged to apply; Experience in collaborative software development and relevant platforms; Demonstrated expertise in software engineering and/or statistical methods in data analysis, preferably with applications to high-throughput sequencing or other biological assays; Enthusiasm for using computational approaches to learn fundamental mechanisms of gene regulation and understand human disease; Excellent communication, organization, and time management skills; Creative, organized, motivated, team player; Bachelor's degree or a combination of education and relevant experience; Experience in a quantitative discipline such as economics, finance, statistics or engineering; Knowledge of Unix, programming, and data analysis tools (Python, R, or equivalent); Substantial experience with MS Office and analytical programs; Strong writing and analytical skills; Ability to prioritize workload; Sitting in place at computer for long periods of time with extensive keyboarding/dexterity; Occasionally use a telephone; Rarely writing by hand; Interpersonal Skills: Demonstrates the ability to work well with Stanford colleagues and clients and with external organizations; 14 more items(s)","The expected pay range for this position is $66,560 to $97,000 per annum; Stanford University provides pay ranges representing its good faith estimate of what the university reasonably expects to pay for a position","Unix, Python, R, MS Office"
Senior Staff Data Engineer,Sirius XM Radio,"Sirius XM Radio • Oakland, CA •  via ZipRecruiter",5 days ago,Full-time,"ape the future of audio, where everyone can be effortlessly connected to the voices, stories, and music they love wherever they are.This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM's vision to life every day.SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM's platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment, and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics, and monetization. The Company's advertising sales organization, which operates as SiriusXM Media, leverages its scale, cross-platform sales organization, and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through SiriusXM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.How you'll make an impact:In this role you will be a member of a team responsible for designing and developing a data ecosystem to promote an environment of data democratization and utilization across SiriusXM and Pandora. This role will report to a Director of Data Engineering.What you'll do:• Build cloud-based data pipeline frameworks and operations to power business intelligence.• Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments.• Implement data governance through proper structure, access controls, and safeguards Build command line tools that improve user experience and encourage best practices in the cloud.• Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform.• Write documentation to encourage adoption of platform tools and support users in their use.• Strengthen corporate best practices around data engineering software development processes.What you'll need:• 5+ years' experience developing data ETL pipelines and data tools in Scala and/or Python.• BS/MS or above in Computer Science or relevant experience• Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop.• Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark.• Experience developing SQL applications of significant complexity• Experience with cloud computing - Google Cloud Platform, Amazon Web Services.• Experience developing for Linux-based deployment platforms, developing scalable, multithreaded server-side software for deployment.• Experience developing service-oriented architectures / orchestration.• Experience with API design/development - RPC, REST, JSON.• Experience with unit and integration testing frameworks.• Experience with CI/CD, build and deployment technologies such as Jenkins.• Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.).• Experience working in a Cloud Environment (AWS, GCP, etc.).• Experience developing and deploying machine learning algorithms.• Experience developing with additional languages - R, Scala.• Experience with workflow tools - Airflow/Composer/Luigi.• Experience with data serialization system - Avro, Protobuf.• Good public speaking and presentation skills.• Interpersonal skills and ability to interact and work with staff at all levels.• Excellent written and verbal communication skills.• Ability to work independently and in a team environment.• Ability to pay attention to details and be organized.• Ability to project professionalism over the phone and in person.• Ability to handle multiple tasks in a fast-paced environment.• Commitment to ""internal client"" and customer service principles.• Willingness to take initiative and to follow through on projects.• Creative writing ability.• Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.• Must have legal right to work in the U.S.At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $128,000 to $170,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.#LI-NS1Employment Type: FULL_TIME","5+ years' experience developing data ETL pipelines and data tools in Scala and/or Python; BS/MS or above in Computer Science or relevant experience; Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop; Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark; Experience developing SQL applications of significant complexity; Experience with cloud computing - Google Cloud Platform, Amazon Web Services; Experience developing for Linux-based deployment platforms, developing scalable, multithreaded server-side software for deployment; Experience developing service-oriented architectures / orchestration; Experience with API design/development - RPC, REST, JSON; Experience with unit and integration testing frameworks; Experience with CI/CD, build and deployment technologies such as Jenkins; Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.); Experience working in a Cloud Environment (AWS, GCP, etc.); Experience developing and deploying machine learning algorithms; Experience developing with additional languages - R, Scala; Experience with workflow tools - Airflow/Composer/Luigi; Experience with data serialization system - Avro, Protobuf; Good public speaking and presentation skills; Interpersonal skills and ability to interact and work with staff at all levels; Excellent written and verbal communication skills; Ability to work independently and in a team environment; Ability to pay attention to details and be organized; Ability to project professionalism over the phone and in person; Ability to handle multiple tasks in a fast-paced environment; Commitment to ""internal client"" and customer service principles; Willingness to take initiative and to follow through on projects; Creative writing ability; Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment; Must have legal right to work in the U.S; 26 more items(s)","At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience; These considerations can cause your compensation to vary; We expect the base salary for this position to be in the range of $128,000 to $170,000 and will depend on your skills, qualifications, and experience; Additionally, this role might be eligible for discretionary short-term and long-term incentives; 1 more items(s)","Scala, Python, MapReduce, HDFS, Hive, Tez, Spark, Sqoop, Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Google Cloud Platform, Amazon Web Services, Linux, RPC, REST, JSON, Jenkins, Zeppelin, Tableau, AWS, GCP, R, Airflow, Composer, Luigi, Avro, Protobuf"
Principal Software Engineer - Database Platform,ServiceNow,"ServiceNow • Santa Clara, CA •  via JobzMall",140K–200K a year,Full-time,"ong technical expertise, as well as a passion for problem-solving and driving innovation. If this sounds like you, keep reading to learn more about this exciting opportunity!Design and develop highly scalable and efficient data pipelines to support various business needs and requirements.Collaborate with cross-functional teams to understand data needs and implement solutions that meet business objectives.Analyze and optimize data storage, ingestion, and processing strategies to ensure high performance and reliability.Stay up-to-date with emerging technologies and industry best practices to continuously improve our data platform.Identify and troubleshoot data quality issues, implementing solutions to maintain data integrity.Develop and maintain data documentation, including data dictionaries, data lineage, and data flow diagrams.Mentor and provide technical guidance to junior data engineers on the team.Work closely with data scientists and analysts to ensure the availability and accuracy of data for analytical and reporting purposes.Collaborate with database administrators to ensure the security, backup, and recovery of our data platform.Participate in code reviews, provide feedback, and maintain coding standards and best practices.Communicate effectively with stakeholders to gather requirements and provide project updates.Take ownership of projects and proactively identify and resolve any potential roadblocks.Continuously evaluate and improve data processes and procedures to increase efficiency and effectiveness.Participate in on-call rotations and respond to production issues in a timely manner.Uphold the company's data governance policies and procedures.ServiceNow is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.","We are looking for someone with strong technical expertise, as well as a passion for problem-solving and driving innovation","Design and develop highly scalable and efficient data pipelines to support various business needs and requirements; Collaborate with cross-functional teams to understand data needs and implement solutions that meet business objectives; Analyze and optimize data storage, ingestion, and processing strategies to ensure high performance and reliability; Stay up-to-date with emerging technologies and industry best practices to continuously improve our data platform; Identify and troubleshoot data quality issues, implementing solutions to maintain data integrity; Develop and maintain data documentation, including data dictionaries, data lineage, and data flow diagrams; Mentor and provide technical guidance to junior data engineers on the team; Work closely with data scientists and analysts to ensure the availability and accuracy of data for analytical and reporting purposes; Collaborate with database administrators to ensure the security, backup, and recovery of our data platform; Participate in code reviews, provide feedback, and maintain coding standards and best practices; Communicate effectively with stakeholders to gather requirements and provide project updates; Take ownership of projects and proactively identify and resolve any potential roadblocks; Continuously evaluate and improve data processes and procedures to increase efficiency and effectiveness; Participate in on-call rotations and respond to production issues in a timely manner; Uphold the company's data governance policies and procedures; 12 more items(s)",ServiceNow
Junior Data Analyst/Scientist/Engineer,Snowflake,"Snowflake • San Mateo, CA •  via Snowflake Careers",11 days ago,Full-time,"ke’s products and services and is rapidly evolving to meet Snowflake’s future needs.FDB runs on multiple cloud providers including Amazon Web Services, Microsoft Azure and Google Cloud. The elastic infrastructure FDB runs on is being built from the ground up and is envisioned to be a cloud agnostic, fully automated manageability platform that provides:• Autoscaling and auto-balancing of clusters based on utilization, traffic and workloads• Auto-provisioning of new clusters with zero manual intervention• Self-healing capabilities that prevent, mitigate and resolve any production impact• Built-in configuration management that guarantees FDB runs correctly and on the intended topologies• Self-optimizing COGS efficiency, ensuring we run our clusters at optimal utilizationWe are looking for an outstanding Principal Software Engineer with a passion for large scale databases and distributed systems to help us take the FDB platform to the next level.AS A PRINCIPAL SOFTWARE ENGINEER ON THIS TEAM, YOU WILL:• Lead a team responsible for the health and growth of the FDB platform service at Snowflake• Design and implement scalable distributed system solutions for our cloud agnostic platform, with features including• Auto-provisioning, Auto-scaling and auto-balancing clusters based on utilization and workloads• Self-healing capabilities that prevent, mitigate and resolve any production impact• Built-in cluster configuration management with release automation• Analyze fault-tolerance and high availability issues, performance and scale challenges, and solve them.• Own the end to end health and growth of the FDB platform, including identifying the right problems to solve, the solution, design, implementation, as well as testing strategy, safe production rollout and stability.• Understand trade-offs between consistency, durability and costs to build solutions which can meet the demands of rapidly growing services.• Build the next generation transaction system, caching, storage engine and multi tenant capabilities• Evangelize best practices in database usage and end-to-end architecture.• Pinpoint problems, instrument relevant components as needed, and ultimately implement solutions.• Mentor and grow engineers.AN IDEAL CANDIDATE WILL HAVE:• 10+ years industry experience designing, building and supporting large scale infrastructure in production.• Experience designing, building, and operating large-scale distributed systems supporting stateful services• Experience in container orchestration, cluster management, or autoscaling.• Excellent understanding of operating systems concepts including multi-threading, memory management, networking and storage, performance and scale.• Systems programming skills including multi-threading, concurrency, etc. Fluency in Java, C++, or C is preferred.• Solid understanding of the internals of Kubernetes, Mesos, OpenShift, or other container platforms• Experience with scalable Key-Value stores such as FoundationDB, RocksDB/LevelDB, DynamoDB, Redis, etc. a plus.• Track record of leading and delivering highly complex projects in the distributed systems space• Intense curiosity, willingness to question and passion for making systems better• Experience with one or more of the following highly desired:• Big Data storage technologies and their applications (HDFS, Cassandra, Columnar Databases, etc.)• Scalable Key-Value stores such as FoundationDB, RocksDB/LevelDB, DynamoDB, Redis, Cassandra, etc.• BS in Computer Science; Masters or PhD Preferred.About Snowflake:Snowflake SIGMOD 2016 paperAbout FoundationDB:FDB SIGMOD 21 PaperFoundationDB Summit 2018 and FoundationDB Summit 2019How FDB powers Snowflake Metadata Forward!Every Snowflake employee is expected to follow the company’s confidentiality and security standards for handling sensitive data. Snowflake employees must abide by the company’s data security plan as an essential part of their duties. It is every employee's duty to keep customer information secure and confidential.","10+ years industry experience designing, building and supporting large scale infrastructure in production; Experience designing, building, and operating large-scale distributed systems supporting stateful services; Experience in container orchestration, cluster management, or autoscaling; Excellent understanding of operating systems concepts including multi-threading, memory management, networking and storage, performance and scale; Systems programming skills including multi-threading, concurrency, etc; Track record of leading and delivering highly complex projects in the distributed systems space; Intense curiosity, willingness to question and passion for making systems better; Big Data storage technologies and their applications (HDFS, Cassandra, Columnar Databases, etc.); Scalable Key-Value stores such as FoundationDB, RocksDB/LevelDB, DynamoDB, Redis, Cassandra, etc; 6 more items(s)","Autoscaling and auto-balancing of clusters based on utilization, traffic and workloads; Auto-provisioning of new clusters with zero manual intervention; Self-healing capabilities that prevent, mitigate and resolve any production impact; Built-in configuration management that guarantees FDB runs correctly and on the intended topologies; Self-optimizing COGS efficiency, ensuring we run our clusters at optimal utilization; Lead a team responsible for the health and growth of the FDB platform service at Snowflake; Design and implement scalable distributed system solutions for our cloud agnostic platform, with features including; Auto-provisioning, Auto-scaling and auto-balancing clusters based on utilization and workloads; Self-healing capabilities that prevent, mitigate and resolve any production impact; Built-in cluster configuration management with release automation; Analyze fault-tolerance and high availability issues, performance and scale challenges, and solve them; Own the end to end health and growth of the FDB platform, including identifying the right problems to solve, the solution, design, implementation, as well as testing strategy, safe production rollout and stability; Understand trade-offs between consistency, durability and costs to build solutions which can meet the demands of rapidly growing services; Build the next generation transaction system, caching, storage engine and multi tenant capabilities; Evangelize best practices in database usage and end-to-end architecture; Pinpoint problems, instrument relevant components as needed, and ultimately implement solutions; Mentor and grow engineers; 14 more items(s)","Amazon Web Services, Microsoft Azure, Google Cloud, Java, C++, C, Kubernetes, Mesos, OpenShift, FoundationDB, RocksDB, LevelDB, DynamoDB, Redis, Cassandra, HDFS"
Senior Data Engineer (Hybrid role),SynergisticIT,"SynergisticIT • Oakland, CA •  via LinkedIn",Contractor,,"ee the success outcomes of our candidates our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careershttps://www.synergisticit.com/candidate-outcomes/https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalogWe regularly interact with the Top Tech companies to give our candidates a competitive advantage see us exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023All Positions are open for all visas and US citizensWe at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like Apple, Google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry?We assist in filing for STEM extension and also for H1b and Green card filing to Candidateshttps://www.youtube.com/watch?v=OFoqPTNORewhttps://www.youtube.com/watch?v=-HkNN1ag6Zkhttps://www.youtube.com/watch?v=OAFOhcGy9Z8https://youtu.be/bJJl27D8bh0We are looking for the right matching candidates for our clientsRequired SkillsREQUIRED SKILLS For Java /Full stack/Software Programmer• Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT• Highly motivated, self-learner, and technically inquisitive• Experience in programming language Java and understanding of the software development life cycle• Project work on the skills• Knowledge of Core Java , javascript, C++, or software programming• Spring boot, Microservices, Docker, Jenkins, and REST API experience• Excellent written and verbal communication skillsFor data Science/Machine learning PositionsRequired Skills• Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT• Project work on the technologies needed• Highly motivated, self-learner, and technically inquisitive• Experience in programming language Java and understanding of the software development life cycle• Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools• Excellent written and verbal communication skillsPreferred skills: NLP, Text mining, Tableau, PowerBI, SAS, TensorflowIf you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements.No phone calls please. Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","REQUIRED SKILLS For Java /Full stack/Software Programmer; Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT; Highly motivated, self-learner, and technically inquisitive; Experience in programming language Java and understanding of the software development life cycle; Project work on the skills; Knowledge of Core Java , javascript, C++, or software programming; Spring boot, Microservices, Docker, Jenkins, and REST API experience; Excellent written and verbal communication skills; For data Science/Machine learning Positions; Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT; Project work on the technologies needed; Highly motivated, self-learner, and technically inquisitive; Experience in programming language Java and understanding of the software development life cycle; Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools; Excellent written and verbal communication skills; 12 more items(s)",,"Java, JavaScript, C++, Spring Boot, Microservices, Docker, Jenkins, REST API, SAS, Python, Computer Vision, Tableau, PowerBI, TensorFlow"
"Lead Data Engineer, Data Platform","PDF Solutions, Inc.","PDF Solutions, Inc. • Santa Clara, CA •  via Jobtrees",Full-time,,"s deep expertise in both AI/ML and large-scale data engineering, and you find excitement in architecting transformative analytics solutions, then this is your call. We're seeking a seasoned professional to lead our AI/ML initiatives within the context of big data, pushing the boundaries of predictive analytics and data-driven insights.Responsibilities• Architect Large-Scale AI/ML Solutions: Design scalable, robust AI/ML pipelines, integrating distributed computing frameworks (Spark, etc.) for efficient big data processing and model training.• Champion Model Optimization & Deployment: Drive performance tuning, model selection, and create seamless deployment strategies for production environments, ensuring real-time insights.• Oversee Data Infrastructure: Collaborate with data engineers to architect big data systems that support AI/ML workloads, ensure data quality, and guide optimization for model requirements.• Lead Research & Innovation: Initiate and oversee research into promising new AI/ML techniques or libraries specifically relevant to large-scale data challenges.• Mentor & Guide: Provide technical leadership, mentor junior colleagues, and establish best practices across the analytics team.Qualifications• Advanced Degree & Experience: MS/Ph.D. in Computer Science, Data Science, or a related field. 10+ years of proven experience designing and implementing AI/ML solutions within big data environments.• Big Data Mastery: Expertise in Spark, Hadoop Ecosystem, cloud-based big data technologies, and distributed computing principles.• Deep AI/ML Fluency: Extensive knowledge of supervised/unsupervised learning, deep learning frameworks (TensorFlow, PyTorch, etc.), NLP, or other specialized AI/ML domains.• Strategic Thinker: Ability to translate business objectives into AI/ML project roadmaps, identify potential ROI, and evaluate technologies strategically.• Exceptional Communication & Leadership: Proven ability to communicate complex technical concepts to a variety of stakeholders, inspire collaboration, and lead teams towards a unified vision.Why You Should Apply:• Make History in Analytics: Shape the forefront of big data analytics, influencing how industries leverage AI/ML to solve unprecedented challenges.• Realize the Full Potential of AI/ML: Apply your skills to complex, real-world datasets, where models can have a true, tangible impact on business success.• Lead a Team of Innovators: Collaborate with brilliant data scientists, engineers, and industry experts united in driving cutting-edge solutions.Pay RangeUSD $180,455.00 - USD $212,300.00 /Yr.","Advanced Degree & Experience: MS/Ph.D. in Computer Science, Data Science, or a related field; 10+ years of proven experience designing and implementing AI/ML solutions within big data environments; Big Data Mastery: Expertise in Spark, Hadoop Ecosystem, cloud-based big data technologies, and distributed computing principles; Deep AI/ML Fluency: Extensive knowledge of supervised/unsupervised learning, deep learning frameworks (TensorFlow, PyTorch, etc.), NLP, or other specialized AI/ML domains; Strategic Thinker: Ability to translate business objectives into AI/ML project roadmaps, identify potential ROI, and evaluate technologies strategically; Exceptional Communication & Leadership: Proven ability to communicate complex technical concepts to a variety of stakeholders, inspire collaboration, and lead teams towards a unified vision; Make History in Analytics: Shape the forefront of big data analytics, influencing how industries leverage AI/ML to solve unprecedented challenges; Realize the Full Potential of AI/ML: Apply your skills to complex, real-world datasets, where models can have a true, tangible impact on business success; 5 more items(s)","USD $180,455.00 - USD $212,300.00 /Yr","Spark, Hadoop Ecosystem, TensorFlow, PyTorch"
Senior Data Analysis Engineer,"Tubi, Inc.","Tubi, Inc. • San Francisco, CA •  via Karkidi",120K–190K a year,Full-time,"innovation and leadership for our small data platform team. You will be responsible for scaling and maintaining high quality infrastructure to make sure we can support the business with performant, scalable, and accurate datasets. Leading the Data Platform team you will have an opportunity to have an outsize impact on an organization where data is core to everything we do. We would prefer this candidate to be located in the pacific standard time zone.Responsibilities:• Build and scale Tubi's primary Data Platform. You will start with a team of at least 5 direct reports• Your team is responsible for all things Data Platform, from data ingestion pipelines, to the storage, delivery, and discoverability of high quality datasets• You will build and maintain infrastructure to support real-time streaming pipelines that create massive datasets for use in Tubi's ML pipelines and Data Lakehouse• You will manage infrastructure for both batch and streaming pipelines for 3rd party data integrations• You will guide Data Governance with a tireless commitment to high data quality and usability• You will be expected to be a hands-on leader, guiding your team by example. You will be expected to be technical and competent in the day-to-day data platform operations, but still big-picture enough to build a well-oiled, self-sustaining team that solves business needsYour Background:• Master’s or Bachelor’s degree from an accredited university, or equivalent work experience• 7+ years hands on industry experience, and 2+ years of experience leading and managing a team• Experience being a team lead & comfortable managing a growing and proactive team• Hands-on experience building out PB scale data lakes on transactional data formats (Delta, Iceberg)• Deep experience with publishing and consuming streaming datasets with either Apache Spark (Structured Streaming) or Apache Flink• Strong understanding of distributed queues (Kafka/Kinesis) and distributed databases (Cassandra, Redis)• Excellent communication / coordination skills to work with a cross functional team to onboard new datasets for diverse business verticals• Experience working with Terraform, k8s, and building CI/CD pipelines for data applications• Strong experience in a Scala or Java (scala preferred)","You will be expected to be a hands-on leader, guiding your team by example; You will be expected to be technical and competent in the day-to-day data platform operations, but still big-picture enough to build a well-oiled, self-sustaining team that solves business needs; Master’s or Bachelor’s degree from an accredited university, or equivalent work experience; 7+ years hands on industry experience, and 2+ years of experience leading and managing a team; Experience being a team lead & comfortable managing a growing and proactive team; Hands-on experience building out PB scale data lakes on transactional data formats (Delta, Iceberg); Deep experience with publishing and consuming streaming datasets with either Apache Spark (Structured Streaming) or Apache Flink; Strong understanding of distributed queues (Kafka/Kinesis) and distributed databases (Cassandra, Redis); Excellent communication / coordination skills to work with a cross functional team to onboard new datasets for diverse business verticals; Experience working with Terraform, k8s, and building CI/CD pipelines for data applications; 7 more items(s)","You will be responsible for scaling and maintaining high quality infrastructure to make sure we can support the business with performant, scalable, and accurate datasets; Leading the Data Platform team you will have an opportunity to have an outsize impact on an organization where data is core to everything we do; We would prefer this candidate to be located in the pacific standard time zone; Build and scale Tubi's primary Data Platform; You will start with a team of at least 5 direct reports; Your team is responsible for all things Data Platform, from data ingestion pipelines, to the storage, delivery, and discoverability of high quality datasets; You will build and maintain infrastructure to support real-time streaming pipelines that create massive datasets for use in Tubi's ML pipelines and Data Lakehouse; You will manage infrastructure for both batch and streaming pipelines for 3rd party data integrations; You will guide Data Governance with a tireless commitment to high data quality and usability; 6 more items(s)","Delta, Iceberg, Apache Spark, Apache Flink, Kafka, Kinesis, Cassandra, Redis, Terraform, k8s, Scala, Java"
"Data Engineer, Core Growth",NVIDIA,"NVIDIA • Santa Clara, CA •  via JobzMall",142K–179K a year,Full-time,"solutions that further our mission.The ideal candidate is a highly organized and motivated individual with a keen eye for detail and a passion for data. To be successful as a Senior Data Analysis Engineer at NVIDIA, you should have a strong background in data engineering and analytics, including working with large datasets, developing complex algorithms, and building data-driven models. You should also be comfortable working in both a team and an individual setting and possess excellent communication and problem-solving skills.In addition to the above, the successful candidate must possess the following qualifications:• BS/MS in Computer Science, Engineering, Mathematics, Statistics, or a related field• 5+ years of experience in data engineering and analytics• Experience in working with large datasets• Expertise in developing complex algorithms and models• Advanced knowledge of Python, SQL, data visualization tools, and big data technologies• Excellent communication and problem-solving skills• Proven ability to work in both a team and individual settingIf you have the necessary qualifications and are passionate about data, this is an incredible opportunity to join NVIDIA and become part of a world-class team.Responsibilities:Develop analytical models and build data-driven solutions that further NVIDIA's mission.Work collaboratively with data scientists, engineers, and other stakeholders.Utilize large datasets to develop complex algorithms and models.Utilize Python, SQL, data visualization tools, and big data technologies.Communicate effectively and possess excellent problem-solving skills.Work in both a team and individual setting.Maintain up-to-date knowledge of data engineering and analytics.NVIDIA is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.","The ideal candidate is a highly organized and motivated individual with a keen eye for detail and a passion for data; To be successful as a Senior Data Analysis Engineer at NVIDIA, you should have a strong background in data engineering and analytics, including working with large datasets, developing complex algorithms, and building data-driven models; You should also be comfortable working in both a team and an individual setting and possess excellent communication and problem-solving skills.In addition to the above, the successful candidate must possess the following qualifications:• BS/MS in Computer Science, Engineering, Mathematics, Statistics, or a related field• 5+ years of experience in data engineering and analytics• Experience in working with large datasets• Expertise in developing complex algorithms and models• Advanced knowledge of Python, SQL, data visualization tools, and big data technologies• Excellent communication and problem-solving skills• Proven ability to work in both a team and individual setting; If you have the necessary qualifications and are passionate about data, this is an incredible opportunity to join NVIDIA and become part of a world-class team; Utilize Python, SQL, data visualization tools, and big data technologies; Communicate effectively and possess excellent problem-solving skills; Work in both a team and individual setting; Maintain up-to-date knowledge of data engineering and analytics; 5 more items(s)","As a Senior Data Analysis Engineer, you will be an integral part of our organization and will be expected to work collaboratively with data scientists, engineers, and other stakeholders to develop analytical models and build data-driven solutions that further our mission; Develop analytical models and build data-driven solutions that further NVIDIA's mission; Work collaboratively with data scientists, engineers, and other stakeholders; Utilize large datasets to develop complex algorithms and models; 1 more items(s)","Python, SQL, data visualization tools, big data technologies"
Electrical/Data Engineer,ClassDojo,"ClassDojo • San Francisco, CA •  via ZipRecruiter",Full-time,No Degree Mentioned,"used in over 95% of US schools, reaching over 50 million children in 180 countries, with a team of just around 200 people [1]. We are now beginning to use this network to give kids the best learning experiences in the world, far beyond those a standard school can provide.We hire for talent density. Our team comprises the most talented, entrepreneurial, and innovative teammates from around the world, with experience in education and large scale consumer internet companies, including Instagram, Netflix, Dropbox, Stripe, Uber, Y Combinator, and more. We're building a company where the most talented people want to work. We believe you'll do the best work of your life here-and you'll pioneer the future of education, too.What you'll do:We are seeking a Data Platform engineer to join our Data and Engineering team. The ideal candidate will possess a passion for education and a strong foundation in all aspects of data engineering. In this pivotal role, you will be instrumental in making data accessible and actionable, facilitating informed decision-making throughout the company.You will be a match if:You have 7+ years experience with data / analytics engineeringCollaboration and Communication:• You have owned large-scale data projects that drove significant impact across the company• You enjoy collaborating with and supporting others.• You are an effective partner with engineering, data analysts, data scientists, and other business stakeholders.• You have an ownership mindset and can make things happenTechnical Excellence• You're a strong independent contributor who's comfortable working across the data stack• You've designed and built data platforms at previous organizations• Your SQL, python, and infrastructure skills are top-notch.[1] Some more context:(If you are on LinkedIn, you will not be able to access the hyperlinks below. Once you click apply, you will be directed to our career website (if you are not on there already) and will be able to access the hyperlinks)- ClassDojo's $125m Series D (Forbes) and Sam's note about it.- ClassDojo is one of Y Combinator's Top 100 companies- ClassDojo's Second Act Comes with First Profits (TechCrunch) and Sam's note about it.We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. In accordance with the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are happy to accommodate any disabilities or special needs. We are a distributed company, so we hire regardless of location, as long as you are willing to have significant hours overlap with one of the Americas time zones.ClassDojo takes a number of factors into consideration when determining compensation, including geographic location, experience, and skillset. Salary ranges (United States):CA, WA, NY, NJ, CT states: $171,500 - $244,000 (USD)All other states in the US: $146,000 - $207,500 (USD)#LI-Remote","The ideal candidate will possess a passion for education and a strong foundation in all aspects of data engineering; You have 7+ years experience with data / analytics engineering; You have owned large-scale data projects that drove significant impact across the company; You enjoy collaborating with and supporting others; You are an effective partner with engineering, data analysts, data scientists, and other business stakeholders; You have an ownership mindset and can make things happen; Technical Excellence; You're a strong independent contributor who's comfortable working across the data stack; You've designed and built data platforms at previous organizations; Your SQL, python, and infrastructure skills are top-notch; 7 more items(s)","CA, WA, NY, NJ, CT states: $171,500 - $244,000 (USD)","SQL, Python"
Azure Data Engineer with Strong Python,DCS Corp,"DCS Corp • Ridgecrest, CA •  via DCS Corp",17 days ago,Full-time,"airborne data/video.Essential Job Functions:Conduct Research, Design, Fabrication and Integration of Airborne, Telemetry and Ground based Data and Video Collection Systems, Post-Event Debrief and Analysis Ground Stations.Generate documentation, drawings and training related to the operation, maintenance, of the aforementioned data systems in accordance with Task Team Operating Procedures.Provide training and technical support for the aforementioned data systems to other team members (both engineers and technicians.Function as liaison to other stakeholder entities as appropriate.Provide summaries of activities in the Monthly Progress / Status Report.Interface with all levels of NAVAIR / NAWCWD organizations and stakeholder entities as required to accomplish assigned tasks.Participate in meetings with contractor and government personnel to resolve scheduling, planning, maintenance, and other technical issues.Actively contribute to organizational process improvement activities and task team internal reviews.Initiates own professional development and maintains an up-to-date understanding of all technical project requirements and organizational processes as defined in applicable documentation.Additional duties as assigned.Required Skills:Due to the sensitivity of customer related requirements, U.S. Citizenship is required.Must have one of the following: Bachelors with 5+ years of experience, Masters with 4+ years of experience, PHD with 2+ years of experience. Preferred degree concentration Electrical Engineering or a related discipline.Must be able to be granted a TS security clearance and maintain one for the duration of employment.Experience in the design, development, management, and administration of information systems related to airborne data acquisition, recording and telemetry.Possess teachability, team-orientation, sound character, resilience, a solutions-focused attitude, and tenacity in execution.Ability to perform, manage, and prioritize multiple related tasks in a highly dynamic and time-sensitive environment.Possession of excellent English oral and written communication skills.Possession of excellent English oral and written communication skills.Desired Skills:Practical knowledge of system administration, systems security, information assurance, network and computer operation support. This includes intimate familiarity with DoD-approved operating systems, network connectivity (both copper and fiber), and data warehousing/mining (including Storage Area Networks, Network Attached Storage and/or RAID technologies).Direct experience with post-event data processing and storage performed at NAWCWD China Lakes Range Control Center (RCC) Range Data Systems; training or experience with airborne and ground instrumentation systems, video and data recording/telemetry, MIL-STD-1553, and related data-bus and digital recorders.Ability and desire to work in a group environment.Diverse engineering work background (multiple disciplines or multiple technologies).Active security clearance, Secret or higher.Familiarity with modern military systems, both foreign and domestic, especially sensors technology.6192","Due to the sensitivity of customer related requirements, U.S. Citizenship is required; Must have one of the following: Bachelors with 5+ years of experience, Masters with 4+ years of experience, PHD with 2+ years of experience; Must be able to be granted a TS security clearance and maintain one for the duration of employment; Experience in the design, development, management, and administration of information systems related to airborne data acquisition, recording and telemetry; Possess teachability, team-orientation, sound character, resilience, a solutions-focused attitude, and tenacity in execution; Ability to perform, manage, and prioritize multiple related tasks in a highly dynamic and time-sensitive environment; Possession of excellent English oral and written communication skills; Possession of excellent English oral and written communication skills; Practical knowledge of system administration, systems security, information assurance, network and computer operation support; This includes intimate familiarity with DoD-approved operating systems, network connectivity (both copper and fiber), and data warehousing/mining (including Storage Area Networks, Network Attached Storage and/or RAID technologies); Direct experience with post-event data processing and storage performed at NAWCWD China Lakes Range Control Center (RCC) Range Data Systems; training or experience with airborne and ground instrumentation systems, video and data recording/telemetry, MIL-STD-1553, and related data-bus and digital recorders; Ability and desire to work in a group environment; Diverse engineering work background (multiple disciplines or multiple technologies); Active security clearance, Secret or higher; Familiarity with modern military systems, both foreign and domestic, especially sensors technology; 12 more items(s)","Salary Range: $79,616 - $110,000","DoD-approved operating systems, Storage Area Networks, Network Attached Storage, RAID technologies"
Founding Engineer - Full-Stack Development & Data Engineering,BlueRose Technologies,"BlueRose Technologies • Fremont, CA •  via LinkedIn",8 days ago,Contractor,"SQL DB, Fabric.• Proven experience with SQL, namely schema design and dimensional data modelling• Solid knowledge of data warehouse best practices, development standards and methodologies• Strong experience with Azure Cloud on data integration with Databricks• Be an independent self-learner with the “let’s get this done” approach and ability to work in Fast paced and Dynamic environment","12+ months; Hands on Interview on SQL, Python/Pyspark; Must have Skills : Azure, ADLS, Databricks, Data Factory, ADF, Synapse, Python, SQL; Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric; Proven experience with SQL, namely schema design and dimensional data modelling; Solid knowledge of data warehouse best practices, development standards and methodologies; Strong experience with Azure Cloud on data integration with Databricks; Be an independent self-learner with the “let’s get this done” approach and ability to work in Fast paced and Dynamic environment; 5 more items(s)",,"SQL DB, Fabric, SQL, Azure, ADLS, Databricks, Data Factory, ADF, Synapse, Python, Pyspark, Azure Data Factory, Synapse/DW, Azure SQL DB"
"Lead Data Engineer (Sunnyvale/San Jose, CA)",just words,"just words • San Francisco, CA •  via Hiregeeks",120K–180K a year,Full-time,"Join as a Founding Engineer in San Francisco, focusing on Full-Stack Development & Data Engineering at a dynamic AI marketing startup.",,,There are no specific tools or software mentioned in this job description to extract.
Network Data Engineer,"Elekta, Inc.","Elekta, Inc. • Sunnyvale, CA •  via Workday",Full-time,Dental insurance,"g opportunity available - please contact us for more details! Location: Sunnyvale/San Jose (Hybrid-3 days/wk. on-site) We don’t just build technology. We build hope for everyone dealing with cancer. Do you wish you could make a bigger impact? At Elekta, you can make a difference. Our software engineers connect thousands of patients to care each day. And that is just the beginning. Ideal candidates should be self-motivated, thrive in a fast-paced environment and are always searching for a better way. If you are creative, smart, and work best in teams, we’d like to talk to you.   Elekta’s growing software development engineering team has opportunities for a lead data engineer in development and testing of new and existing data management functionality and architecture. Our product teams thrive in a scaled, agile environment. Architects, product owners, designers and developers collaborate closely to rethink and reimagine. We need your expertise to create software that is simple, elegant, and easy to use. We want to make sure that physicians spend their time fighting cancer, not software. Elekta is seeking a detail-oriented and highly motivated individual to join our Oncology Information Systems (OIS) engineering team as an experienced Lead Data Engineer. In this role, you will be responsible for reviewing, redesigning, developing, and executing changes to the implementation of the data management and relational, binary, and document-based database infrastructure of MOSAIQ, Elekta’s certified health information system that manages the treatment of cancer patients. You will work closely with a team of experienced engineers and oncology domain experts to ensure the delivery of high-quality software and data solutions. Please note that this role is focusing on an OLTP system (transactional system), and not on ETL, pipelines, or analytics. In addition, this role is in context of a software product that is individually deployed for customers on-premises and in the cloud. Responsibilities: Collaborate with the development team to understand software and data requirements and identify test scenarios for automation and manual testing Review the current data management and database strategy and develop a plan for system throughput increase and end user latency reduction, on a database as well as database access functionality level Devise a strategy, proof of concepts and product implementation to simplify and streamline data management and database functionality Assess the API and system integration strategy from a data management viewpoint and provide feedback on the current implementation as well as develop advanced interface strategies for external system integration (asynchronous as well as synchronous) Implement and deliver data management and database management functionality to address performance, scalability and operational requirements required Identify and prioritize test cases for automation, considering factors such as criticality, risk, appropriateness for automation and frequency of use Report and track defects in the defect management system, and work with the development team to resolve them Conduct system performance and stability analysis and optimization to ensure scalability, reliability and security Evaluate design alternatives and perform cost benefit analysis, concept exploration and prototyping when necessary What you will bring: To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill and/or ability required. Bachelor’s Degree in Engineering, Computer Science, Computer Information Systems, related field, or equivalent practical experience, Master’s degree preferred Experience in software engineering in context of an installable software product at customers’ sites or in the cloud Experience in data management and database technologies from at least one public cloud provider like Google Cloud, Microsoft Azure, or Amazon Web Services Experience in data management systems and databases for on-premises deployment in addition to cloud deployments Experience with version control systems, such as Git Familiarity with agile software development methodologies and processes Excellent problem-solving and analytical skills Strong attention to detail and ability to effectively prioritize Good communication and collaboration skills Experience with continuous integration/continuous delivery (CI/CD) pipelines and tools What you’ll get In this role, you will work for a higher purpose; hope for everyone dealing with cancer, and for everyone regardless of where in the world, to have access to the best cancer care. In addition to this, Elekta offers a range of benefits. What we offer: Opportunity to work with a proactive and supportive team Hybrid work option (you are required to work on location at least 3 days/week) Excellent Medical, Dental and Vision coverage 401k, paid Vacation and Holiday A health of additional benefits including wellness reimbursement, tuition reimbursement and flexible spending account Opportunity to work on cutting edge in medical advancement. Close-knit company culture Upward mobility How to proceed? We are looking forward to hearing from you! Apply by submitting your application and résumé in English, via the “Apply” button. Please note that we do not accept applications by e-mail. We are an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, disability, veteran status, or any other protected characteristic. The US base salary range for this position is $ 175,000- $195,000. About Elekta As a leader in precision radiation therapy, Elekta is committed to ensuring every patient has access to the best cancer care possible. We openly collaborate with customers to advance sustainable, outcome-driven and cost-efficient solutions to meet evolving patient needs, improve lives and bring hope to everyone dealing with cancer. To us, it's personal, and our global team of 4,700 employees combine passion, science, and imagination to profoundly change cancer care. We don’t just build technology, we build hope. Elekta is headquartered in Stockholm, Sweden, with offices in more than 120 countries and listed on Nasdaq Stockholm. For more information, visit elekta.com or follow us on LinkedIn, Glassdoor, Twitter, Facebook, Instagram and YouTube.",,,"Google Cloud, Microsoft Azure, Amazon Web Services, Git"
Data Scientist and AI Engineer,Tekfortune Inc.,"Tekfortune Inc. • San Francisco, CA •  via ZipRecruiter",Full-time,No Degree Mentioned,job for you.Role:Location:Duration:Required Skills:Job Description:For more information and other jobs available please contact our recruitment team at careers@tekfortune.com. To view all the jobs available in the USA and Asia please visit our website at https://www.tekfortune.com/careers/.,,,No tools or software are mentioned in the job description provided.
Business Data Engineer,SKT Lab,"SKT Lab • Santa Clara, CA •  via Indeed",Full-time,,perience with big data tools• Experience with relational SQL and NoSQL databases• Developing in R and Python• Experience working with Hadoop /Spark• Have Knowledge of Machine learning (supervised/unsupervised) & common ML frameworks,"We are looking for a candidate with 5+ years of experience in a Data Scientist role, who has Bachelor's or Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field; Experience with big data tools; Experience with relational SQL and NoSQL databases; Developing in R and Python; Experience working with Hadoop /Spark; Have Knowledge of Machine learning (supervised/unsupervised) & common ML frameworks; 3 more items(s)",,"SQL, NoSQL, R, Python, Hadoop, Spark"
"Research Software Engineer, Data Quality",ClifyX,"ClifyX • Sunnyvale, CA •  via LinkedIn",Full-time,,"and diversified profile)JD:In the role of Lead Data Engineer, you will interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation, Application Architecture definition and Design. You will play an important role in creating the high-level design artifacts. You will also deliver high quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.Required Qualifications:Bachelor's degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of educationAt least 6 years of experience in Information Technology.At least 3 years of hands on experience with Hadoop distributed frameworks while handling large amount of big data using Spark and Hadoop Ecosystems.At Least 3 Years Of Experience With Spark/PySpark Required.At least 2 years of experience with Scala required.At least 3 years of experience with SQL with any RDBMS.Preferred Qualifications:At least 1 years of AWS development experience is preferredAbility to work within deadlines and effectively prioritize and execute on tasks.Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels.Experience in Drive automationsDevOps Knowledge is an added advantage.","Bachelor's degree or foreign equivalent required from an accredited institution; Will also consider three years of progressive experience in the specialty in lieu of every year of education; At least 6 years of experience in Information Technology; At least 3 years of hands on experience with Hadoop distributed frameworks while handling large amount of big data using Spark and Hadoop Ecosystems; At Least 3 Years Of Experience With Spark/PySpark Required; At least 2 years of experience with Scala required; At least 3 years of experience with SQL with any RDBMS; Ability to work within deadlines and effectively prioritize and execute on tasks; Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels; Experience in Drive automations; DevOps Knowledge is an added advantage; 8 more items(s)",Rate: MSA (Can go up to $84 max if the candidate is strong in technical & communication skills and diversified profile),"Hadoop, Spark, PySpark, Scala, SQL, AWS, RDBMS, DevOps"
Data Engineer (Informatica/SQL/Teradata),Walmart,"Walmart • Piedmont, CA •  via WiseSpotlight",2 days ago,Full-time,"dvertising data analytics related to demand, supply, and overall marketplace health. You will be responsible for extracting meaningful insights about campaign performance, marketplace efficiency, and gaps in systems and products, surfacing marketplace health metrics via dashboards, and working with cross-functional partners to move the needle in display advertising.What You Will DoBuilding data pipelines and dashboards to monitor display ads serving funnel and marketplace health.Leading analytics for display ads marketplace and presenting findings and recommendations to engineering teams and cross-functional partners.Collaborating with engineering and data science leads to improve data quality, building data warehouse, and delivering predictive models for marketplace insights and anomaly detection.Bringing data-driven culture to the engineering team to assure product and system quality.What You'll BringBachelor's degree in data science, computer science, statistics, operation research, or related fields.6 years' experience of building data pipelines, extracting signals from noisy data, and establishing metrics for monitoring.Proficiency in data analysis tools (e.g., Python, R, SQL) and data visualization tools (e.g., Tableau, Superset, Looker).Analytical thinking and detail-oriented mindset. Familiar with SQL and Non-SQL databases.Strong communication skills. Ability to convey complex ideas and findings to non-technical stakeholders.Knowledge of A/B testing.Preferred QualificationsAdvanced degree in data science, computer science, statistics, operation research, or related fields.6+ years' experience in programmatic advertising, online content distribution, or e-commerce.Knowledge of optimization, auction, and ML applications.People management experience is a big plus.About Walmart Global TechImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That is what we do at Walmart Global Tech. We are a team of software engineers, data scientists, cybersecurity experts, and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity and those looking for the opportunity to define their career. Here, you can kickstart a distinguished career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.Flexible, Hybrid WorkWe use a hybrid way of working that is primarily in the office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.BenefitsBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.Equal Opportunity EmployerWalmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity - unique styles, experiences, identities, ideas, and opinions - while being inclusive of all people.Minimum QualificationsOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.Option 1: Bachelor's degree in Computer Science and 5 years' experience in software engineering or related field.Option 2: 7 years' experience in software engineering or related field.Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.4 years' experience in data engineering, database engineering, business intelligence, or business analytics.Preferred QualificationsOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud.Master's degree in Computer Science or related field and 5 years' experience in software engineering or related field.We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture.Primary Location840 W CALIFORNIA AVE, SUNNYVALE, CA 94086-4828, United States of America#J-18808-Ljbffr","Bachelor's degree in data science, computer science, statistics, operation research, or related fields.6 years' experience of building data pipelines, extracting signals from noisy data, and establishing metrics for monitoring; Proficiency in data analysis tools (e.g., Python, R, SQL) and data visualization tools (e.g., Tableau, Superset, Looker); Analytical thinking and detail-oriented mindset; Familiar with SQL and Non-SQL databases; Strong communication skills; Ability to convey complex ideas and findings to non-technical stakeholders; Knowledge of A/B testing; Advanced degree in data science, computer science, statistics, operation research, or related fields.6+ years' experience in programmatic advertising, online content distribution, or e-commerce; Knowledge of optimization, auction, and ML applications; People management experience is a big plus; That means understanding, respecting and valuing diversity - unique styles, experiences, identities, ideas, and opinions - while being inclusive of all people; Option 1: Bachelor's degree in Computer Science and 5 years' experience in software engineering or related field; Option 2: 7 years' experience in software engineering or related field; Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.4 years' experience in data engineering, database engineering, business intelligence, or business analytics; Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud; Master's degree in Computer Science or related field and 5 years' experience in software engineering or related field.We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly; The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture; 14 more items(s)","Beyond our great compensation package, you can receive incentive awards for your performance; Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more","Python, R, SQL, Tableau, Superset, Looker, ETL"
Data engineers - Graph DB,Mutual of Omaha,"Mutual of Omaha • Remote, OR •  via Indeed",9 hours ago,95K–145K a year,"ions and other key functions. If you are an experienced engineer who is excited by new challenges, apply today to shape the future of Mutual of Omaha for our associates and customers!WHAT WE CAN OFFER YOU:• Estimated Salary (Levels have variable responsibilities and qualifications):• Engineer I: $95,000 - $115,000, plus annual bonus opportunity.• Engineer II:$115,000 - $130,000, plus annual bonus opportunity.• Engineer III $130,000 - $145,000, plus annual bonus opportunity.• Benefits and Perks, 401(k) plan with a 2% company contribution and 6% company match.• Regular associates working 40 hours a week can earn up to 15 days of vacation each year.• Regular associates receive 11 paid holidays in 2024, which includes 2 floating holidays that are added to your prorated personal time to be used at your discretion.• Regular associates are provided sick leave through the use of personal time. Associates working 40 hours a week can receive up to 40 hours of personal time in 2024, which is prorated based on the start date. Additionally you will receive two floating holidays in 2024 by way of personal time that may be used at your discretion.WHAT YOU'LL DO:• Research, design, build, optimize, test, integrate, and maintain reliable, efficient and accessible data systems, data pipelines, and/or models as defined by the product owners and business teams.• Support, with initial guidance, the analytic and/or operational use of data.• Collaborate with business owners to optimize data collection, storage and usage to maximize the value of information within supported systems.• Translate business needs into data architecture solutions, designing and implementing in production environments within supported data systems.• Deploys applications to production in partnership with business units.WHAT YOU’LL BRING:• 5+ years of experience solving build, development, and implementation challenges using Informatica Power Center/IDMC , SQL and Teradata.• Experience working in Agile methodology (SAFe) and familiarity with Jira.• Knowledge of ETL/ELT.• Data Warehouse Design, Data Modeling and Integration.• Ability to maintain data quality through systematic approaches and methodologies (e.g. MDM) within supported data systems.• You promote a culture of diversity and inclusion, value different ideas and opinions, and listen courageously, remaining curious in all that you do.• Able to work remotely with access to a high-speed internet connection and located in the United States or Puerto Rico.PREFERRED:• Experience in Snowflake, Python, Github Integrations and Control M.• Amazon Web Services such as S3, Glue for Data Processing.• Knowledge of the insurance industry including products relevant to Mutual of Omaha.We value diverse experience, skills, and passion for innovation. If your experience aligns with the listed requirements, please apply!If you have questions about your application or the hiring process, email our Talent Acquisition area at careers@mutualofomaha.com. Please allow at least one week from time of applying if you are checking on the status.#CircaNeed help? Email UsGreat place to workTogether we achieve greatness. Not only is this a core value, but it’s also representative of the kind of place we are — built by the strength and integrity of our employees. It’s why we’re named a “Great Place to Work”.See All AwardsAn inclusive cultureSurround yourself with an authentic and inclusive culture. Your strengths and differences will be valued and celebrated by a diverse community of co‑workers.Discover Our CultureRelated Job OpeningsInformatica Data Engineer- Development & Data Modeling SpecialistVarious Locations | 503318Summer 2025 Information Technology Intern – RemoteRemote, Nebraska | 503339Summer 2025 Analytics Engineer Intern - RemoteRemote, Nebraska | 503351","5+ years of experience solving build, development, and implementation challenges using Informatica Power Center/IDMC , SQL and Teradata; Experience working in Agile methodology (SAFe) and familiarity with Jira; Knowledge of ETL/ELT; Data Warehouse Design, Data Modeling and Integration; Ability to maintain data quality through systematic approaches and methodologies (e.g; MDM) within supported data systems; You promote a culture of diversity and inclusion, value different ideas and opinions, and listen courageously, remaining curious in all that you do; Able to work remotely with access to a high-speed internet connection and located in the United States or Puerto Rico; We value diverse experience, skills, and passion for innovation; 6 more items(s)","Estimated Salary (Levels have variable responsibilities and qualifications):; Engineer I: $95,000 - $115,000, plus annual bonus opportunity; Engineer II:$; 115,000 - $130,000, plus annual bonus opportunity; Engineer III $130,000 - $145,000, plus annual bonus opportunity; Benefits and Perks, 401(k) plan with a 2% company contribution and 6% company match; Regular associates working 40 hours a week can earn up to 15 days of vacation each year; Regular associates receive 11 paid holidays in 2024, which includes 2 floating holidays that are added to your prorated personal time to be used at your discretion; Regular associates are provided sick leave through the use of personal time; Associates working 40 hours a week can receive up to 40 hours of personal time in 2024, which is prorated based on the start date; Additionally you will receive two floating holidays in 2024 by way of personal time that may be used at your discretion; Surround yourself with an authentic and inclusive culture; 9 more items(s)","Informatica Power Center/IDMC, SQL, Teradata, Agile methodology (SAFe), Jira, Snowflake, Python, Github Integrations, Control M, Amazon Web Services, S3, Glue"
Back-End Data Engineer (Big data/ Cloud),Diverse Lynx,"Diverse Lynx • Newark, CA •  via Recruitment Hires",9 days ago,Full-time,"ent, and presentation skills- Create and maintain reports, create and manage data models, leverage data across complex hierarchies using multiple data sources.- Experience in building ETL pipelines.- Leverage process improvement techniques to drive improvements in data quality.- Perform testing to support system implementations and upgradesDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.",Ability to rapidly learn and adapt to business changes; Experience in building ETL pipelines,"Fluency in Advanced SQL (complex joins, stored procedures, subqueries, window functions, performance optimization, etc.), Snowflake, Python; Provide analytical reporting and analytics to the operations team and external partners; Ability to operate in a fast paced, rapidly changing environment; Excellent communication, project management, and presentation skills; Create and maintain reports, create and manage data models, leverage data across complex hierarchies using multiple data sources; Leverage process improvement techniques to drive improvements in data quality; Perform testing to support system implementations and upgrades; 4 more items(s)","Advanced SQL, Snowflake, Python"
"Senior Data Engineer at Fiserv Berkeley, CA",San Jose State University,"San Jose State University • Campbell, CA •  via Adzuna",Full-time,Health insurance,"housing/Business Intelligence implementation. This position will play a key role in the research, design, architecture, creation, development, implementation and maintenance of Campus Data Warehouse. This includes identifying and pulling data from 500+ individual databases across SJSU and curating them into a single data source in order to facilitate data-driven decisions.The Lead Data Engineer should possess a breadth of knowledge, technical skills, and strategic thinking to build a Data Warehouse to answer important questions across a variety of functional areas and collaborate with business stakeholders and IT management to understand solution requirements and system design for delivering the data model, developing and implementing the solution to deliver the desired business outcomes. Assist in conducting technical reviews, creating definitions of business problems, including the preparation of business/technical requirements to support the mission of the university and administrative departments, provide systems and technical support of vendor and locally developed software, and work with the Data Warehouse Project Manager, SJSU IT Enterprise Solutions resources, external vendors, and IT team members. Duties and system assignments can be temporarily or permanently changed to meet the needs and goals of the department and university.Key Responsibilities- Lead role where the position defines and oversees how the organization captures, maintains, and applies data and information in order to support key business processes.- Implement the strategic direction for building data management programs and optimizing how the organization uses data both internally and externally.- Evaluate data that recognizes trends, calling insights, and making recommendations to the executive leadership team to make recommendations to define business strategy.- Develop the data strategy and vision for building the analytic ecosystem to evaluate, acquire, integrate, and consume data that drive insights.- Define cloud strategy and lead cross-disciplinary teams to design and implement a scalable future state analytical platform architecture.- Assists with the development of strategies and coordinates the implementation that support the Organization's development plans and that capture new opportunities.- Partner cohesively with the broader Insights leaders across campus concerning the alignment capabilities.- Oversee the implementation of analytic methodologies including segmentation, predictive modeling, advanced statistical methods, regression, experimental design, etc.- Lead and own the interaction with campus technical support staff and customers to provide reliable services.- Proposes and leads analytics and organization intelligence initiatives in the Data and Analytics area.- Defines appropriate analytical models necessary to support use cases and leverages the power of predictive insights and analytics.- Defines and enables teams to implement augmented intelligence and machine learning algorithms and models for efficient processing and delivery of analytics.Knowledge, Skills & Abilities- Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources- Strong interpersonal, communication, organization and planning skills- Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission- Advanced technical knowledge in IT systems and emerging technology trends and issues- Ability to synthesize data from multiple sources to address complex business questions- Advanced knowledge with analytics tools, ""big data"" technologies, cloud computing environments, relational database- Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations- Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions- Strong ability to deliver on high-impact analytics projects- Skilled in recruiting, developing, inspiring, leading a growing team of data analysts.- Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases- Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript- Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak- Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets- Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc.Required Qualifications- A bachelor's degree, preferably in computer science or business, or equivalent training and applied experience- Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computersPreferred Qualifications- Master's in Business Administration, Computer Science or equivalent degree- 3+ years' experience leading a team- 3+ years' experience developing, maintaining and collecting structured and unstructured data sets for analysis and reporting- 3+ years advanced analytics experience in support of business strategy.- 3+ years working experience in implementing public cloud solutions.CompensationClassification: Analyst/Programmer - ExpertCSU Salary Range: $7,020/month - $13,594/monthSan José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary. For more information on programs available, please see the Employee Benefits Summary (https://www2.calstate.edu/csu-system/careers/benefits/Documents/employee-benefits-summary.pdf) .Application ProcedureClick Apply Now to complete the SJSU Online Employment Application and attach the following documents:- Resume- Letter of InterestAll applicants must apply within the specified application period: May 16, 2024 through May 30, 2024. This position is open until filled; however, applications received after screening has begun will be considered at the discretion of the university.Contact InformationUniversity Personneljobs@sjsu.edu408-924-2252CSU Vaccination PolicyThe CSU strongly recommends that all individuals who access any in-person program or activity (on- or off-campus) operated or controlled by the University follow COVID-19 vaccine recommendations adopted by the U.S. Centers for Disease Control and Prevention (CDC) and the California Department of Public Health (CDPH) applicable to their age, medical condition, and other relevant indications and comply with other safety measures established by each campus. The system wide policy can be found at https://calstate.policystat.com/policy/9779821/latest/ and questions may be sent to jobs@sjsu.edu.Additional InformationSatisfactory completion of a background check (including a criminal records check) is required for employment. SJSU will issue a contingent offer of employment to the selected candidate, which may be rescinded if the background check reveals disqualifying information, and/or it is discovered that the candidate knowingly withheld or falsified information. Failure to satisfactorily complete the background check may affect the continued employment of a current CSU employee who was offered the position on a contingent basis.The standard background check includes: criminal check, employment and education verification. Depending on the position, a motor vehicle and/or credit check may be required. All background checks are conducted through the university's third party vendor, Accurate Background. Some positions may also require fingerprinting. SJSU will pay all costs associated with this procedure. Evidence of required degree(s) or certification(s) will be required at time of hire.SJSU IS NOT A SPONSORING AGENCY FOR STAFF OR MANAGEMENT POSITIONS. (e.g. H1-B VISAS)All San José State University employees are considered mandated reporters under the California Child Abuse and Neglect Reporting Act and are required to comply with the requirements set forth in CSU Executive Order 1083 as a condition of employment. Incumbent is also required to promptly report any knowledge of a possible Title IX related incident to the Title IX Office or report any discrimination, harassment, and/or retaliation to the Office of Equal Opportunity.Jeanne Clery Disclosure of Campus Security Policy and Crime Statistics Act and Campus Housing Fire Safety Notification:Pursuant to the Jeanne Clery Disclosure of Campus Security Policy and Campus Crime Statistics Act, the Annual Security Report (ASR) is also now available for viewing at https://www.sjsu.edu/clery/docs/SJSU-Annual-Security-Report.pdf. The ASR contains the current security and safety-related policy statements, emergency preparedness and evacuation information, crime prevention and Sexual Assault prevention information, and information about drug and alcohol prevention programming. The ASR also contains statistics of Clery crimes for San José State University locations for the three most recent calendar years. A paper copy of the ASR is available upon request by contacting the Office of the Clery Director by phone at 408-924-1501 or by email at clerycompliance@sjsu.edu.Pursuant to the Higher Education Opportunity Act, the Annual Fire Safety Report (AFSR) is also available for viewing at https://www.sjsu.edu/clery/docs/SJSU-Annual-Fire-Safety-Report.pdf. The purpose of this report is to disclose statistics for fires that occurred within SJSU on-campus housing facilities for the three most recent calendar years, and to distribute fire safety policies and procedures intended to promote safety on Campus. A paper copy of the AFSR is available upon request by contacting the Housing Office by phone at 408-795-5600 or by email at uhs-frontdesk@sjsu.edu.Campus Security Authority - In accordance with the Jeanne Clery Disclosure of Campus Security Policy and Campus Crime Statistics Act (Clery Act) and CSU systemwide policy, this position is subject to ongoing review for designation as a Campus Security Authority. Individuals that are designated as Campus Security Authorities are required to immediately report Clery incidents to the institution and complete Clery Act training as determined by the university Clery Director.Equal Employment StatementSan José State University (SJSU) is an Equal Opportunity/Affirmative Action employer committed to nondiscrimination on the basis of age, ancestry, citizenship status, color, creed, disability, ethnicity, gender, genetic information, marital status, medical condition, national origin, race, religion or lack thereof, sex, sexual orientation, transgender, or protected veteran status consistent with applicable federal and state laws. This policy applies to all SJSU students, faculty and staff programs and activities. Title IX of the Education Amendments of 1972, and certain other federal and state laws, prohibit discrimination on the basis of sex in all education programs and activities operated by the university (both on and off campus).Advertised: May 16, 2024 (9:00 AM) Pacific Daylight TimeApplications close:Back to search results Apply now (https://secure.dc4.pageuppeople.com/apply/873/gateway/default.aspx?c=apply&lJobID=539662&lJobSourceTypeID=837&sLanguage=en-us) Refer a friend","Advanced skills in strategic planning, analytical and holistic problem-solving, project management, effective communication and coordination of internal and external resources; Strong interpersonal, communication, organization and planning skills; Ability to understand complex challenges and lead teams of technologists to deploy appropriate technological solutions that meet campus vision and mission; Advanced technical knowledge in IT systems and emerging technology trends and issues; Ability to synthesize data from multiple sources to address complex business questions; Advanced knowledge with analytics tools, ""big data"" technologies, cloud computing environments, relational database; Advanced working knowledge in large ERP scale analytics, optimization, business intelligence, and statistics for IT organizations; Strong ability to provide strategic guidance on emerging technologies to innovate and ensure the SJSU is well-positioned in delivering analytic solutions; Strong ability to deliver on high-impact analytics projects; Skilled in recruiting, developing, inspiring, leading a growing team of data analysts; Advanced working SQL knowledge and skills working with relational databases like Oracle, query authoring (SQL), as well as, working familiarity with a variety of databases; Expert knowledge in high level programming languages like: Python, SQL, Java, and JavaScript; Advanced knowledge in application architectures on public cloud platform such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or IBM Cloud Pak; Advanced knowledge of enterprise data architecture, ETL integration, data warehousing techniques, analytics/end-user reporting tool sets; Advanced knowledge in Visualization tools like Google Data Studio, Tableau, Looker etc; A bachelor's degree, preferably in computer science or business, or equivalent training and applied experience; Five years of experience in business applications analysis, design, and programming for medium or large scale, multi-programmed computers; Satisfactory completion of a background check (including a criminal records check) is required for employment; The standard background check includes: criminal check, employment and education verification; Depending on the position, a motor vehicle and/or credit check may be required; All background checks are conducted through the university's third party vendor, Accurate Background; Some positions may also require fingerprinting; Evidence of required degree(s) or certification(s) will be required at time of hire; 20 more items(s)","CSU Salary Range: $7,020/month - $13,594/month; San José State University offers employees a comprehensive benefits package typically worth 30-35% of your base salary","Oracle, SQL, Python, Java, JavaScript, Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud Pak, Google Data Studio, Tableau, Looker"
Senior Big Data Engineer,Fiserv,"Fiserv • Berkeley, CA •  via Colorspray Formulator",17 days ago,Full-time,"it card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.Job TitleSenior Data Engineer What does a successful Senior Data Engineer do at Fiserv?As a member of our Data Commerce Solutions group, you will build and take ownership of the design and development of data engineering projects within Fiserv’s Enterprise Data Commerce Solutions division. You will be responsible for building and taking ownership over large-scale data engineering, integration, and warehousing projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by using the Data integration tool with coding across several languages such as Java, Python, and SQL. Additional responsibilities include, but are not limited to architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.What you will do:Collaborate with cross-functional teams to design scalable data architecture and create robust data processing pipelines.Design and implement data models that align with business requirements, enabling seamless data access and analytics.Identify opportunities to enhance data processing efficiency and implement performance optimizations for our data pipelines.Implement data quality checks and validation processes to ensure the accuracy and integrity of our data.Work closely with data scientists, analysts, and software engineers to understand data needs, provide technical support, and troubleshoot data-related issues.Stay up to date with the latest data engineering technologies and tools and recommend improvements to our existing systems.Maintain comprehensive documentation of data engineering processes, data flows, and system configurations.What you will need to have:6-10 years of overall industry experience, with at least 6+ years' experience in building large-scale big data applications development, and a bachelor’s degree in computer science or a related fieldPossess strong technical leadership skills, demonstrating expertise in developing data solutions, building frameworks, and designing solutions for processing large volumes of data using data processing tools and Big Data platforms.Hands-on experience building Data Lake, EDW, and data applications on Azure cloud. Proficiency in major programming/scripting languages like Java, and/or PythonStrong understanding of cluster and parallel architecture, experience with high-scale databases and SQL, and exposure to NoSQL databases like Cassandra, HBase, DynamoDB, and Elastic SearchConduct code reviews and strive for improvement in software engineering quality.Experience in real-time data processing and streaming technologies like Kafka and Apache Beam, as well as a successful track record in delivering big data projects using Kafka and SparkWhat would be great to have:Experience in Banking, Financial domainAdvanced certifications in data engineering or related fields.Familiarity with machine learning frameworks and data science workflows.Knowledge of containerization technologies like Docker and orchestration tools like Kubernetes.Experience working with PCI Data and collaborating with data scientists. Knowledge of data governance, security, and privacy principle.This role is not eligible to be performed in Colorado, California, Hawaii, New York or Washington.Please note that salary ranges provided for this role on external job boards are salary estimates made by outside parties and may not be accurate.Thank you for considering employment with Fiserv. Please:Apply using your legal nameComplete the step-by-step profile and attach your resume (either is acceptable, both are preferable).What you should know about us:Fiserv is a global leader in payments and financial technology with more than 40,000 associates proudly serving clients in more than 100 countries. As one of Fortune ® magazine's World's Most Admired Companies™ 9 of the last 10 years, one of Fast Company’s Most Innovative Companies, and a top scorer on Bloomberg’s Gender-Equality Index, we are committed to innovation and excellence.Our commitment to Diversity and Inclusion:Fiserv is an Equal Opportunity Employer, and we welcome and encourage diversity in our workforce that reflects our world. All qualified applicants will receive consideration for employment without regard to race, color, religion, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other category protected by law.We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.Warning about fake job posts:Please be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information.Any communications from a Fiserv representative will come from a legitimate business email address. We will not hire through text message, social media, or email alone, and any interviews will be conducted in person or through a secure video call. We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process. We also won’t send you a check to cash on Fiserv’s behalf.If you see suspicious activity or believe that you have been the victim of a job posting scam, you should report it to your local FBI field office or to the FBI’s Internet Crime Complaint Center.","6-10 years of overall industry experience, with at least 6+ years' experience in building large-scale big data applications development, and a bachelor’s degree in computer science or a related field; Possess strong technical leadership skills, demonstrating expertise in developing data solutions, building frameworks, and designing solutions for processing large volumes of data using data processing tools and Big Data platforms; Hands-on experience building Data Lake, EDW, and data applications on Azure cloud; Proficiency in major programming/scripting languages like Java, and/or Python; Strong understanding of cluster and parallel architecture, experience with high-scale databases and SQL, and exposure to NoSQL databases like Cassandra, HBase, DynamoDB, and Elastic Search; Experience in Banking, Financial domain; Advanced certifications in data engineering or related fields; Familiarity with machine learning frameworks and data science workflows; Knowledge of containerization technologies like Docker and orchestration tools like Kubernetes; Experience working with PCI Data and collaborating with data scientists; Knowledge of data governance, security, and privacy principle; This role is not eligible to be performed in Colorado, California, Hawaii, New York or Washington; 9 more items(s)","You will be responsible for building and taking ownership over large-scale data engineering, integration, and warehousing projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by using the Data integration tool with coding across several languages such as Java, Python, and SQL; Additional responsibilities include, but are not limited to architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create; Collaborate with cross-functional teams to design scalable data architecture and create robust data processing pipelines; Design and implement data models that align with business requirements, enabling seamless data access and analytics; Identify opportunities to enhance data processing efficiency and implement performance optimizations for our data pipelines; Implement data quality checks and validation processes to ensure the accuracy and integrity of our data; Work closely with data scientists, analysts, and software engineers to understand data needs, provide technical support, and troubleshoot data-related issues; Stay up to date with the latest data engineering technologies and tools and recommend improvements to our existing systems; Maintain comprehensive documentation of data engineering processes, data flows, and system configurations; Conduct code reviews and strive for improvement in software engineering quality; Experience in real-time data processing and streaming technologies like Kafka and Apache Beam, as well as a successful track record in delivering big data projects using Kafka and Spark; 8 more items(s)","Java, Python, SQL, Azure, Cassandra, HBase, DynamoDB, Elastic Search, Docker, Kubernetes, Kafka, Apache Beam, Spark"
Staff Data Engineer (IRIS),Lightspark,"Lightspark • Culver City, CA •  via Jobs",Full-time,,"problems for our customers with best-in-class innovative software solutions. We aim to help businesses around the world benefit from real time payments and build amazing new experiences for themselves and their customers. Lightspark is headquartered in Los Angeles but serving the world.Lightspark is seeking a Senior Data Engineer to join the Data Science team. As a Senior Data Engineer at Lightspark, you will have the opportunity to lead core data engineering initiatives that touch many aspects of the company – from Product and Engineering to Strategy and Legal, to Accounting and Finance.Data Engineers at Lightspark are highly entrepreneurial. You must be passionate about building and excited to tackle an array of interesting problems. The role is a fusion of a Strategic Partner and a full-stack Data Scientist.You will have a strong sense of ownership and the ability to lead and shape Lightspark’s business and data engineering initiatives. You must be adept at navigating various functions while also operating independently. You will have the capacity to wield both analytical and strategic capabilities across domains. This will span everything from analyzing current operations to charting long-term initiatives that will drive Lightspark’s path to success.WHAT YOU'LL BE DOING:• Designing and measuring key product, operational, and risk metrics to surface critical insights to the Strategy, Data Science, and Engineering teams.• Conceptualize, build, and own Lightspark’s data analytics architecture and infrastructure, including overall design, data models, and data pipelines.• Working with our Machine Learning, Data Science, and Engineering teams to build scalable and efficient data models to advance Lightspark’s product and strategy initiatives.• Design, lead, and implement security models for Lightspark’s data systems, including privacy requirements, governance processes, and data quality controls.• Operate as a strategic partner from Data Science to functional teams, using strategic thinking and technical excellence to inform key business and product decisions.• Guide and inform Lightspark’s business operations, supporting Lightspark’s Legal, Finance, and Accounting teams to ensure Lightspark’s business operations are documented accurately.• Analyzing large and complex datasets to guide Lightspark’s strategic priorities.WHAT WE ARE LOOKING FOR:• M.S / BA / BS degree in a technical field (Mathematics, Statistics, Economics, Computer Science) with 5+ years of experience or a Ph.D in a technical field with 2+ years of experience• Experience working in a fast-paced, startup environment ideally in tech, payments, crypto, or blockchain companies• Experience working on a wide variety of problems cross-functionally• Experience understanding and applying statistical concepts (including causal inference, machine learning, and experimentation).• Experience working as a full-stack Data Scientist, including writing data pipelines, modeling theory, writing code, and deploying applications in production systems.• Experience programming and modeling in Python and data analysis in Pandas / Numpy / SQL.• Experience with ETL, data pipelines, and workflow management tools such as Stitch, Fivetran, or Airflow.• Experience architecting and using massively parallel processing technology, such as Google Bigquery, Snowflake, Redshift, etc.• Self-motivated and able to work independently• Willing to live or relocate to our HQ in Los Angeles, CaliforniaLightspark is on a mission to build an open payment protocol for the Internet at scale and therefore we’re committed to creating a more inclusive and diverse workplace to reflect the customers we serve. We welcome interest from individuals of all backgrounds and levels of experience who share our mission. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or other applicable legally protected characteristics.","As a Senior Data Engineer at Lightspark, you will have the opportunity to lead core data engineering initiatives that touch many aspects of the company – from Product and Engineering to Strategy and Legal, to Accounting and Finance; Data Engineers at Lightspark are highly entrepreneurial; You must be passionate about building and excited to tackle an array of interesting problems; The role is a fusion of a Strategic Partner and a full-stack Data Scientist; You will have a strong sense of ownership and the ability to lead and shape Lightspark’s business and data engineering initiatives; You must be adept at navigating various functions while also operating independently; You will have the capacity to wield both analytical and strategic capabilities across domains; M.S / BA / BS degree in a technical field (Mathematics, Statistics, Economics, Computer Science) with 5+ years of experience or a Ph.D in a technical field with 2+ years of experience; Experience working in a fast-paced, startup environment ideally in tech, payments, crypto, or blockchain companies; Experience working on a wide variety of problems cross-functionally; Experience understanding and applying statistical concepts (including causal inference, machine learning, and experimentation); Experience working as a full-stack Data Scientist, including writing data pipelines, modeling theory, writing code, and deploying applications in production systems; Experience programming and modeling in Python and data analysis in Pandas / Numpy / SQL; Experience with ETL, data pipelines, and workflow management tools such as Stitch, Fivetran, or Airflow; Experience architecting and using massively parallel processing technology, such as Google Bigquery, Snowflake, Redshift, etc; Self-motivated and able to work independently; Willing to live or relocate to our HQ in Los Angeles, California; 14 more items(s)","This will span everything from analyzing current operations to charting long-term initiatives that will drive Lightspark’s path to success; Designing and measuring key product, operational, and risk metrics to surface critical insights to the Strategy, Data Science, and Engineering teams; Conceptualize, build, and own Lightspark’s data analytics architecture and infrastructure, including overall design, data models, and data pipelines; Working with our Machine Learning, Data Science, and Engineering teams to build scalable and efficient data models to advance Lightspark’s product and strategy initiatives; Design, lead, and implement security models for Lightspark’s data systems, including privacy requirements, governance processes, and data quality controls; Operate as a strategic partner from Data Science to functional teams, using strategic thinking and technical excellence to inform key business and product decisions; Guide and inform Lightspark’s business operations, supporting Lightspark’s Legal, Finance, and Accounting teams to ensure Lightspark’s business operations are documented accurately; Analyzing large and complex datasets to guide Lightspark’s strategic priorities; 5 more items(s)","Python, Pandas, Numpy, SQL, Stitch, Fivetran, Airflow, Google Bigquery, Snowflake, Redshift"
Staff Data Engineer - Research/Machine Learning,TEKsystems,"TEKsystems • Anaheim, CA •  via TEKsystems Careers",5 days ago,70–85 an hour,"ce- 3+ years of Informatica- Azure- 2+ years of SnowflakeReally Nice to have:- MDM – He said this was a nice to have but he also made it seem like a must have- Microsoft Dynamics 360- CoPilot- Customer InsightsThey are adding MDM into Informatica and moving away from Informatica on-premCurrently Informatica is the only thing in their environment that is on-prem - everything else is in the cloudMDM – They are trying to figure out if they are going to move Informatica to the cloud or build their ownA lot of their data is in SQLPOSITION SUMMARY:Responsible for managing and supporting the corporate Master Data Management (MDM), Operational Data Stores (ODS) and Enterprise Data Warehouse (EDW) platforms. This will include collaborating with the Data Analytics Steering Committee (DASC), Technology team and product stakeholders to design, implement and maintain these enterprise platforms.Develop innovative and best practice techniques for modeling, developing, enhancing, and deploying database, reporting, and integration solutions. Lead new data integration and reporting initiatives that support organizational priorities while complying with the organization's IT and architectural standards. Establish organization wide Common Data Platform (CDP) and data integration standards. Participate in systems strategy, developing systems requirements, designing and prototyping, testing, training, defining support procedures, and implementing practical business solutions under multiple deadlines.ESSENTIAL POSITION FUNCTIONS:Design, construct, install, test and maintain highly scalable MDM, ODS, CRM and EDW platforms with state-of-the-art security, monitoring and logging practices.RDBMS (Microsoft SQL) and NoSQL design, implementation and maintenance for purposes of integration to other platforms.Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.Design and implement data tools for analytics and data analyst team members to help them in building, optimizing and tuning our product.Integrate new data management technologies and software engineering tools into existing structures.Help in building high-performance algorithms, prototypes, predictive models and proof of concepts.Assist business teams to better understand, consume and appreciate the implications and benefits of MDM, ODS, CRW and EDW solutions.EXPERIENCE, SKILLS AND ABILITIES:At least five (5) years of experience in architecting, designing and configuring multi-domain MDM and Enterprise Data Warehouse (EDW)At least five (5) years of relevant data management consulting or industry experience (master data, metadata, data architecture, data governance, data quality, data modeling)At least five (5) years of experience with relational SQL systems including PL/SQL, T-SQL, Job configuration/management and performance tuning.At least five (5) years of experience with designing, implementing and supporting ETL, ELT, Change Data Capture (CDC), streaming and similar data ingestion processes.At least three (3) years of experience with non RDBMS platforms.At least three (3) years of experience with modern programming languages.At least three (3) years of experience leading work streams with significant experience leading components of data engagements.Deep understanding of MDM and upstream and downstream integrationKnowledgeable in ETL design development.Experience in Pub/Sub model for data integration both on premise and cloud.Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint).Proficient in Microsoft applications (Word, Excel, Outlook)Bachelor’s Degree in Computer Science or related field or 5+ years equivalent professional experiencePREFERRED SKILLS:Experience in the latest components of Informatica multi-domain MDM editions including Entity 360 and ActiveVOSExperience with other data management toolsets: Informatica Powercenter, Informatica Data Quality, IICS, etc.Experience in designing and architecting integrations between Informatica MDM and other systems using standard connectors, publish-subscribe, and other mechanismsExperience with iPaaS toolsets: MuleSoft, Informatica Cloud, TIBCOExperience with Microsoft SQL versions 2008 and higher including high availability.Experience with non RDBMS platforms: HBase, Cassandra, Hadoop, MongoDB, Snowflake, etc.Experience with one or more of Python, PySpark, Java, Databricks, Node.JS, React JS, Restful APIs.Experience with one or more Cloud platforms (Azure, AWS).Experience with one or more data and reporting tools such as Informatica, SSIS, SSRS, Tableau, Power BI, Micro Strategy.Experience in a Project Management and/or Busines Analyst or similar role(s)Experience with CRM platforms: Salesforce, Microsoft DynamicsoEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to specific elections, plan, or program terms.  If eligible, the benefits available for this temporary role may include the following:§ Medical, dental & vision§ Critical Illness, Accident, and Hospital§ 401(k) Retirement Plan – Pre-tax and Roth post-tax contributions available§ Life Insurance (Voluntary Life & AD&D for the employee and dependents)§ Short and long-term disability§ Health Spending Account (HSA)§ Transportation benefits§ Employee Assistance Program§ Time Off/Leave (PTO, Vacation or Sick Leave)About TEKsystems:We're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.","This is a hybrid position and candidates must be willing to be onsite once a month; Minimum 5+ years of SQL Development experience; 3+ years of Informatica; Azure; 2+ years of Snowflake; MDM – He said this was a nice to have but he also made it seem like a must have; Microsoft Dynamics 360; CoPilot; At least five (5) years of experience in architecting, designing and configuring multi-domain MDM and Enterprise Data Warehouse (EDW); At least five (5) years of relevant data management consulting or industry experience (master data, metadata, data architecture, data governance, data quality, data modeling); At least five (5) years of experience with relational SQL systems including PL/SQL, T-SQL, Job configuration/management and performance tuning; At least five (5) years of experience with designing, implementing and supporting ETL, ELT, Change Data Capture (CDC), streaming and similar data ingestion processes; At least three (3) years of experience with non RDBMS platforms; At least three (3) years of experience with modern programming languages; At least three (3) years of experience leading work streams with significant experience leading components of data engagements; Deep understanding of MDM and upstream and downstream integration; Knowledgeable in ETL design development; Experience in Pub/Sub model for data integration both on premise and cloud; Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint); Proficient in Microsoft applications (Word, Excel, Outlook); Bachelor’s Degree in Computer Science or related field or 5+ years equivalent professional experience; Experience in the latest components of Informatica multi-domain MDM editions including Entity 360 and ActiveVOS; Experience with other data management toolsets: Informatica Powercenter, Informatica Data Quality, IICS, etc; Experience in designing and architecting integrations between Informatica MDM and other systems using standard connectors, publish-subscribe, and other mechanisms; Experience with iPaaS toolsets: MuleSoft, Informatica Cloud, TIBCO; Experience with Microsoft SQL versions 2008 and higher including high availability; Experience with non RDBMS platforms: HBase, Cassandra, Hadoop, MongoDB, Snowflake, etc; Experience with one or more of Python, PySpark, Java, Databricks, Node.JS, React JS, Restful APIs; Experience with one or more Cloud platforms (Azure, AWS); Experience with one or more data and reporting tools such as Informatica, SSIS, SSRS, Tableau, Power BI, Micro Strategy; Experience in a Project Management and/or Busines Analyst or similar role(s); Experience with CRM platforms: Salesforce, Microsoft Dynamics; 29 more items(s)","§ Medical, dental & vision; § Critical Illness, Accident, and Hospital; § 401(k) Retirement Plan – Pre-tax and Roth post-tax contributions available; § Life Insurance (Voluntary Life & AD&D for the employee and dependents); § Short and long-term disability; § Health Spending Account (HSA); § Transportation benefits; § Employee Assistance Program; § Time Off/Leave (PTO, Vacation or Sick Leave); 6 more items(s)","Informatica, Azure, Snowflake, MDM, Microsoft Dynamics 360, CoPilot, Customer Insights, SQL, RDBMS, Microsoft SQL, NoSQL, PL/SQL, T-SQL, ETL, ELT, CDC, HBase, Cassandra, Hadoop, MongoDB, Python, PySpark, Java, Databricks, Node.JS, React JS, Restful APIs, AWS, SSIS, SSRS, Tableau, Power BI, Micro Strategy, Informatica Powercenter, Informatica Data Quality, IICS, MuleSoft, Informatica Cloud, TIBCO, Salesforce"
"Data Engineer-CA, Remote",Character.ai,"Character.ai • Menlo Park, CA •  via ZipRecruiter",9 days ago,150K–350K a year,"g a diverse set of sources, including structured and unstructured content from text and multimedia formats. Your engineering expertise is crucial in crafting the infrastructure and tools necessary to efficiently collect and manage petabytes of data.• Second, you will experiment with various methods of extracting a balanced and comprehensive training dataset from the raw data. You will leverage your expertise in data to build datasets reflecting a hypothesis, train models, and evaluate experimental results. Through this experimentation, you will create the training datasets for our largest models.These are critical steps in the construction of AI. With petabytes of data and numerous design decisions, each step requires careful attention. Expertise in AI is not necessary, but enthusiasm for the space and a track record of adapting to new domains is important.Who we're looking forRequired Experience:• 5+ years of production software engineering experience• Experience building large-scale data processing pipelines, with tools like PySpark, Beam, or Flink• Familiarity with Machine Learning and NLP and willingness to learn more on the job• Track record of adapting to new domains and a desire to use data to improve productsAdditional Desired Experience:• ML experience as an ML engineer, Data Scientist, or another similar role• Experience with cloud platforms like AWS or Azure, or tools such as Kubernetes and Terraform• Passionate about Conversational AI or large language modelsYou will be a good fit if you are proactive and have a ""get things done"" mindset. Given our current pace of growth and load on our systems, most people have had a significant impact during their first week at the company.About Character.AIFounded in 2021, Character is a leading AI company offering personalized experiences through customizable AI 'Characters.' As one of the most widely used AI platforms worldwide, Character enables users to interact with AI tailored to their unique needs and preferences.In just two years, we achieved unicorn status and were named Google Play's AI App of the Year - a testament to our groundbreaking technology and vision.Ready to shape the future of Consumer AI?At Character, we value diversity and welcome applicants from all backgrounds. As an equal opportunity employer, we firmly uphold a non-discrimination policy based on race, religion, national origin, gender, sexual orientation, age, veteran status, or disability. Your unique perspectives are vital to our success.","Expertise in AI is not necessary, but enthusiasm for the space and a track record of adapting to new domains is important; 5+ years of production software engineering experience; Experience building large-scale data processing pipelines, with tools like PySpark, Beam, or Flink; Familiarity with Machine Learning and NLP and willingness to learn more on the job; Track record of adapting to new domains and a desire to use data to improve products; You will be a good fit if you are proactive and have a ""get things done"" mindset; 3 more items(s)","You would be a great fit for this role if you are an experienced engineer who will be instrumental in building the world's best LLMs by collecting and refining the essential training data that powers them; First, identify and collect data at the scale required to feed our largest models; This involves managing a diverse set of sources, including structured and unstructured content from text and multimedia formats; Your engineering expertise is crucial in crafting the infrastructure and tools necessary to efficiently collect and manage petabytes of data; Second, you will experiment with various methods of extracting a balanced and comprehensive training dataset from the raw data; You will leverage your expertise in data to build datasets reflecting a hypothesis, train models, and evaluate experimental results; Through this experimentation, you will create the training datasets for our largest models; These are critical steps in the construction of AI; With petabytes of data and numerous design decisions, each step requires careful attention; 6 more items(s)","PySpark, Beam, Flink, AWS, Azure, Kubernetes, Terraform"
"Senior Azure Data engineers at Starcom consulting limited Los Angeles, CA",Georgia IT Inc.,Georgia IT Inc. • California •  via Monster,Contractor,No Degree Mentioned,authoring (SQL).• Experience building batch data pipelines in Spark Scala and General software engineering skills (Java or Python).RemoteAbout the Company:Georgia IT Inc.,"4+ years of relevant industry experience; Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions; Working knowledge of relational databases and query authoring (SQL); Experience building batch data pipelines in Spark Scala and General software engineering skills (Java or Python); 1 more items(s)",,"SQL, Spark Scala, Java, Python"
Cloud Data Engineer IV,Abnormal Security,"Abnormal Security • San Francisco, CA •  via Hiregeeks",20 days ago,147K–173K a year,"Join Abnormal Security as a Software Engineer II on the Data Platform team, working remotely to build scalable data solutions.","Join Abnormal Security as a Software Engineer II on the Data Platform team, working remotely to build scalable data solutions",,No tools or software mentioned.
Sr. Specialist Solutions Architect - Data Engineering & Cloud Infrastructure,Habemco,"Habemco • Upper Lake, CA •  via Paycomonline.net",Full-time,Health insurance,"rvices to various tribal business and government entities. Habemco’s primary support services power the Tribe’s flagship online lending brand, Uprova, and any future brands, through product development, technology, and other support needed for growth. The Habemco team plays a critical role in ensuring a successful future for our customers, our employees, and the Tribe.Headquartered in a beautiful, yet remote part of California, the Tribe recognizes that to compete in the highly competitive FinTech industry, the Tribe must access expertise throughout the nation. In addition to employees that work remotely, the Tribe has employees clustered at headquarters in Upper Lake, California, and a call center in Lenexa, Kansas.Employees receive competitive pay and benefits, quarterly performance bonuses and 401(k) with a 4% match. Our team is ambitious, forward-thinking, passionate and moves fast! Are you ready to grow with us?Purpose of the Position:The Cloud Data Engineer IV works with stakeholders and architects in both Business and IT departments to review functional and non-functional requirements and design and develop data solutions that align with and support the business and technology organization strategy and standards. Additionally, this position will be focused on supporting existing data integration and analytics solutions and designing and developing new solutions that support data-driven decision-making. Incumbent in this role performs work that is highly independent, yet collaborative in nature; also serves as a specialist on complex technical and business matters.Essential Duties• Management and alignment of solutions across all technology platforms, working closely with architects, data, and infrastructure teams to optimize application code, ensuring data integrity, security, stability, resiliency, sustainability, growth, and performance.• Conduct design and code reviews to ensure compliance with standards.• Participate in knowledge sharing and own technical issue resolution.• Produce high-quality code that is robust, efficient, testable, and easy to maintain.• Deliver operational automation and tooling to minimize repeated manual tasks.• Create and maintain systems to load and transform large data sets from various internal & external sources.• Understand business goals and drivers and translate those into an appropriate technical cloud solution.• Be the subject matter expert for technologies like MySQL, Redshift, and AWS Data Pipeline.• Create conceptual, logical, and physical design for cloud-based solutions for Data, Infrastructure, and Application platforms.• Team with Software Engineering teams and Product Owners to include new data sources and types in ETL processes and assist in deploying schema changes.• Work with our Data Scientist and Business Intelligence team to create data-driven insights and reports for the organization.• Perform daily monitoring, administration, repair, and maintenance related to databases and related technologies in all environments.• Participate in an on-call rotation supporting production n systems during off-hours.• Regular, reliable attendance during normal business hours.• In-person attendance and travel as requested.• Other duties as assigned.Education and ExperienceRequired:• Bachelor of Science degree from an accredited university majoring in Computer Science or other Engineering discipline.• 7+ years of related work experience, including experience in the following:• Big Data, data streaming, data lake & cloud-based platforms• ETL, Data Modeling, and Data Architecture.• Big Data technologies such as Hadoop/Hive/Spark.• Scripting experience.• Designing data warehouse solutions.• Expertise in ETL optimization, designing, coding, and tuning extensive data processes using Apache Spark or similar technologies.• AWS platform services, such as: compute, containers, integration, internet of things, storage, web, and DevOps.• All offers are contingent upon signing a confidentiality agreement and satisfactory completion of drug screening and background checks. Employer observes federal standards for controlled substances.Preferred:• Experience operating very large data warehouses or data lakes.• Experience building data pipelines and applications to stream and process datasets at low latencies.• Knowledge of Engineering and Operational Excellence using standard methodologies.• Experience in AWS in developing solutions using microservices.• Proven track record of successfully architecting and deploying AWS solutions from End to End.• Knowledge of defining solution architecture, design detailing, and technology delivery with a particular focus on AWS platform services, such as: compute, containers, integration, internet of things, storage, web, and DevOps.Skills and Abilities• Expert-level skills in writing and optimizing SQL.• Proven efficiency in handling data, tracking data lineage, ensuring data quality, and improving discoverability of data.• Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines and knows how to optimize the distribution, partitioning, and MPP of high-level data structures.• Advanced problem-solving skills and the ability to optimize data for the best possible outcome.• Ability to prioritize and manage multiple milestones and projects efficiently.• A willingness to dig deep, learn from others, share your skills, and be part of a talented and dedicated team.• Superior attention to detail.• Highly adaptable, a driver of change, and capable of quickly rallying teams.• Effectively prioritizes and executes tasks in a highly productive yet autonomous environment.• Strong decision-making and problem-solving skills (i.e., design, debugging, and testing) and experience with software development projects.• Ability to present technical ideas in concise, user-friendly, or layman's language.• Strong interpersonal skills used in developing effective working relationships and listening skills.• Result-driven and solutions-oriented with the ability to develop and implement the resolution in time-sensitive situations.• Motivate and mentor team members to grow their skills and careers by creating a nurturing environment that encourages innovation and continual learning.• Ability to work in a fast-paced, time-sensitive, and confidential environment.• Excellent communication skills; able to communicate effectively both orally and in writing with professionalism, excellent grammar, respect and courteousness.• Possess a balance of assertiveness and diplomacy along with adaptability to communicate on all levels.Physical Requirements• Prolonged periods in a stationary seated position, such as working on a computer.• Frequently move, transport, and manipulate computer equipment up to 15 pounds.• Verbal communication sufficient to exchange accurate ideas and information.• In-person attendance and travel as requested.Indian PreferenceNative American Indian preference shall apply to the position in accordance with Title VII of the Civil Rights Act of 1964 (42 U.S.C. 2000e-2(i)), regulations of the Office of Federal Contract Compliance Programs (41 C.F.R. 60-1.5(a)(7)), and/or other relevant laws. Applicants claiming Indian preference must submit verification of Indian eligibility with their application.Qualifications","Bachelor of Science degree from an accredited university majoring in Computer Science or other Engineering discipline; 7+ years of related work experience, including experience in the following:; Big Data, data streaming, data lake & cloud-based platforms; ETL, Data Modeling, and Data Architecture; Big Data technologies such as Hadoop/Hive/Spark; Scripting experience; Designing data warehouse solutions; Expertise in ETL optimization, designing, coding, and tuning extensive data processes using Apache Spark or similar technologies; AWS platform services, such as: compute, containers, integration, internet of things, storage, web, and DevOps; All offers are contingent upon signing a confidentiality agreement and satisfactory completion of drug screening and background checks; Employer observes federal standards for controlled substances; Expert-level skills in writing and optimizing SQL; Proven efficiency in handling data, tracking data lineage, ensuring data quality, and improving discoverability of data; Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines and knows how to optimize the distribution, partitioning, and MPP of high-level data structures; Advanced problem-solving skills and the ability to optimize data for the best possible outcome; Ability to prioritize and manage multiple milestones and projects efficiently; A willingness to dig deep, learn from others, share your skills, and be part of a talented and dedicated team; Superior attention to detail; Highly adaptable, a driver of change, and capable of quickly rallying teams; Effectively prioritizes and executes tasks in a highly productive yet autonomous environment; Strong decision-making and problem-solving skills (i.e., design, debugging, and testing) and experience with software development projects; Ability to present technical ideas in concise, user-friendly, or layman's language; Strong interpersonal skills used in developing effective working relationships and listening skills; Result-driven and solutions-oriented with the ability to develop and implement the resolution in time-sensitive situations; Motivate and mentor team members to grow their skills and careers by creating a nurturing environment that encourages innovation and continual learning; Ability to work in a fast-paced, time-sensitive, and confidential environment; Excellent communication skills; able to communicate effectively both orally and in writing with professionalism, excellent grammar, respect and courteousness; Possess a balance of assertiveness and diplomacy along with adaptability to communicate on all levels; Prolonged periods in a stationary seated position, such as working on a computer; Frequently move, transport, and manipulate computer equipment up to 15 pounds; Verbal communication sufficient to exchange accurate ideas and information; In-person attendance and travel as requested; Applicants claiming Indian preference must submit verification of Indian eligibility with their application; 30 more items(s)","Salary Range: Undisclosed; Employees receive competitive pay and benefits, quarterly performance bonuses and 401(k) with a 4% match","MySQL, Redshift, AWS Data Pipeline, Hadoop, Hive, Spark, Apache Spark"
Sr. Test Data Engineer,Databricks,"Databricks • Berkeley, CA •  via LinkedIn",8 days ago,Full-time,"ience with Apache Spark™ and expertise in other data technologies. SSAs help customers through design and successful implementation of essential workloads while aligning their technical roadmap for expanding the usage of the Databricks Data Intelligence Platform. As a deep go-to-expert reporting to the Specialist Field Engineering Manager, you will continue to strengthen your technical skills through mentorship, learning, and internal training programs and establish yourself in an area of specialty - whether that be streaming, performance tuning, industry expertise, or more.The Impact You Will Have• Provide technical leadership to guide strategic customers to successful implementations on big data projects, ranging from architectural design to data engineering to model deployment• Architect production level data pipelines, including end-to-end pipeline load performance testing and optimization, as well as production level deployments and meeting necessary security and networking requirements• Become a technical expert in an area such as data lake technology, big data streaming, or big data ingestion and workflows, as well as cloud platforms, automation, security, networking, or identity management• Assist Solution Architects with more advanced aspects of the technical sale including custom proof of concept content, estimating workload sizing, and custom architectures• Provide tutorials and training to improve community adoption (including hackathons and conference presentations)• Contribute to the Databricks CommunityWhat We Look For• 7+ years experience in a technical role with expertise in the following:• Software Engineering/Data Engineering: data ingestion, streaming technologies - such as Spark Streaming and Kafka, performance tuning, troubleshooting, and debugging Spark or other big data solutions• Data Applications Engineering: Build use cases that use data - such as risk modeling, fraud detection, customer life-time value• Cloud Infrastructure & Security: cloud deployments, security, networking, Identify management, platform administration and/or infrastructure automation• Extensive experience building big data pipelines• Experience maintaining and extending production data systems to evolve with complex needs• Deep Specialty Expertise in the following areas:• Experience scaling big data workloads (such as ETL) that are performant and cost-effective• Experience migrating Hadoop workloads to the public cloud - AWS, Azure, or GCP• Experience with large scale data ingestion pipelines and data migrations - including CDC and streaming ingestion pipelines• Expert with cloud data lake technologies - such as Delta and Delta Live• Bachelor's degree in Computer Science, Information Systems, Engineering, or equivalent experience through work experience.• Production programming experience in SQL and Python, Scala, or Java.• 4 years professional experience with Big Data technologies (Ex: Spark, Hadoop, Kafka) and architectures• 4 years customer-facing experience in a pre-sales or post-sales role• Can meet expectations for technical training and role-specific outcomes within 6 months of hire• Can travel up to 30% when neededBenefits• Comprehensive health coverage including medical, dental, and vision• 401(k) Plan• Equity awards• Flexible time off• Paid parental leave• Family Planning• Gym reimbursement• Annual personal development fund• Work headphones reimbursement• Employee Assistance Program (EAP)• Business travel accident insurance• Mental wellness resourcesPay Range TransparencyDatabricks is committed to fair and equitable compensation practices. The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location. Based on the factors above, Databricks utilizes the full width of the range. The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above. For more information regarding which range your location is in visit our page here.Zone 1 Pay Range$169,000—$299,000 USDZone 2 Pay Range$169,000—$299,000 USDZone 3 Pay Range$169,000—$299,000 USDAbout DatabricksDatabricks is the data and AI company. More than 10,000 organizations worldwide — including Comcast, Condé Nast, Grammarly, and over 50% of the Fortune 500 — rely on the Databricks Data Intelligence Platform to unify and democratize data, analytics and AI. Databricks is headquartered in San Francisco, with offices around the globe and was founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow. To learn more, follow Databricks on Twitter, LinkedIn and Facebook.Our Commitment to Diversity and InclusionAt Databricks, we are committed to fostering a diverse and inclusive culture where everyone can excel. We take great care to ensure that our hiring practices are inclusive and meet equal employment opportunity standards. Individuals looking for employment at Databricks are considered without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, veteran status, and other protected characteristics.ComplianceIf access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone.","7+ years experience in a technical role with expertise in the following:; Software Engineering/Data Engineering: data ingestion, streaming technologies - such as Spark Streaming and Kafka, performance tuning, troubleshooting, and debugging Spark or other big data solutions; Data Applications Engineering: Build use cases that use data - such as risk modeling, fraud detection, customer life-time value; Cloud Infrastructure & Security: cloud deployments, security, networking, Identify management, platform administration and/or infrastructure automation; Extensive experience building big data pipelines; Experience maintaining and extending production data systems to evolve with complex needs; Deep Specialty Expertise in the following areas:; Experience scaling big data workloads (such as ETL) that are performant and cost-effective; Experience migrating Hadoop workloads to the public cloud - AWS, Azure, or GCP; Experience with large scale data ingestion pipelines and data migrations - including CDC and streaming ingestion pipelines; Expert with cloud data lake technologies - such as Delta and Delta Live; Bachelor's degree in Computer Science, Information Systems, Engineering, or equivalent experience through work experience; Production programming experience in SQL and Python, Scala, or Java; 4 years professional experience with Big Data technologies (Ex: Spark, Hadoop, Kafka) and architectures; 4 years customer-facing experience in a pre-sales or post-sales role; Can meet expectations for technical training and role-specific outcomes within 6 months of hire; Can travel up to 30% when needed; If access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone; 15 more items(s)","Comprehensive health coverage including medical, dental, and vision; 401(k) Plan; Equity awards; Flexible time off; Paid parental leave; Family Planning; Gym reimbursement; Annual personal development fund; Work headphones reimbursement; Employee Assistance Program (EAP); Business travel accident insurance; Mental wellness resources; Pay Range Transparency; Databricks is committed to fair and equitable compensation practices; The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles; Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location; The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above; $169,000—$299,000 USD; Zone 2 Pay Range; $169,000—$299,000 USD; $169,000—$299,000 USD; 18 more items(s)","Apache Spark, Spark Streaming, Kafka, AWS, Azure, GCP, Delta, Delta Live, SQL, Python, Scala, Java, Hadoop"
"Data Engineer, Consultant",ClickJobs.io,"ClickJobs.io • Whittier, CA •  via Salary.com",8 days ago,180K–218K a year,"pproach to entering the AAM market with a product that meets the high level of safety and reliability in today’s air transportation system.The future of mobility starts with people. We believe in creative thinking and collaboration to help build a better mobility experience for everyone, improving people’s ability to move – whether for work or play. Join our dynamic team as we strive to be a part of something greater where potential powers tomorrow!Supernal provides an inviting open-space workplace designed to foster collaboration, which aligns with one of our core values. This position is required to work on-site 5 days a week.What we do:The Sr. Test Data Engineer will be responsible for developing innovative tools and solutions to manage data generated to test, evaluate, and certify (TEC) the next-generation aircraft. The role is to develop software tools needed to integrate various types of aircraft test data for accessibility and usability from a single hub.What you can do:• Create custom data management solutions and infrastructures that will involve the greater group, such as IT, Testing, Engineering, Manufacturing, Certification, and Leadership• Build data pipelines to move information from different sources (i.e. design management tools to control stations) for usability• Link complex requirements, and configurations and extract information from sources (i.e. PLM tools)• Develop data analytic applications to help visualize aircraft test data• Generate tools to convert certain data formats (i.e. network packets to CSV)• Up to 25% of domestic and international travel• Other duties as assignedWhat you can contribute:• Bachelor's degree in Computer Science, Software Engineering, Data Science, Information Technology, Electrical Engineering, Aerospace, or a related field required; Master's degree preferred• Minimum (5) five years of experience in data storage solutions at an administrative level in cloud environments (AWS, Azure)• Minimum (5) five years of experience in developing and maintaining high-performance APIs (an equivalent combination of education and experience may be considered)• Proficiency in developing a relational database with structured and unstructured data• Experience in aircraft and/or passenger vehicle R&D industry• Experience in being part of flight testing and/or prototype vehicle development team• Knowledge of vehicle communication/control protocols such as CAN and UDP• Knowledge of methods used to view live and recorded test data• Proficiency in test procedures and development phases to build prototype vehicles for production• Proficiency in at least one scripting language such as Python• Proficiency in computing platforms to develop data visualization and analysis tools such as MATLAB• Knowledge of database architecture• Knowledge of Azure Databricks• Systems admin experience for requirements management tools• Excellent verbal and written communication skills• Sense of urgency, proactive delivery of communication and follow-up• Excellent organizational skills and attention to detail• Must have the ability to independently prioritize and accomplish work within time constraints• Proficiency with Excel VLOOKUP, pivot tables, MS Office SuiteYou may also be able to contribute:• Microsoft Certified Azure Data Engineer preferred• Experience with Agile Methodologies preferred• Experience in AI knowledge, trends, and security preferred• Experience in building network infrastructure preferredBase pay offered may vary depending on skills, experience, job-related knowledge and location. This position is also eligible for a bonus as part of total compensation.The pay range for this position is:$180,100—$218,300 USDClick HERE or visit: https://jobs.supernal.aero/benefits to view our benefits!Any offer of employment is conditioned upon the successful completion of a background check. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, citizenship, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other category or class protected under applicable federal, state or local law. Individuals with disabilities may request a reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation at: ta-support@supernal.aeroThis position may include access to certain technology and/or software source code subject to U.S. export controls laws and regulations. If an export authorization from an applicable US regulatory agency is required in connection with your employment, your employment is contingent upon Supernal’s receipt of such regulatory authorization(s) and your continued compliance with all conditions and limitations pursuant to such authorization(s).","Minimum (5) five years of experience in data storage solutions at an administrative level in cloud environments (AWS, Azure); Minimum (5) five years of experience in developing and maintaining high-performance APIs (an equivalent combination of education and experience may be considered); Proficiency in developing a relational database with structured and unstructured data; Experience in aircraft and/or passenger vehicle R&D industry; Experience in being part of flight testing and/or prototype vehicle development team; Knowledge of vehicle communication/control protocols such as CAN and UDP; Knowledge of methods used to view live and recorded test data; Proficiency in test procedures and development phases to build prototype vehicles for production; Proficiency in at least one scripting language such as Python; Proficiency in computing platforms to develop data visualization and analysis tools such as MATLAB; Knowledge of database architecture; Knowledge of Azure Databricks; Systems admin experience for requirements management tools; Excellent verbal and written communication skills; Sense of urgency, proactive delivery of communication and follow-up; Excellent organizational skills and attention to detail; Must have the ability to independently prioritize and accomplish work within time constraints; Proficiency with Excel VLOOKUP, pivot tables, MS Office Suite; 15 more items(s)","Base pay offered may vary depending on skills, experience, job-related knowledge and location; This position is also eligible for a bonus as part of total compensation; $180,100—$218,300 USD","AWS, Azure, Python, MATLAB, Excel, MS Office Suite, Azure Databricks"
Data Visualization Engineer,Blue Shield Of California,"Blue Shield Of California • El Dorado Hills, CA •  via Ladders",119K–178K a year,Full-time,"ntegration, data warehouse and data mart (ELT/ETL) solutions using on-prem as well as Cloud technologies.Your WorkIn this role, you will:• Build data pipelines: Create, maintain, and optimize workloads from development to production for specific use cases• Use innovative and modern tools, techniques and architectures listed below to drive automation of most-common, repeatable data preparation and integration tasks with goal of minimizing reducing defects and improving productivity• Develop and enforce data integration and data quality standards across all development initiatives according to the organization's Enterprise information services policies as well as best practices• Assist in renovating our data management infrastructure with quality of data preparation, integration and AI-enabled metadata management tools and techniques• Track data consumption patterns most used and valued to our customers by performing intelligent sampling and monitoring• Triage data issues, analyzing end to end data pipelines and in working with business users in troubleshooting/resolving data pipeline issues• Execute and oversee the analysis and remediation of root causes, including deficiencies in technology, process, or resource capabilities• Work in an Agile/DevSecOps pod model alongside solution leads, data modelers, analysts, business partners and other developers in delivery of data• Mentor junior data engineers/developersQualificationsYour Knowledge and Experience• Requires a bachelor's degree or equivalent experience• Requires at least 12 years of prior relevant experience• Hands on experience with programming languages including SQL, PL/SQL on cloud data platforms like Snowflake, Oracle, SQL Server and Netezza• Strong technical understanding of data modeling (data vault 2.0), data mining, master data management, data integration, data architecture, data virtualization, data warehousing and data quality techniques with hands on experience using data management technologies like Informatica PowerCenter/IICS, Collibra, Master Data Management, DBT Cloud, Denodo and Golden Gate• Working knowledge of Git repositories (bitbucket, GitHub), CI/CD (Jenkins, Azure DevOps)and software development tools, including incident tracking, version control, release management, subversion change management(Atlassian toolset - Jira/Confluence), testing tools and systems and scheduling software (Tidal, Control-m)• Experience working with popular data discovery, analytics and BI software tools like Tableau, Qlik, PowerBI and others for semantic-layer-based data discovery• Experienced in popular open-source and commercial data science platforms such as Python, R, KNIME, Alteryx, others is a strong plus but not required/compulsory• Basic experience in working with data governance and data security and specifically information stewards and privacy and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification• Adept in agile methodologies and capable of applying DevOps and increasingly Data Operations principles to data pipelines to improve the communication, integration, reuse and automation of data flows between data managers and consumers across an organizationPay Range:The pay range for this role is: $ 118800.00 to $ 178200.00 for California.Note:Please note that this range represents the pay range for this and many other positions at Blue Shield that fall into this pay grade. Blue Shield salaries are based on a variety of factors, including the candidate's experience, location (California, Bay area, or outside California), and current employee salaries for similar roles.#LI-CM1About the TeamBlue Shield of California's mission is to ensure all Californians have access to high-quality health care at a sustainably affordable price. We are transforming health care in a way that genuinely serves our nonprofit mission by lowering costs, improving quality, and enhancing the member and physician experience.To fulfill our mission, we must ensure a diverse, equitable, and inclusive environment where all employees can be their authentic selves and fully contribute to meet the needs of the multifaceted communities we serve. Our continued commitment to diversity, equity, and inclusion upholds our values and advances our goal of creating a healthcare system that is worthy of our family and friends while addressing health disparities, promoting social justice, and integrating health equity through our products, business practices, and presence as a corporate citizen.Blue Shield has received awards and recognition for being a certified Fortune 100 Best Companies to Work, Military Friendly Employer, People Companies that Care, a Leading Disability Employer, and one of California's top companies in volunteering and giving. Here at Blue Shield, we strive to make a positive change across our industry and communities - join us!Our Values:• Honest. We hold ourselves to the highest ethical and integrity standards. We build trust by doing what we say we're going to do and by acknowledging and correcting where we fall short.• Human. We strive to be our authentic selves, listening and communicating effectively, and showing empathy towards others by walking in their shoes.• Courageous. We stand up for what we believe in and are committed to the hard work necessary to achieve our ambitious goals.Our Workplace Model:Blue Shield of California is dedicated to making work-life balance a reality. Whether you prefer to work in an office or from home, we understand flexibility is more important than ever. That's why Blue Shield is a hybrid company, offering you the opportunity to decide where you can do your best and most meaningful work.Two ways of working: Hybrid (our default) and office• Hybrid - In a business unit approved office a few times per year to 3 days per week, on average• Office - In a business unit approved office 4+ days a week, on average. If the role you're applying for is deemed an ""Essential Role,"" the company has determined that the role can only be performed in a Blue Shield office or in the field and would require your to meet the office worker classification.Physical Requirements:Office Environment - roles involving part to full time schedule in Office Environment. Due to the current public health emergency in California, Blue Shield employees are almost all working remotely. Based in our physical offices and work from home office/deskwork - Activity level: Sedentary, frequency most of work day.Please click here for further physical requirement detail.Equal Employment Opportunity:External hires must pass a background check/drug screen. Qualified applicants with arrest records and/or conviction records will be considered for employment in a manner consistent with Federal, State and local laws, including but not limited to the San Francisco Fair Chance Ordinance. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, national origin, sexual orientation, gender identity, protected veteran status or disability status and any other classification protected by Federal, State and local laws.","Requires a bachelor's degree or equivalent experience; Requires at least 12 years of prior relevant experience; Hands on experience with programming languages including SQL, PL/SQL on cloud data platforms like Snowflake, Oracle, SQL Server and Netezza; Strong technical understanding of data modeling (data vault 2.0), data mining, master data management, data integration, data architecture, data virtualization, data warehousing and data quality techniques with hands on experience using data management technologies like Informatica PowerCenter/IICS, Collibra, Master Data Management, DBT Cloud, Denodo and Golden Gate; Working knowledge of Git repositories (bitbucket, GitHub), CI/CD (Jenkins, Azure DevOps)and software development tools, including incident tracking, version control, release management, subversion change management(Atlassian toolset - Jira/Confluence), testing tools and systems and scheduling software (Tidal, Control-m); Experience working with popular data discovery, analytics and BI software tools like Tableau, Qlik, PowerBI and others for semantic-layer-based data discovery; Experienced in popular open-source and commercial data science platforms such as Python, R, KNIME, Alteryx, others is a strong plus but not required/compulsory; Basic experience in working with data governance and data security and specifically information stewards and privacy and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification; Adept in agile methodologies and capable of applying DevOps and increasingly Data Operations principles to data pipelines to improve the communication, integration, reuse and automation of data flows between data managers and consumers across an organization; Human; Courageous; If the role you're applying for is deemed an ""Essential Role,"" the company has determined that the role can only be performed in a Blue Shield office or in the field and would require your to meet the office worker classification; Office Environment - roles involving part to full time schedule in Office Environment; External hires must pass a background check/drug screen; Qualified applicants with arrest records and/or conviction records will be considered for employment in a manner consistent with Federal, State and local laws, including but not limited to the San Francisco Fair Chance Ordinance; 12 more items(s)","The pay range for this role is: $ 118800.00 to $ 178200.00 for California; Hybrid - In a business unit approved office a few times per year to 3 days per week, on average; Office - In a business unit approved office 4+ days a week, on average","SQL, PL/SQL, Snowflake, Oracle, SQL Server, Netezza, Informatica PowerCenter/IICS, Collibra, Master Data Management, DBT Cloud, Denodo, Golden Gate, Git, bitbucket, GitHub, Jenkins, Azure DevOps, Atlassian toolset, Jira, Confluence, Tidal, Control-m, Tableau, Qlik, PowerBI, Python, R, KNIME, Alteryx"
Director of Data Engineering,DB Driven,"DB Driven • Remote, OR •  via Indeed",2 days ago,57–74 an hour,"ments to verify efficient data retrieval and manipulation within reporting outputs.• Strong proficiency using SQL to profile data, execute data quality checks, and verify the accuracy of reporting outputs.• Experience designing and developing reporting solutions to join, de-duplicate, de-identify, and aggregate data for reports.• Experience writing test conditions and validating the accuracy of reporting measures in accordance with the established design documentation.• Experience working with healthcare data to improve patient outcomes.• Experience working with reporting tools to develop interactive, highly-polished data dashboards (e.g., Power BI, Tableau).• Desired technical skills: Power BI, SQL, Snowflake.Other RequirementsStatus: US Citizen or valid H1B VISALocation: Remote position, but must be physically located in the US. Prefer VA or other states where we currently have employees (VA, NV, NC, SC, GA, IA, WI, MD).Project Length: 1+ year(s)BenefitsHealthcareVacation401KJob Type: ContractPay: $57.00 - $74.00 per hourBenefits:• 401(k)• Health insurance• Paid time offEducation:• Bachelor's (Preferred)Experience:• AWS: 2 years (Preferred)• SQL: 2 years (Preferred)• Data warehouse: 2 years (Preferred)Work Location: Remote","Experience working with reporting tools to develop interactive, highly-polished data dashboards (e.g., Power BI, Tableau); Status: US Citizen or valid H1B VISA; Location: Remote position, but must be physically located in the US; Project Length: 1+ year(s); 1 more items(s)",Healthcare; Vacation; 401K; Pay: $57.00 - $74.00 per hour; 401(k); Health insurance; Paid time off; 4 more items(s),"SQL, Power BI, Tableau, Snowflake, AWS"
Data Engineer - REMOTE Direct Hire - W2 ONLY- HealthCare - $100k-120k/year,SEON Fraud Fighters,"SEON Fraud Fighters • Remote, OR •  via Recruiter",Full-time,No Degree Mentioned,"the internet a safer place to do business!SEON provides an API-first solution that helps our customers (many of the world's leading providers of digital experiences for consumer financial services, insurance, online entertainment, etc.) defend their customers against Fraud and Financial Crimes. With over 250 Fraud Fighters across four global offices (Austin, Budapest, London, and Jakarta), our goal remains unwavering: to make the internet safe for businesses and consumers to transact. Our achievements, including a record-breaking Series B funding round and recognition in TechCrunch, have led to recognition as the World's quickest-growing fraud prevention company. We take pride in our rapid growth and mission to democratize fraud-fighting while empowering the best online businesses. Join us in our journey to make the internet safer for everyone.What You'll Do:As the Director of Data Engineering at SEON, you will be responsible for the ownership and optimization of the technology teams responsible for Data Engineering, Data Science, Business Intelligence, & our AI/ML technology stack.Develop and execute data strategy and architecture to drive business objectives and empower engineering teams and data analysts.Establish data pipeline and modeling standards for the core platform, data science, and engineering teams to ensure consistency and usability.Enable business and engineering teams to access cross-functional data quickly and without discrepancies.Design data models for both business analytics and customer applications.Stay abreast of emerging technologies and industry trends in data engineering and architecture.Establish and maintain data governance policies and procedures.Make proposals and suggestions on how to solve data problems and present insights and recommendations to executive leadership.Design processes for operational excellence in system reliability and project management.Define and manage SLA's for data infrastructure systems, datasets and processes in production.Manage relationships and negotiations with external vendors.Implement processes and policies for infrastructure cost containment and cost forecasting.Proactively uncover areas of SEON's product and user experience that can be enhanced by generative AI and machine learning solutions and partner with Technology leadership to prioritize them.Operate as a thought partner and leader to Technology and GTM leadership, identifying risks and opportunities to shape our products and their potential to transform well-being through ML/AI.Promote the adoption of emerging and relevant approaches to solving ML and AI problems and empower the team to embrace pragmatic approaches to evaluating multiple solutions.Lead, develop, and mentor a team of ML engineers, data scientists and data engineers.What You Bring:8+ years of software engineering experience in broad-based information systems, full life-cycle application development, and/or building enterprise applications.7+ years of experience in data engineering and modeling.Experience with data governance and compliance regulations (e.g., GDPR, CCPA), and data security best practices preferred.Expert proficiency in SQL.Comprehensive technical expertise in a variety of data engineering, modeling, and business intelligence applications, such as PostgreSQL, Clickhouse, Elasticsearch, Databricks, SnowFlake, SageMaker, Tableau or comparable technologies.Strong understanding of how web services and SaaS applications work.Exceptional service orientation and ability to lead change using positive and collaborative methods; removes barriers, acts with a sense of urgency, and leads by example.Demonstrated history of leadership in managing high-impact machine learning and engineering projects, preferably in a SaaS startup/scaleup environment.Strong leadership, vision, and proven track record of creating and successfully executing a data strategy.A track record of managing a globally distributed, multi-discipline Data team, with experience overseeing Data Science, Data Engineering, and Machine Learning.Experience with distributed system design, system scaling, and performance optimization.Practical experience implementing machine learning models & GenAI for predictive business outcomes.Strong communication and collaboration skills across multiple levels, from detailed technical designs with engineering to broad product initiatives with key business stakeholders.It's a Plus If You Have:Demonstrated experience building high-performance systems with a major cloud provider (AWS preferred).Knowledge of modern DevOps technologies, such as Terraform, Kubernetes, ELK stack, Prometheus, etc.Technical expertise with the building blocks of a modern data stack, such as Prefect, AWS Glue, Iceberg, MLflow, dbt, Snowflake, and others.Experience with large-scale data migration projects, ensuring minimal disruption and data integrity.Expertise in designing and implementing real-time data processing and streaming solutions.What We Offer:Competitive salary and incentive compensation structure.Hybrid work model.Extensive Benefits package (Health, Dental, Vision, Disability, 401k w/ match, etc.).Best-in-class tech gear.Immediate impact on a smart, growing, global team.Access to continuous development tools & a strong coaching culture.Employee stock ownership plan (ESOP).Generous vacation & leave policies.Complimentary weekly language courses.Weekly in-office lunch delivery budget.What's Next:Does that sound good? Great, we can't wait to hear from you! Would you like to learn more about what it's like to work at SEON first?Here you go:https://careers.seon.io/Diversity Statement:SEON is an Equal Opportunity Employer. We celebrate differences and do not discriminate based on ethnicity, religion, color, sex, gender identity, sexual orientation, age, or mental disability. Please let your recruiter know if you need reasonable adjustments to our recruitment process.#J-18808-Ljbffr","What You Bring:8+ years of software engineering experience in broad-based information systems, full life-cycle application development, and/or building enterprise applications.7+ years of experience in data engineering and modeling; Expert proficiency in SQL; Comprehensive technical expertise in a variety of data engineering, modeling, and business intelligence applications, such as PostgreSQL, Clickhouse, Elasticsearch, Databricks, SnowFlake, SageMaker, Tableau or comparable technologies; Strong understanding of how web services and SaaS applications work; Exceptional service orientation and ability to lead change using positive and collaborative methods; removes barriers, acts with a sense of urgency, and leads by example; Demonstrated history of leadership in managing high-impact machine learning and engineering projects, preferably in a SaaS startup/scaleup environment; Strong leadership, vision, and proven track record of creating and successfully executing a data strategy.A track record of managing a globally distributed, multi-discipline Data team, with experience overseeing Data Science, Data Engineering, and Machine Learning; Experience with distributed system design, system scaling, and performance optimization; Practical experience implementing machine learning models & GenAI for predictive business outcomes; Strong communication and collaboration skills across multiple levels, from detailed technical designs with engineering to broad product initiatives with key business stakeholders; Knowledge of modern DevOps technologies, such as Terraform, Kubernetes, ELK stack, Prometheus, etc; Technical expertise with the building blocks of a modern data stack, such as Prefect, AWS Glue, Iceberg, MLflow, dbt, Snowflake, and others; Experience with large-scale data migration projects, ensuring minimal disruption and data integrity; Expertise in designing and implementing real-time data processing and streaming solutions; 11 more items(s)","What We Offer:Competitive salary and incentive compensation structure; Hybrid work model; Extensive Benefits package (Health, Dental, Vision, Disability, 401k w/ match, etc.).Best-in-class tech gear; Access to continuous development tools & a strong coaching culture; Employee stock ownership plan (ESOP); Generous vacation & leave policies; Complimentary weekly language courses; Weekly in-office lunch delivery budget; 5 more items(s)","SQL, PostgreSQL, Clickhouse, Elasticsearch, Databricks, Snowflake, SageMaker, Tableau, Terraform, Kubernetes, ELK stack, Prometheus, Prefect, AWS Glue, Iceberg, MLflow, dbt"
Senior Java Data Engineer,Apex Systems,"Apex Systems • San Jose, CA •  via Ladders",13 days ago,Full-time,"sesses an Strong level of SQL expertise and a solid understanding of the healthcare industry. Python for scripting and performance tuning. Talend for ETL processes. ---Key Responsibilities: 1+ years healthcare data experience is a huge plus1. Strong SQL expertise - 2+ years - Write, optimize, and maintain SQL queries using joins, data functions, tables, stored procs and other functionalities.2. Strong Python - Scripting, performance tuning, etc3. Provide support for ETL (Extract, Transform, Load) development and processes. TALEND highly preferred4. Utilize healthcare knowledge to contribute to the development of health-related projects and ensure data integrity and accuracy.5. Participate in regular team meetings to discuss ongoing projects, share insights, and ensure best practices in SQL development.6. Assist in troubleshooting database-related issues and provide solutions in a timely manner.","The ideal candidate possesses an Strong level of SQL expertise and a solid understanding of the healthcare industry; Python for scripting and performance tuning; --Key Responsibilities: 1+ years healthcare data experience is a huge plus1; Strong SQL expertise - 2+ years - Write, optimize, and maintain SQL queries using joins, data functions, tables, stored procs and other functionalities; Strong Python - Scripting, performance tuning, etc3; 2 more items(s)","Talend for ETL processes; Provide support for ETL (Extract, Transform, Load) development and processes; TALEND highly preferred4; Utilize healthcare knowledge to contribute to the development of health-related projects and ensure data integrity and accuracy.5. Participate in regular team meetings to discuss ongoing projects, share insights, and ensure best practices in SQL development.6. Assist in troubleshooting database-related issues and provide solutions in a timely manner; 1 more items(s)","SQL, Python, Talend"
Senior Devops/ Data Engineer,Skimlinks,"Skimlinks • Los Angeles, CA •  via Jobtrees",Full-time,No Degree Mentioned,"ess in the US and in Europe. Our Engineers develop next-generation technologies that help retailers reach those qualified consumers.We have an opportunity available for an experienced back-end Sr. Software Engineer, that is passionate about building reliable, loosely coupled systems while prioritizing security and performance. A good match for this position is someone who is skilled in problem-solving, cares about code quality, keeps up-to-date on industry best practices, is strategic in their vision and tactical in their execution.Stack• Currently OnPrem and migrating to GCP• Kubernetes is our default choice for microservices• Current principal system language is Java• Datastores: MySQL, BigTable, BigQuery, Oracle• Data Processing: Hadoop, Beam, Dataflow• Messaging: Google PubSub, KafkaResponsibilities:• Develop data processing pipeline to ingest and enrich product data for internal and external customers• Develop our search platform that helps our internal and external customers to discover, monetize our product inventory• Help implement our new applications and infrastructure leveraging: Google Cloud, Kubernetes• Ensure high code-quality and high availability of our inventory platform","Software Engineer, that is passionate about building reliable, loosely coupled systems while prioritizing security and performance; A good match for this position is someone who is skilled in problem-solving, cares about code quality, keeps up-to-date on industry best practices, is strategic in their vision and tactical in their execution; Currently OnPrem and migrating to GCP; Kubernetes is our default choice for microservices; Current principal system language is Java; Datastores: MySQL, BigTable, BigQuery, Oracle; Data Processing: Hadoop, Beam, Dataflow; 4 more items(s)","Develop data processing pipeline to ingest and enrich product data for internal and external customers; Develop our search platform that helps our internal and external customers to discover, monetize our product inventory; Help implement our new applications and infrastructure leveraging: Google Cloud, Kubernetes; Ensure high code-quality and high availability of our inventory platform; 1 more items(s)","GCP, Kubernetes, Java, MySQL, BigTable, BigQuery, Oracle, Hadoop, Beam, Dataflow, Google PubSub, Kafka"
Senior Data Engineer - Tech Lead,"CrowdStrike, Inc.","CrowdStrike, Inc. • Remote, OR •  via Virtual Vocations",25 days ago,Full-time,BMS optimization and SQL query design• Proficiency in scripting and automation with Go• Experience with network management protocols and APIs• Strong focus on security in code and system development,10+ years of experience in a large-scale production environment; Experience with RDBMS optimization and SQL query design; Proficiency in scripting and automation with Go; Experience with network management protocols and APIs; Strong focus on security in code and system development; 2 more items(s),Design and implement data pipelines from diverse sources; Identify and resolve discrepancies between data sources; Develop automated quality checks and monitoring procedures for data consistency,"SQL, Go, RDBMS"
Sr Data Engineer,"Netpace, Inc.","Netpace, Inc. • Milpitas, CA •  via ZipRecruiter",Full-time,No Degree Mentioned,"ition has a proven track record as a consultant with strong technical skills, demonstrates an ability to lead small teams and can communicate and build business relationships.ResponsibilitiesDesign and develop the end to end solution for the deployment of dashboards and reports to various stakeholders Work directly with the business teams to rapidly prototype analytics solutions based upon business requirements Architect data pipelines and ETL processes to connect with various data sources Design and maintain enterprise data warehouse models Tune and improve the performance of reporting solutions Manage cloud based data & analytics platform (MS Azure) Create Semantic Data Models as the foundation for self-service enterprise reportingSkills:RequirementsMinimum of five years in working with Enterprise Data Warehouse technologies including Multi-Dimensional Data Modeling, Data Architectures or other work related to the construction of enterprise data assets Proven understanding and demonstrable implementation experience of cloud data platform technologies (Microsoft Azure, SQL Data Warehouse, Azure Analysis Services) Experience with Data Visualization tools and technologies (Power BI PRO) Proven track record driving rapid prototyping and designs Strong SQL programming background with the ability to troubleshoot and tune the code Takes ownership of problems and proactively resolves technical problems, ensuring that technical solutions continue to meet business requirements and aligns with the vision, mission, objectives, strategy of the team Highly motivated, goal driven, innovative, curious and open minded, Fun to work with, team playerAdditional QualificationsAbility to spend extended periods of time on computer equipment and/or in meeting/classroom environments Must be able to speak and listen clearly on telephone Ability to maintain regular, punctual attendance consistent with the ADA, FMLA and other federal, state and local standards",,,"Microsoft Azure, SQL Data Warehouse, Azure Analysis Services, Power BI PRO, SQL"
Cybersecurity Data Engineer I (Hybrid),SpaceX,"SpaceX • Hawthorne, CA •  via JobzMall",110K–145K a year,Full-time,"eam that is revolutionizing space technology.The successful candidate should have a deep knowledge of data engineering principles and experience with data warehousing, database design, and ETL processes. You should also have a strong understanding of distributed systems, cloud infrastructure, and database architectures. Excellent communication skills, a passion for complex problem-solving, and a drive to create scalable and robust data solutions are essential. If you have a passion for data engineering, a keen eye for details, and the drive to build something extraordinary, we want to hear from you!SpaceX is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.","The successful candidate should have a deep knowledge of data engineering principles and experience with data warehousing, database design, and ETL processes; You should also have a strong understanding of distributed systems, cloud infrastructure, and database architectures; Excellent communication skills, a passion for complex problem-solving, and a drive to create scalable and robust data solutions are essential; If you have a passion for data engineering, a keen eye for details, and the drive to build something extraordinary, we want to hear from you!; 1 more items(s)",,"data warehousing, database design, ETL processes, distributed systems, cloud infrastructure, database architectures"
Senior Data Engineer-AWS (Onsite or Remote) (NP – 19233),Sierra Nevada Corporation,Sierra Nevada Corporation •  via EWorker,7 days ago,Full-time,"ds the SIEM and data science platforms in Azure Gov Cloud.As SNC’s corporate team, we provide the company and its business areas with strategic direction and business support spanning executive management, finance and accounting, operations, human resources, legal, IT, information security, facilities, marketing, and communications.• Role is Contingent Upon Award*Responsibilities:Collect data from a variety of systemsDesign, build and maintain batch or real-time data pipelines.Maintain and optimize the data infrastructure required for accurate extraction, transformation, and loading of data from a wide variety of data sources.Develop ETL (extract, transform, load) processes to help extract and manipulate data from multiple sources.Automate data workflows such as data ingestion, aggregation, and ETL processing.Partner with data scientists to deploy machine learning models.Build, maintain, and deploy data products for analytics and data science teams.Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.Monitor data systems performance and implement optimization strategies.Leverage data controls to maintain data privacy, security, compliance, and quality for allocated areas of ownership.Must-haves:Bachelor’s degree in Cybersecurity, Network Engineering, Information Technology, Computer Science, or related Engineering/Science discipline and 0-2 years of relevant experienceHigher education may substitute for relevant experienceRelevant experience may be considered in lieu of required educationWorking SQL knowledge and experience working with relational databasesExperience with cloud-based data platforms such as AWS, AzureExperience in Analytics, Statistics, Software Development, Cybersecurity, Mathematics, or related fieldExperience supporting government customers and contractorsUnderstanding of the multitude of tools and processes required to be implemented and managed by a Security Operations organizationBackground running tools in and supporting on-premise, Cloud, and hybrid environmentsThe ability to obtain and maintain a Secret U.S. Security Clearance is required.Preferred:Knowledge of Cloud platforms such as AWS, Azure, etcKnowledge of Data Management, Data Warehousing concepts & methodologiesExperience combining and correlating large, complex, and disparate data sourcesExperience working with data engineering/science Python packagesExperience working with working on large data sets and distributed computing (e.g. Hive/Hadoop/Spark/Databricks/Azure Synapse)Experience working with SIEM solutions like ElasticSearch, Sentinel, Splunk, etc.Experience working with both relational and non-relational databasesExperience working with various data platforms and file formats like Hive, Hadoop, Kafka, json, parquet, pickle, etcExperience working with APIs / APIM (Azure API Management)Experience working with code editors like VS Code and AnacondaExperience working with source control on Github and developer tools such as Terraform, ARM/BicepExperience working with containerization tools like Docker, Kubernetes (like Azure Kubernetes Service, AKS), etcSNC offers a generous benefit package, including medical, dental, and vision plans, 401(k) with 150% match up to 6%, life insurance, 3 weeks paid time off, tuition reimbursement, and more.IMPORTANT NOTICE:This position requires the ability to obtain and maintain a Secret U.S. Security Clearance. U.S. Citizenship status is required as this position needs an active U.S. Security Clearance for employment. Non-U.S. citizens may not be eligible to obtain a security clearance. The Department of Defense Consolidated Adjudications Facility (DoD CAF), a federal government agency, handles the adjudicative aspects of the security clearance eligibility process for industry applicants. Adjudicative factors which affect the outcome of the eligibility determination include, but are not limited to, allegiance to the U.S., foreign influence, foreign preference, criminal conduct, security violations and illegal drug use.Learn more about the background check process for Security Clearances.SNC is a global leader in aerospace and national security committed to moving the American Dream forward. We’re known and respected for our mission and execution focus, agility, and disruptive and rapid innovation. We provide leading edge technologies and transformative solutions that support our nation’s most critical security needs. If you are mission-focused, thrive in collaborative environments, and want to make our country stronger with state-of-the-art technologies that safeguard freedom, join our team!As an Equal Opportunity Employer, we welcome our employees to bring their whole selves to their work. SNC is committed to fostering an inclusive, accepting, and diverse environment free of discrimination. Employment decisions are made without regarding to race, color, age, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran or other characteristics protected by law. Contributions to SNC come in many shapes and styles, and we believe diversity in our workforce fosters new and greater ways to dream, innovate, and inspire.","Bachelor’s degree in Cybersecurity, Network Engineering, Information Technology, Computer Science, or related Engineering/Science discipline and 0-2 years of relevant experience; Higher education may substitute for relevant experience; Relevant experience may be considered in lieu of required education; Working SQL knowledge and experience working with relational databases; Experience with cloud-based data platforms such as AWS, Azure; Experience in Analytics, Statistics, Software Development, Cybersecurity, Mathematics, or related field; Experience supporting government customers and contractors; Understanding of the multitude of tools and processes required to be implemented and managed by a Security Operations organization; Background running tools in and supporting on-premise, Cloud, and hybrid environments; The ability to obtain and maintain a Secret U.S. Security Clearance is required; Knowledge of Cloud platforms such as AWS, Azure, etc; Knowledge of Data Management, Data Warehousing concepts & methodologies; Experience combining and correlating large, complex, and disparate data sources; Experience working with data engineering/science Python packages; Experience working with working on large data sets and distributed computing (e.g; Experience working with SIEM solutions like ElasticSearch, Sentinel, Splunk, etc; Experience working with both relational and non-relational databases; Experience working with various data platforms and file formats like Hive, Hadoop, Kafka, json, parquet, pickle, etc; Experience working with APIs / APIM (Azure API Management); Experience working with code editors like VS Code and Anaconda; Experience working with source control on Github and developer tools such as Terraform, ARM/Bicep; Experience working with containerization tools like Docker, Kubernetes (like Azure Kubernetes Service, AKS), etc; This position requires the ability to obtain and maintain a Secret U.S. Security Clearance; U.S. Citizenship status is required as this position needs an active U.S. Security Clearance for employment; 21 more items(s)","SNC offers a generous benefit package, including medical, dental, and vision plans, 401(k) with 150% match up to 6%, life insurance, 3 weeks paid time off, tuition reimbursement, and more","AWS, Azure, Hive, Hadoop, Kafka, json, parquet, pickle, ElasticSearch, Sentinel, Splunk, Azure API Management, VS Code, Anaconda, Github, Terraform, ARM, Bicep, Docker, Kubernetes, Azure Kubernetes Service (AKS)"
Big Data Engineer (Contract W2 Hybrid onsite),GSPANN Technologies,"GSPANN Technologies • San Francisco, CA •  via Dice",18 hours ago,Contractor,"four global delivery centers, and approximately 1400 employees globally, we offer the intimacy of a boutique consultancy with capabilities of a large IT services firm.Google Cloud Platform Data Architect / EngineerLocation-San Francisco, CA (East Bay Area or Bay Area)Job Type-Long Term ContractWork type: 3 days on-siteSkills required:• Expert level of understanding of Google Cloud Data & associated services• Hands on programming expertise in SQL, Python & Spark• Design and implementation of end-to-end cloud based data platforms• Expert in data warehousing and analytics concepts, architecture and patterns• Good understanding of data modeling techniques and principlesWorking at GSPANN GSPANN is a diverse, prosperous, and rewarding place to work. We provide competitive benefits, educational assistance, and career growth opportunities to our employees. Every employee is valued for their talent and contribution. Working with us will give you an opportunity to work globally with some of the best brands in the industry. The company does and will take an affirmative action to employ and advance in the employment of individuals with disabilities and protected veterans, and to treat qualified individuals without discrimination based on their physical or mental disability status. GSPANN is an equal opportunity employer for minorities/females/veterans/disability.","Expert level of understanding of Google Cloud Data & associated services; Hands on programming expertise in SQL, Python & Spark; Design and implementation of end-to-end cloud based data platforms; Expert in data warehousing and analytics concepts, architecture and patterns; Good understanding of data modeling techniques and principles; 2 more items(s)","We provide competitive benefits, educational assistance, and career growth opportunities to our employees","Google Cloud Platform, SQL, Python, Spark"
Data Engineer (Hadoop / Python),CyberCoders,"CyberCoders • San Francisco, CA •  via Adzuna",2 days ago,80K–110K a year,"proficiency, GitWe are a rapidly growing start-up, and leader in workforce management. Our company empowers people leaders with robust workforce data to lead better decisions for their staff and their organizations. If you are an experienced Data Engineer looking for a great company to work for on the ground level, please apply now!• What you will be doing:*• Develop and maintain efficient data pipelines, addressing data challenges such as inconsistency, quality issues, and complex transformations.• Clean and process diverse data sets, including internal client data and data from external providers.• Collaborate with the lead engineer and CTO to understand pipeline structure and data processing requirements.• Integrate predictive analytics into data pipelines autonomously and collaboratively.• Prepare and format data for reporting purposes.• Utilize GCP services such as Cloud Storage and Compute Engine.• Develop comprehensive testing strategies to ensure data integrity and pipeline functionality.• Collaborate closely with the Project Manager to design and execute development sprints, drive iterative progress, and effectively integrate client feedback into ongoing projects.• Lead technical project development with a high degree of autonomy, owning key work streams and ensuring successful delivery• What you need for this role:*• 3+ years in Data Engineering, Machine Learning, AI related roles• Bachelor's degree in Computer Science or a related field. Master's Degree preferred• Proficiency in Python and its data-related libraries (Pandas, NumPy, SciPy)• Familiarity with the use of NLP to set up experiments• Experience working with major cloud platforms, GCP a plus• Experience with version control using Git• Strong attention to processing integrity and accuracy• Excellent communication skills• Effective problem-solving abilities using best practices in product testing• Demonstrated ability to work independently and collaboratively• High attention to detail, particularly with data security best practices• If you're interested, please APPLY FIRST, then provide the following to get the ball rolling:*1) Email me your updated resume, some details on why you might be a good fit2) Salary requirements (a range preferably)3) Interview availability this week and next week, for initial screening, time zones included• My email is:* [Brett.Lorin@cybercoders.com](mailto:Brett.Lorin@cybercoders.com)• Benefits*- Competitive Base Salary!- Start-Up, Rapidly Growing Company- PTO- Equity for qualified candidates- Full Benefits- Career growth opportunities+ More!- Applicants must be authorized to work in the U.S.• CyberCoders is proud to be an Equal Opportunity Employer*All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable state and local law, including but not limited to the Los Angeles County Fair Chance Ordinance, the San Francisco Fair Chance Ordinance, and the California Fair Chance Act. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements.","3+ years in Data Engineering, Machine Learning, AI related roles; Bachelor's degree in Computer Science or a related field; Applicants must be authorized to work in the U.S; CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable state and local law, including but not limited to the Los Angeles County Fair Chance Ordinance, the San Francisco Fair Chance Ordinance, and the California Fair Chance Act; 1 more items(s)","$80,000-$110,000 - Depending on Experience, plus equity potential; Competitive Base Salary!; Start-Up, Rapidly Growing Company; PTO; Equity for qualified candidates; Full Benefits; Career growth opportunities; 4 more items(s)","Git, GCP, Cloud Storage, Compute Engine, Python, Pandas, NumPy, SciPy"
Senior Data Engineer – Information Technology Department - San Francisco Human Services Agency (1054) (140650),Aircall,"Aircall • San Francisco, CA •  via Lever",Full-time,Health insurance,"k, San Francisco, Sydney, Madrid, London, Berlin, or at home – everyone has a voice that is valued.Whatever your background, wherever you’re from – we want you to join the conversation. Let’s talk.As a Data Engineer, you will be joining our San Francisco office with our strong C-level presence on the West Coast, including our CTO and CPO. We are building the future of communication and we would love for you to be a part of it.About the role:We are looking for a passionate and experienced data engineer to join our team and contribute to Aircall’s rapid growth. As a Senior Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure.This role offers the unique opportunity to be at the forefront of our engineering efforts to develop and optimize the technologies that will scale our business via enhancing the customer journey. Collaborating with teams across the company, you will leverage your technical skills to solve complex challenges, improve operational efficiency and lead our business towards its strategic goals.Your mission @ Aircall:• Design, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.• Enhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.• Implement best practices for data management, storage and security to ensure data integrity and compliance with regulations.• Own the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.• Participate in code reviews to ensure code quality and share knowledge.• Lead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.• Define and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.• Mentor junior members of the team, providing guidance and support in their professional development.• Collaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestrationA little more about you:• Bachelor's degree or higher in Computer Science, Engineering, or a related field.• 5+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.• Proficient in SQL and Python, with the ability to translate complexity into efficient code.• Experience with data workflow development and management tools (dbt, Airflow).• Solid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.• Strong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.• Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.• Experience with data tooling, data governance, business intelligence and data privacy is a plus.$120,000 - $220,000 a yearThis is not including equity and other benefits. The actual salary offered will carefully consider a wide range of factors, including your skills, qualifications, and experience.Aircall is constantly moving forward. We’re building new roads to complete our journey, and we’re taking people with us who have the same builder mentality.Let’s grow together: Aircall is a place for those who dare to be bold and seek responsibility, excellence, and the opportunity to push themselves to new heights.We’re creating a place where great people trust one another and thrive together.People flourish at Aircall and now is the time to be part of the team and the journey we’re on.Why join us?🚀 Key moment to join Aircall in terms of growth and opportunities💆‍♀️ Our people matter, work-life balance is important at Aircall📚 Fast-learning environment, entrepreneurial and strong team spirit🌍 45+ Nationalities: cosmopolite & multi-cultural mindset💵 Competitive salary package & equity🏨 Medical, dental, and vision insurance is 100% covered📈 401k plan with company matching!✈️ Unlimited PTO — take the time you need to come to work feeling great!⭐️ Wellness, commuter, and childcare reimbursements💚 Generous parental leave policyDE&I Statement:At Aircall, we believe diversity, equity and inclusion – irrespective of origins, identity, background and orientations – are core to our journey.We pride ourselves on promoting active inclusion within our business to foster a strong sense of belonging for all. We’re working to create a place filled with diverse people who can enrich and learn from one another. We’re committed to ensuring that everyone not only has a seat at the table but is valued and respected at it by providing equal opportunities to develop and thrive.We will constantly challenge ourselves to make sure that we live up to our ambitions around diversity, equity and inclusion, and keep this conversation open. Above all else, we understand and acknowledge that we have work to do and much to learn.Want to know more about candidate privacy? Find our Candidate Privacy Notice here.","Bachelor's degree or higher in Computer Science, Engineering, or a related field; 5+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure; Proficient in SQL and Python, with the ability to translate complexity into efficient code; Experience with data workflow development and management tools (dbt, Airflow); Solid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure; Strong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues; Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment; 4 more items(s)","$120,000 - $220,000 a year; This is not including equity and other benefits; The actual salary offered will carefully consider a wide range of factors, including your skills, qualifications, and experience; 💵 Competitive salary package & equity; 🏨 Medical, dental, and vision insurance is 100% covered; 📈 401k plan with company matching!; ✈️ Unlimited PTO — take the time you need to come to work feeling great!; ⭐️ Wellness, commuter, and childcare reimbursements; 💚 Generous parental leave policy; 6 more items(s)","SQL, Python, dbt, Airflow, AWS, GCP, Azure"
Staff Engineer Software (Data Engineering),City and County of San Francisco,"City and County of San Francisco • San Francisco, CA •  via SF Careers",Full-time,,", including data analysis, cost-benefit analysis, project planning and management, data project implementation and data validation, code migration to production, technical and procedural documentation, user training, and post-implementation assessment and administration.Under general direction, the Senior Data Engineer will help meet ongoing needs for the development, maintenance and enhancement of the Human Services Agency’s data warehousing and data analytics requirements.ESSENTIAL DUTIES AND FUNCTIONS• Coordinating and conducting user analysis sessions to understand the business process, determine user demands & document requirements for the data team.• Analyze and review requirements for new and existing data integration projects• Design and evolve the data warehouse design using dimensional modeling for the data engineering team to meet the organizations data demands.• Generate detailed low level design for data integration user stories.• Build complex ETL/ELT packages and solutions to facilitate the data integration needs of the organization using Microsoft SSIS or Oracle Data Integrator (ODI).• Perform unit testing and regression testing of data integration solutions developed in Microsoft SSIS & ODI.• Collaborate with the Business Intelligence (BI) Engineer to evolve the BI metadata design and assist in maintaining the enterprise metadata model.• Analyze and review current ETL/ELT jobs for performance, data latency and database resources in order to tune and redesign jobs as needed.• Provide technical assistance in prioritizing and resolving operationalized ETL/ELT processes.• Analyzing and developing complex ad hoc data extracts, reports to support decision makers.• Ensuring data results are verified and data issues are debugged.• Preparing user and technical manuals and instructions.","Under general direction, provides direct ongoing supervision to other IS Business Analysts (Data Engineers), serves as the top technical authority for data integration projects, provides technical leadership and direction and assumes technical responsibility for the completion of data projects; performs or oversees all or most of the phases of the data projects and ongoing administration functions, including data analysis, cost-benefit analysis, project planning and management, data project implementation and data validation, code migration to production, technical and procedural documentation, user training, and post-implementation assessment and administration; Under general direction, the Senior Data Engineer will help meet ongoing needs for the development, maintenance and enhancement of the Human Services Agency’s data warehousing and data analytics requirements; Coordinating and conducting user analysis sessions to understand the business process, determine user demands & document requirements for the data team; Analyze and review requirements for new and existing data integration projects; Design and evolve the data warehouse design using dimensional modeling for the data engineering team to meet the organizations data demands; Generate detailed low level design for data integration user stories; Build complex ETL/ELT packages and solutions to facilitate the data integration needs of the organization using Microsoft SSIS or Oracle Data Integrator (ODI); Perform unit testing and regression testing of data integration solutions developed in Microsoft SSIS & ODI; Collaborate with the Business Intelligence (BI) Engineer to evolve the BI metadata design and assist in maintaining the enterprise metadata model; Analyze and review current ETL/ELT jobs for performance, data latency and database resources in order to tune and redesign jobs as needed; Provide technical assistance in prioritizing and resolving operationalized ETL/ELT processes; Analyzing and developing complex ad hoc data extracts, reports to support decision makers; Ensuring data results are verified and data issues are debugged; Preparing user and technical manuals and instructions; 11 more items(s)",,"Microsoft SSIS, Oracle Data Integrator (ODI)"
Sr. Data Engineer - Remote,Unreal Gigs,"Unreal Gigs • San Francisco, CA •  via ZipRecruiter",Full-time,Paid time off,"the future of big data engineering.Position Overview: As a Senior Big Data Engineer, you'll play a pivotal role in designing, building, and maintaining our big data infrastructure and pipelines. You'll work on challenging projects, from data ingestion and processing to data storage and retrieval, to support the needs of our data-driven organization. If you're a seasoned engineer with expertise in big data technologies and a passion for building robust data systems, we want you on our team.RequirementsKey Responsibilities:• Big Data Infrastructure Design: Design, architect, and implement scalable and efficient big data infrastructure, including data lakes, data warehouses, and streaming data platforms, leveraging technologies such as Hadoop, Spark, Kafka, and more.• Data Pipeline Development: Develop, deploy, and manage data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data from diverse sources, ensuring reliability, scalability, and performance.• Data Modeling: Design and implement data models and schemas to support analytical and operational requirements, ensuring data integrity, consistency, and performance in a distributed environment.• Data Integration: Integrate data from disparate sources and systems, ensuring data consistency, quality, and completeness throughout the data lifecycle.• Performance Optimization: Optimize data pipelines and queries for performance and efficiency, identifying and addressing bottlenecks and inefficiencies to improve system scalability and reliability.• Monitoring and Alerting: Implement monitoring and alerting systems to track big data platform performance and health, detecting and mitigating issues proactively to minimize downtime and data loss.• Security and Compliance: Implement security controls and data governance policies to ensure data security, privacy, and compliance with regulatory requirements.• Documentation and Best Practices: Document big data infrastructure and pipelines, providing clear and comprehensive documentation to facilitate understanding and collaboration among team members.• Collaboration: Collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders, to understand requirements and deliver big data solutions that meet business needs.• Mentorship and Leadership: Mentor junior engineers, providing guidance, support, and technical leadership in big data engineering best practices and technologies.Qualifications:• Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field.• 5+ years of experience in big data engineering, with a focus on designing, building, and maintaining big data infrastructure and pipelines.• Proficiency in programming languages such as Java, Scala, or Python, and experience with big data technologies such as Hadoop, Spark, Kafka, Hive, HBase, and more.• Strong understanding of distributed systems and parallel processing, with experience designing and optimizing data pipelines for performance and scalability.• Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and familiarity with cloud-based big data services such as Amazon EMR, Azure HDInsight, or Google Dataproc.• Experience with SQL and NoSQL databases, data warehousing, and ETL/ELT processes.• Strong problem-solving skills and analytical thinking, with the ability to troubleshoot complex data issues and optimize system performance.• Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders.Benefits• Competitive salary: The industry standard salary for Senior Big Data Engineers typically ranges from $170,000 to $230,000 per year, depending on experience and qualifications.• Comprehensive health, dental, and vision insurance plans.• Flexible work hours and remote work options.• Generous vacation and paid time off.• Professional development opportunities, including access to training programs, conferences, and workshops.• State-of-the-art technology environment with access to cutting-edge tools and resources.• Vibrant and inclusive company culture with opportunities for growth and advancement.• Exciting projects with real-world impact at the forefront of big data innovation.Join Us: Ready to shape the future of big data engineering? Apply now to join our team and be part of the data revolution!","Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field; 5+ years of experience in big data engineering, with a focus on designing, building, and maintaining big data infrastructure and pipelines; Proficiency in programming languages such as Java, Scala, or Python, and experience with big data technologies such as Hadoop, Spark, Kafka, Hive, HBase, and more; Strong understanding of distributed systems and parallel processing, with experience designing and optimizing data pipelines for performance and scalability; Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and familiarity with cloud-based big data services such as Amazon EMR, Azure HDInsight, or Google Dataproc; Experience with SQL and NoSQL databases, data warehousing, and ETL/ELT processes; Strong problem-solving skills and analytical thinking, with the ability to troubleshoot complex data issues and optimize system performance; Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders; 5 more items(s)","Competitive salary: The industry standard salary for Senior Big Data Engineers typically ranges from $170,000 to $230,000 per year, depending on experience and qualifications; Comprehensive health, dental, and vision insurance plans; Flexible work hours and remote work options; Generous vacation and paid time off; Professional development opportunities, including access to training programs, conferences, and workshops; State-of-the-art technology environment with access to cutting-edge tools and resources; Vibrant and inclusive company culture with opportunities for growth and advancement; 4 more items(s)","Hadoop, Spark, Kafka, Java, Scala, Python, Hive, HBase, AWS, Azure, Google Cloud Platform, Amazon EMR, Azure HDInsight, Google Dataproc, SQL, NoSQL"
Big Data Engineer (Contract W2 Hybrid onsite),Dice,"Dice • San Francisco, CA •  via LinkedIn",2 hours ago,Contractor,tgres etc. Skill level expected: goodBig Data Engineer (Contract W2 Hybrid onsite),"Core skill needed is Apache Spark and a programming language (preferably Scala with Java) or else Python; Skill level expected: excellent; Secondary skill needed is some experience with Distributed computing, Big Data, Oozie, Postgres etc; Skill level expected: good; 1 more items(s)","Expert level of understanding of Google Cloud Data & associated services; Hands on programming expertise in SQL, Python & Spark; Design and implementation of end-to-end cloud based data platforms; Expert in data warehousing and analytics concepts, architecture and patterns; Good understanding of data modeling techniques and principles; Working at GSPANN; 3 more items(s)","Apache Spark, Scala, Java, Python, Distributed computing, Oozie, Postgres, Google Cloud Data, SQL"
Senior Data Engineer - Real Estate and Workplace,ClassDojo,"ClassDojo • San Francisco, CA •  via Greenhouse",Full-time,No Degree Mentioned,"used in over 95% of US schools, reaching over 50 million children in 180 countries, with a team of just around 200 people [1]. We are now beginning to use this network to give kids the best learning experiences in the world, far beyond those a standard school can provide.We hire for talent density. Our team comprises the most talented, entrepreneurial, and innovative teammates from around the world, with experience in education and large scale consumer internet companies, including Instagram, Netflix, Dropbox, Stripe, Uber, Y Combinator, and more. We’re building a company where the most talented people want to work. We believe you’ll do the best work of your life here—and you’ll pioneer the future of education, too.What you’ll do:We are seeking a Data Platform engineer to join our Data and Engineering team. The ideal candidate will possess a passion for education and a strong foundation in all aspects of data engineering. In this pivotal role, you will be instrumental in making data accessible and actionable, facilitating informed decision-making throughout the company.You will be a match if:You have 7+ years experience with data / analytics engineeringCollaboration and Communication:• You have owned large-scale data projects that drove significant impact across the company• You enjoy collaborating with and supporting others.• You are an effective partner with engineering, data analysts, data scientists, and other business stakeholders.• You have an ownership mindset and can make things happenTechnical Excellence• You're a strong independent contributor who's comfortable working across the data stack• You've designed and built data platforms at previous organizations• Your SQL, python, and infrastructure skills are top-notch.[1] Some more context:(If you are on LinkedIn, you will not be able to access the hyperlinks below. Once you click apply, you will be directed to our career website (if you are not on there already) and will be able to access the hyperlinks)- ClassDojo's $125m Series D (Forbes) and Sam’s note about it.- ClassDojo is one of Y Combinator’s Top 100 companies- ClassDojo's Second Act Comes with First Profits (TechCrunch) and Sam's note about it.We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. In accordance with the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are happy to accommodate any disabilities or special needs. We are a distributed company, so we hire regardless of location, as long as you are willing to have significant hours overlap with one of the Americas time zones.ClassDojo takes a number of factors into consideration when determining compensation, including geographic location, experience, and skillset. Salary ranges (United States):CA, WA, NY, NJ, CT states: $171,500 - $244,000 (USD)All other states in the US: $146,000 - $207,500 (USD)#LI-Remote","The ideal candidate will possess a passion for education and a strong foundation in all aspects of data engineering; You have 7+ years experience with data / analytics engineering; You have owned large-scale data projects that drove significant impact across the company; You enjoy collaborating with and supporting others; You are an effective partner with engineering, data analysts, data scientists, and other business stakeholders; You have an ownership mindset and can make things happen; Technical Excellence; You're a strong independent contributor who's comfortable working across the data stack; You've designed and built data platforms at previous organizations; Your SQL, python, and infrastructure skills are top-notch; 7 more items(s)","CA, WA, NY, NJ, CT states: $171,500 - $244,000 (USD)","SQL, Python"
Senior Geospatial Data Engineer,OpenAI,"OpenAI • San Francisco, CA •  via Hiregeeks",Full-time,No Degree Mentioned,"Senior Data Engineer for Real Estate and Workplace at OpenAI, skilled in ETL, Apache Spark, and Airflow.","The ideal candidate will possess substantial professional experience in spatial data analysis, quality assurance/control, data engineering, and experience working with large-scale, scalable data solutions; Join a team that is focused on spatial data evaluation, automation, and data quality measurement, comprised of diverse skills and backgrounds, all with the same goal: helping to build the world’s best map; Far more than any specific experience or skill, we are looking for engineers who are driven to build robust and reliable software, eager to learn and hone their skills, and enthusiastic about facilitating growth and mentorship among team members; Bachelor’s degree or equivalent experience in Geography, Geomatics, Computer Science, or a related field and 8+ years of experience; Experience as a Geospatial Data Engineer or similar role, with expertise working with spatial data using Postgres/PostGIS, and familiarity with tools like Tableau for reporting; Experience with AWS/GCC services such as EKS, EMR, and familiarity with big data technologies like Spark; Experience with data serialization formats like protobuf/avro and data management tools like Iceberg; Strong understanding of geographic projections, spatial boolean fundamentals, and 2D/3D data processing; Knowledge of raster and vector data formats, DEM/TIN data, and associated spatial libraries and tools; 6 more items(s)","Pay & Benefits; Pay & Benefits; At Apple, base pay is one part of our total compensation package and is determined within a range; This provides the opportunity to progress as you grow and develop within a role; The base pay range for this role is between $175,800 and $312,200, and your base pay will depend on your skills, qualifications, experience, and location; Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs; Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan; You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition; Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation; Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program; 7 more items(s)","ETL, Apache Spark, Airflow, Postgres/PostGIS, Tableau, AWS, EKS, EMR, protobuf, avro, Iceberg"
Snowflake Data Engineer,Apple,"Apple • Cupertino, CA •  via Careers At Apple",Full-time,Dental insurance,"al data evaluation, automation, and data quality measurement, comprised of diverse skills and backgrounds, all with the same goal: helping to build the world’s best map.We believe that every engineer brings unique skills and perspectives to the table. Far more than any specific experience or skill, we are looking for engineers who are driven to build robust and reliable software, eager to learn and hone their skills, and enthusiastic about facilitating growth and mentorship among team members.DescriptionDescriptionOur Data Evaluation team collaborates with various groups within Apple Maps to enhance the efficiency and accuracy of Maps data evaluation and analysis. You will be working with us tohelp automate the measurement of data quality and change over time, working in coordination with our Evaluation Engineering and Data Engineering teams.Ongoing projects necessitate the design and implementation of novel automated systems that seamlessly integrate with the Maps data production processes, enabling continuous evaluations and the extraction of valuable insights. The successful candidate will closely collaborate with other data engineers, software engineers, data scientists, evaluation and SRE teams to deliver business objectives associated with this project, ensuring alignment to budget and time constraints.Designing and implementing geospatial databases, data models, and ETL processes using tools like Postgres/PostGIS, and working with serialized data formats such as protobuf/avro.- Developing quality checks and metrics on large scale spatial data- Collaborating closely with data scientists, analysts, and partners to understand data requirements and provide geospatial solutions.- Integrating geospatial data with other systems and applications using AWS/GCC services like EKS, EMR, and working with big data technologies like Spark.- Optimizing spatial queries for performance and scalability across multiple toolsets- Ensuring data quality, consistency, and accuracy through automated testing and validation processes.- Documenting processes, systems, and datasets for future reference and replication.- Providing technical support and solving for geospatial data systems.Minimum QualificationsMinimum Qualifications• Bachelor’s degree or equivalent experience in Geography, Geomatics, Computer Science, or a related field and 8+ years of experience.• Experience as a Geospatial Data Engineer or similar role, with expertise working with spatial data using Postgres/PostGIS, and familiarity with tools like Tableau for reporting.• Experience with AWS/GCC services such as EKS, EMR, and familiarity with big data technologies like Spark.• Experience with data serialization formats like protobuf/avro and data management tools like Iceberg.• Strong understanding of geographic projections, spatial boolean fundamentals, and 2D/3D data processing. Knowledge of raster and vector data formats, DEM/TIN data, and associated spatial libraries and tools.Key QualificationsKey QualificationsPreferred QualificationsPreferred Qualifications• Strong proficiency in Python, Scala, SQL, and shell scripting for data manipulation and ETL processes.• Work closely with multiple multi-functional teams to successfully coordinate and handle user expectations• Excellent communication skillsEducation & ExperienceEducation & ExperienceAdditional RequirementsAdditional RequirementsPay & BenefitsPay & Benefits• At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $175,800 and $312,200, and your base pay will depend on your skills, qualifications, experience, and location.Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.More• Apple is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, Veteran status, or other legally protected characteristics. Learn more about your EEO rights as an applicant.","The ideal candidate will possess substantial professional experience in spatial data analysis, quality assurance/control, data engineering, and experience working with large-scale, scalable data solutions; Join a team that is focused on spatial data evaluation, automation, and data quality measurement, comprised of diverse skills and backgrounds, all with the same goal: helping to build the world’s best map; Far more than any specific experience or skill, we are looking for engineers who are driven to build robust and reliable software, eager to learn and hone their skills, and enthusiastic about facilitating growth and mentorship among team members; Bachelor’s degree or equivalent experience in Geography, Geomatics, Computer Science, or a related field and 8+ years of experience; Experience as a Geospatial Data Engineer or similar role, with expertise working with spatial data using Postgres/PostGIS, and familiarity with tools like Tableau for reporting; Experience with AWS/GCC services such as EKS, EMR, and familiarity with big data technologies like Spark; Experience with data serialization formats like protobuf/avro and data management tools like Iceberg; Strong understanding of geographic projections, spatial boolean fundamentals, and 2D/3D data processing; Knowledge of raster and vector data formats, DEM/TIN data, and associated spatial libraries and tools; 6 more items(s)","Pay & Benefits; Pay & Benefits; At Apple, base pay is one part of our total compensation package and is determined within a range; This provides the opportunity to progress as you grow and develop within a role; The base pay range for this role is between $175,800 and $312,200, and your base pay will depend on your skills, qualifications, experience, and location; Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs; Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan; You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition; Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation; Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program; 7 more items(s)","Postgres, PostGIS, protobuf, avro, AWS, EKS, EMR, Spark, Iceberg, Tableau, Python, Scala, SQL, shell scripting"
Data Engineer (Hadoop / Python),Intelliswift Software,"Intelliswift Software • Dublin, CA •  via LinkedIn",15 hours ago,Full-time,"lus.Key Responsibilities:Perform data analysis to identify trends, patterns, and insights.Utilize data analytics tools to process and interpret data.Work with data lakes to gather and organize data.Answer business questions through detailed data analysis.Must-Have Skills:Strong analytical skillsSelf-starter with the ability to work independentlyExperience with SnowflakeProficiency in AzureExperience with Azure Data Factory (ADF)",We are seeking a highly skilled Data Engineer with strong analytical skills and the ability to work independently; The ideal candidate will be a self-starter capable of piecing together data to answer complex questions; Strong analytical skills; Self-starter with the ability to work independently; Experience with Snowflake; Proficiency in Azure; Experience with Azure Data Factory (ADF); 4 more items(s),"Location: Dublin, CA / Onsite Role (5 Days onsite); Duration: Fulltime with Intelliswift; Perform data analysis to identify trends, patterns, and insights; Utilize data analytics tools to process and interpret data; Work with data lakes to gather and organize data; Answer business questions through detailed data analysis; 3 more items(s)","Snowflake, Azure, Azure Data Factory (ADF)"
Senior Data Engineer – Information Technology Department - San Francisco Human Services Agency (1054) (140650),Atria Group LLC,"Atria Group LLC • San Francisco, CA •  via ZipRecruiter",2 days ago,Full-time,"isStrong hands-on, high volume, performance tuning DW experienceVertica experience a plusSelf-sufficient in extremely technical, high pace and exploratory environmentB.S. degree in CS / EE. Higher level degree is a plusLocation: San Francisco, CADuration: 6+ MonthsAdditional InformationApply today!","At least 8 years of experience in end to end data integration/data warehousing implementations and at project experience with large data volumes; Hands-on Hadoop / Hive implementation / data movement experience; Scripting in the following languages: Linux shell and Python; Strong SQL development experience to support data integration and analysis; Strong hands-on, high volume, performance tuning DW experience; Self-sufficient in extremely technical, high pace and exploratory environment; B.S. degree in CS / EE; 4 more items(s)","Under general direction, provides direct ongoing supervision to other IS Business Analysts (Data Engineers), serves as the top technical authority for data integration projects, provides technical leadership and direction and assumes technical responsibility for the completion of data projects; performs or oversees all or most of the phases of the data projects and ongoing administration functions, including data analysis, cost-benefit analysis, project planning and management, data project implementation and data validation, code migration to production, technical and procedural documentation, user training, and post-implementation assessment and administration; Under general direction, the Senior Data Engineer will help meet ongoing needs for the development, maintenance and enhancement of the Human Services Agency’s data warehousing and data analytics requirements; Coordinating and conducting user analysis sessions to understand the business process, determine user demands & document requirements for the data team; Analyze and review requirements for new and existing data integration projects; Design and evolve the data warehouse design using dimensional modeling for the data engineering team to meet the organizations data demands; Generate detailed low level design for data integration user stories; Build complex ETL/ELT packages and solutions to facilitate the data integration needs of the organization using Microsoft SSIS or Oracle Data Integrator (ODI); Perform unit testing and regression testing of data integration solutions developed in Microsoft SSIS & ODI; Collaborate with the Business Intelligence (BI) Engineer to evolve the BI metadata design and assist in maintaining the enterprise metadata model; Analyze and review current ETL/ELT jobs for performance, data latency and database resources in order to tune and redesign jobs as needed; Provide technical assistance in prioritizing and resolving operationalized ETL/ELT processes; Analyzing and developing complex ad hoc data extracts, reports to support decision makers; Ensuring data results are verified and data issues are debugged; Preparing user and technical manuals and instructions; 11 more items(s)","Vertica, Hadoop, Hive, Linux shell, Python, SQL, Microsoft SSIS, Oracle Data Integrator (ODI)"
Snowflake Data Engineer,Meta,"Meta • San Francisco, CA •  via Hiregeeks",134K–204K a year,Full-time,Join Meta as a Data Engineer in Product Analytics to optimize growth and user experience through data engineering and analytics.,Join Meta as a Data Engineer in Product Analytics to optimize growth and user experience through data engineering and analytics,"8+ years of relevant experience as a Data Engineer; Snowflake (mandatory and preferably certified) - Strong implementation experience with a good understanding of Snowflake Architecture being able to design and implement solutions and having good experience in Snowflake performance optimization techniques; Advanced SQL (mandatory and preferably certified) Good understanding of the concept of Slowly Changing Dimensions (SCD), be able to write complex queries using Self Joins, Cursors also recursive, Views/Materialized, strong in PL/SQL etc; Strong experience in SQL Performance tuning especially when dealing with large datasets in millions of data; Understanding of Data semantics and data semantic models; Python (mandatory) Good experience in Python; Expert in Singlestore procedure; Experience with Cloud Computing (AWS or Google Cloud) for deployment purposes is nice to have; 5 more items(s)","Snowflake, Advanced SQL, PL/SQL, Python, Singlestore, Cloud Computing, AWS, Google Cloud"
Senior Geospatial Data Engineer,Pro IT Inc,"Pro IT Inc • Sunnyvale, CA •  via ZipRecruiter",2 days ago,Full-time,"ng of the concept of Slowly Changing Dimensions (SCD), be able to write complex queries using Self Joins, Cursors also recursive, Views/Materialized, strong in PL/SQL etc.a. Strong experience in SQL Performance tuning especially when dealing with large datasets in millions of data.b. Understanding of Data semantics and data semantic modelsPython (mandatory) Good experience in PythonExpert in Singlestore procedure.Experience with Cloud Computing (AWS or Google Cloud) for deployment purposes is nice to have.LocationSunnyvale, CA","8+ years of relevant experience as a Data Engineer; Snowflake (mandatory and preferably certified) - Strong implementation experience with a good understanding of Snowflake Architecture being able to design and implement solutions and having good experience in Snowflake performance optimization techniques; Advanced SQL (mandatory and preferably certified) Good understanding of the concept of Slowly Changing Dimensions (SCD), be able to write complex queries using Self Joins, Cursors also recursive, Views/Materialized, strong in PL/SQL etc; Strong experience in SQL Performance tuning especially when dealing with large datasets in millions of data; Understanding of Data semantics and data semantic models; Python (mandatory) Good experience in Python; Expert in Singlestore procedure; Experience with Cloud Computing (AWS or Google Cloud) for deployment purposes is nice to have; 5 more items(s)","The ideal candidate will possess substantial professional experience in spatial data analysis, quality assurance/control, data engineering, and experience working with large-scale, scalable data solutions; Join a team that is focused on spatial data evaluation, automation, and data quality measurement, comprised of diverse skills and backgrounds, all with the same goal: helping to build the world’s best map; Far more than any specific experience or skill, we are looking for engineers who are driven to build robust and reliable software, eager to learn and hone their skills, and enthusiastic about facilitating growth and mentorship among team members; Bachelor’s degree or equivalent experience in Geography, Geomatics, Computer Science, or a related field and 8+ years of experience; Experience as a Geospatial Data Engineer or similar role, with expertise working with spatial data using Postgres/PostGIS, and familiarity with tools like Tableau for reporting; Experience with AWS/GCC services such as EKS, EMR, and familiarity with big data technologies like Spark; Experience with data serialization formats like protobuf/avro and data management tools like Iceberg; Strong understanding of geographic projections, spatial boolean fundamentals, and 2D/3D data processing; Knowledge of raster and vector data formats, DEM/TIN data, and associated spatial libraries and tools; 6 more items(s)","SQL, PL/SQL, Python, Singlestore, AWS, Google Cloud, Snowflake, Advanced SQL, Postgres/PostGIS, Tableau, EKS, EMR, Spark, protobuf, avro, Iceberg"
DELTA DENTAL: Data Engineer,Quizlet,"Quizlet • San Francisco, CA •  via LinkedIn",3 days ago,Full-time,"of high school students and half of all college students in the US. Combining cognitive science and machine learning, Quizlet guides students through adaptive study activities to confidently reach their learning goals. We’re on track to become the undisputed leader in user-generated learning content and engagement, at the scale of YouTube.Valued at $1 billion, Quizlet has raised over $60 million in venture capital from investors including Icon Ventures, Union Square Ventures, General Atlantic, Costanoa Ventures, Owl Ventures, and Altos Ventures.To serve our global community of learners, our teams tackle lofty technical challenges and design for use cases across cultures and languages. We work hard, act like owners, and collaborate every chance we get. We’re energized by the potential to power more learners through multiple approaches and various tools.About The TeamThe Data Trust and Quality group is a Data Engineering team that focuses on maintaining our analytics tools and developing solutions to our data pipeline integrity and efficiency. The team is working towards a data infrastructure that will help everyone at Quizlet understand and trust the data they use every day.About The RoleWe’re looking for a Staff Data Engineer to help us maintain the foundation of our data warehouse and create an opinionated structure to our data model that can set the standard for our embedded analysts to expand. On a typical day, you will work with analysts to understand how key metrics are derived, help upgrade our behavioral data collection infrastructure, or research tooling that will help us reach our goals of getting the right data in the right place at the right time.This is a hybrid role based out of our Denver, Los Angeles, New York, Seattle, and San Francisco office hubs.In This Role, You Will• Provide leadership on data pipelines and data infrastructure best practices• Collaborate with cross-functional teams like Product Management, Data Analytics, and Finance to design systems that fit a broad range of data use-cases• Design and build tools for our data warehouse that increase data quality and development efficiency• Identify and solve performance challenges with our jobs and systems• Stand up data infrastructure to support large data volumes• Help bring the software development lifecycle to our data by adding automation and working with dbtWhat You Bring To The Table• 8+ years of data engineering experience especially in supporting data analytics platforms and teams• Strong communication skills with people at all levels of data/technical proficiency• Demonstrated expertise with data pipelines to address complex scalability and performance concerns• Ability to create robust data standards for internal and cross-functional teams• Extensive experience in SQL and data warehouse design and implementation• Good software development skills, preferably in Python or GoBonus Points If You Have• Experience with Google Cloud Platform and dbtCompensation, Benefits & Perks• Quizlet is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. Salary transparency helps to mitigate unfair hiring practices when it comes to discrimination and pay gaps. Total compensation for this role is market competitive, including a starting base salary of $150,000 - $200,000, depending on location and experience, as well as company stock options• Collaborate with your manager and team to create a healthy work-life balance• 20 vacation days (and we expect you to take them!)• Competitive health, dental, and vision insurance (100% employee and 75% dependent PPO, HMO, VSP Choice)• Employer-sponsored 401k plan with company match• Access to LinkedIn Learning and other resources to support professional growth• Paid Family Leave, FSA, HSA, Commuter benefits, Wellness benefits• 40 hours of annual paid time off to participate in volunteer programs of choiceWe strive to make everyone feel comfortable and welcome!We work to create a holistic interview process, where both Quizlet and candidates have an opportunity to view what it would be like to work together, in exploring a mutually beneficial partnership.We provide a transparent setting, that gives a comprehensive view of who we are!In ClosingWe hope you are excited about everything you read so far. We highly encourage you to apply for this position, even if you feel you do not meet all the requirements. Quizlet is always looking for amazing folks that believe in our mission and can contribute to our team in various ways - not merely candidates that fit a certain mold.We have a bias for action, take initiative, and take pride in delivering results. We make informed decisions whenever possible but are unafraid to take calculated risks on great ideas to promote learning. We embrace challenges and see effort as the path to mastery. We’re constantly seeking opportunities to learn and we embrace curiosity. Quality matters at Quizlet, and we hold the bar high on everything we do.We treat each other with honesty and respect, encourage vigorous debate, and seek critical feedback. We value diversity, humility, transparency, and collaboration as the best paths to our success — as individuals, as a team, and as a company.Quizlet’s success as an online learning community depends on a strong commitment to diversity, equity, and inclusion. We are actively working to build a team that is representative of the diverse communities we serve, and an open, inclusive work environment where all employees can thrive. As an equal opportunity employer and a tech company committed to societal change, we welcome applicants from all backgrounds. Women, people of color, members of the LGBTQ+ community, individuals with disabilities, and veterans are strongly encouraged to apply. Come join us!To All Recruiters And Placement AgenciesAt this time Quizlet does not accept unsolicited agency resumes and/or profiles.Please do not forward unsolicited agency resumes to our website or to any Quizlet employee. Quizlet will not pay fees to any third-party agency or firm nor will it be responsible for any agency fees associated with unsolicited resumes. All unsolicited resumes received will be considered the property of Quizlet.","8+ years of data engineering experience especially in supporting data analytics platforms and teams; Strong communication skills with people at all levels of data/technical proficiency; Demonstrated expertise with data pipelines to address complex scalability and performance concerns; Ability to create robust data standards for internal and cross-functional teams; Extensive experience in SQL and data warehouse design and implementation; Good software development skills, preferably in Python or Go; Experience with Google Cloud Platform and dbt; 4 more items(s)","Compensation, Benefits & Perks; Quizlet is an equal opportunity employer; Total compensation for this role is market competitive, including a starting base salary of $150,000 - $200,000, depending on location and experience, as well as company stock options; Collaborate with your manager and team to create a healthy work-life balance; 20 vacation days (and we expect you to take them!); Competitive health, dental, and vision insurance (100% employee and 75% dependent PPO, HMO, VSP Choice); Employer-sponsored 401k plan with company match; Access to LinkedIn Learning and other resources to support professional growth; Paid Family Leave, FSA, HSA, Commuter benefits, Wellness benefits; 40 hours of annual paid time off to participate in volunteer programs of choice; 7 more items(s)","SQL, Python, Go, Google Cloud Platform, dbt"
Data Engineer III,SynergisticIT,"SynergisticIT • San Francisco, CA •  via LinkedIn",Contractor,101K–140K a year,"lease check the below links to see the success outcomes and salaries of our candidates .https://www.synergisticit.com/candidate-outcomes/https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalogWe regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023https://synergisticit.wistia.com/medias/tmwjwchxz5https://synergisticit.wistia.com/medias/n8487768dihttps://synergisticit.wistia.com/medias/o5gmv7i9euhttps://synergisticit.wistia.com/medias/k6t6a1n4kbhttps://synergisticit.wistia.com/medias/pgrvq4fgnihttps://synergisticit.wistia.com/medias/ce4syhm853All Positions are open for all visas and US citizensWe at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.We assist in filing for STEM extension and also for H1b and Green card filing to CandidatesWe want Data Science/Machine learning/Data Analyst and Java Full stack candidatesFor data Science/Machine learning PositionsRequired SkillsBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITProject work on the technologies neededHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleKnowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization toolsExcellent written and verbal communication skillsPreferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, TensorflowREQUIRED SKILLS For Java /Full Stack/Software PositionsBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleProject work on the skillsKnowledge of Core Java, Javascript, C++, or software programmingSpring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experienceExcellent written and verbal communication skillsIf you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements.No phone calls, please. Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry; For data Science/Machine learning Positions; Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT; Project work on the technologies needed; Highly motivated, self-learner, and technically inquisitive; Experience in programming language Java and understanding of the software development life cycle; Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools; Excellent written and verbal communication skills; REQUIRED SKILLS For Java /Full Stack/Software Positions; Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT; Highly motivated, self-learner, and technically inquisitive; Experience in programming language Java and understanding of the software development life cycle; Project work on the skills; Knowledge of Core Java, Javascript, C++, or software programming; Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience; Excellent written and verbal communication skills; 13 more items(s)","In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries","Java, Python, Tableau, PowerBI, Databricks, Tensorflow, NLP, Text mining, Core Java, Javascript, C++, Spring Boot, Microservices, Docker, Jenkins, Github, Kubernetes, REST APIs"
Big Data Engineer (Contract W2 Hybrid onsite),Dice,"Dice • San Francisco, CA •  via LinkedIn",2 hours ago,Contractor,"tgres etc. Skill level expected: good

Big Data Engineer (Contract W2 Hybrid onsite)","Core skill needed is Apache Spark and a programming language (preferably Scala with Java) or else Python; Skill level expected: excellent; Secondary skill needed is some experience with Distributed computing, Big Data, Oozie, Postgres etc; Skill level expected: good; 1 more items(s)","Expert level of understanding of Google Cloud Data & associated services; Hands on programming expertise in SQL, Python & Spark; Design and implementation of end-to-end cloud based data platforms; Expert in data warehousing and analytics concepts, architecture and patterns; Good understanding of data modeling techniques and principles; Working at GSPANN; 3 more items(s)","Apache Spark, Scala, Java, Python, Distributed computing, Big Data, Oozie, Postgres, Google Cloud Data, SQL, GSPANN"
GCP Data Engineer/Architect,"GSPANN Technologies, Inc","GSPANN Technologies, Inc • San Francisco, CA •  via LinkedIn",20 hours ago,Contractor,"ique consultancy with the capabilities of a large IT services firm.

GCP Data Architect / Engineer

Location-San Francisco, CA (East Bay Area or Bay Area)

Job Type-Long Term Contract

Work type: 3 days on-site

Skills required:

• Expert level of understanding of Google Cloud Data & associated services

• Hands on programming expertise in SQL, Python & Spark

• Design and implementation of end-to-end cloud based data platforms

• Expert in data warehousing and analytics concepts, architecture and patterns

• Good understanding of data modeling techniques and principles

Working at GSPANN

GSPANN is a diverse, prosperous, and rewarding place to work. We provide competitive benefits, educational assistance, and career growth opportunities to our employees. Every employee is valued for their talent and contribution. Working with us will give you an opportunity to work globally with some of the best brands in the industry.

The company does and will take affirmative action to employ and advance in the employment of individuals with disabilities and protected veterans and to treat qualified individuals without discrimination based on their physical or mental disability status. GSPANN is an equal opportunity employer for minorities/females/veterans/disabled.","Expert level of understanding of Google Cloud Data & associated services; Hands on programming expertise in SQL, Python & Spark; Design and implementation of end-to-end cloud based data platforms; Expert in data warehousing and analytics concepts, architecture and patterns; Good understanding of data modeling techniques and principles; Working at GSPANN; 3 more items(s)","We provide competitive benefits, educational assistance, and career growth opportunities to our employees","Google Cloud Data, SQL, Python, Spark"
Senior Data Engineer 🏆,Visa,"Visa • Foster City, CA •  via Smart Recruiters Jobs",Full-time,Dental insurance,"businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.Make an impact with a purpose-driven industry leader. Join us today and experience #LifeatVisa.Job DescriptionVisa’s Global Sales and Commercial Operations team supports the execution and delivery of key global initiatives and planning activities across the company and reports directly to the CEO. Our mandate is to develop, design, and execute the strategic direction for Visa’s business operations and global sales organization, be the super connector for the markets, regions, global groups, and bring a consistent voice and ‘One Visa’ mindset. The team has the following key focus areas: Business Performance, Sales Excellence, Sales Enablement and Sales Tools.This role will report into the Business Performance organization which includes Scorecard, Sales compensation strategies and programs (MIP and SIP), Target setting, the TRUTH business performance dashboards, and Client Insights program (survey, touchpoints). The Data Engineering Manager on the team is a specialist who makes data available from new sources, builds robust data models, creates, and optimizes data enrichment pipelines, and provides engineering support to specific projects. You will partner with our Data Visualizers to ensure data needed by the business is available and accurate and to develop certified data sets. This is a technical role that acts as a force multiplier to our Visualizers, Analysts, and other data users across the team.Responsibilities• Establish data processes and automations, based upon business and technology requirements, leveraging Visa’s supported data platforms and tools• Deliver small to large data engineering projects either individually or as part of a project team• Develop and maintain data models and schema designs for efficient data storage and retrieval, with a strong understanding of best practices in data modeling and data architecture• Design and implement data pipelines to extract, transform, and load data from various sources into the data warehouse, with a strong focus on reusability, performance, scalability and cost efficiency• Collaborate with cross-functional teams to understand data requirements and ensure data quality, with a focus on implementing data validation and data quality checks at various stages of the pipeline• Provide expertise in data warehousing, ETL, and data modeling to support data-driven decision making, with a strong understanding of best practices in data pipeline design and performance optimization• Extract and manipulate large datasets using standard tools such as Hadoop (Hive), SQL, Spark, Python (pandas, NumPy) and Presto• Provide ongoing production support for monthly Scorecard publishing and TRUTH dashboard updates• Communicate complex concepts in a clear and effective manner• Stay up-to-date with the latest data engineering trends and technologies to ensure the company's data infrastructure is always state-of-the-art and following best practicesThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.QualificationsBasic Qualifications:• 5 or more years of relevant work experience with a Bachelors Degree or at least 2 years of work experience with an Advanced degree (e.g. Masters, MBA, JD, MD) or 0 years of work experience with a PhDPreferred Qualifications:• 6+ years of analytics experience with a focus on data engineering• Strong experience with SQL, Python, and relational databases• Advanced knowledge of SQL (e.g., understands subqueries, self-joining tables, stored procedures, can read an execution plan, sql tuning, etc.)• Solid understanding of best practices in data warehousing, ETL, data modeling, and data architecture.• Strong understanding of best practices in data governance, data validation and data quality checks• Experience in Python, Spark, and exposure to scheduling tools like Tuber/Airflow is preferred.• Able to create data dictionaries, setup and monitor data validation alerts, and execute periodic jobs to maintain data pipelines for completed projects• Experience with visualization software (e.g., Tableau, PowerBI) is a plus.• Strong problem-solving and analytical skills• Effective communicator of statuses, issues, and risks• Extremely high sense of quality standards and attention to detail• A team player and collaborator, able to work well with a diverse group of individuals in a matrixed environmentAdditional InformationWork Hours: Varies upon the needs of the department.Travel Requirements: This position requires travel 5-10% of the time.Mental/Physical Requirements: This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers.Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.Visa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law, including the requirements of Article 49 of the San Francisco Police Code.U.S. APPLICANTS ONLY: The estimated salary range for a new hire into this position is 119100 to 191800 USD per year, which may include potential sales incentive payments (if applicable). Salary may vary depending on job-related factors which may include knowledge, skills, experience, and location. In addition, this position may be eligible for bonus and equity. Visa has a comprehensive benefits package for which this position may be eligible that includes Medical, Dental, Vision, 401 (k), FSA/HSA, Life Insurance, Paid Time Off, and Wellness Program.","5 or more years of relevant work experience with a Bachelors Degree or at least 2 years of work experience with an Advanced degree (e.g. Masters, MBA, JD, MD) or 0 years of work experience with a PhD; The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers","Salary may vary depending on job-related factors which may include knowledge, skills, experience, and location; In addition, this position may be eligible for bonus and equity; Visa has a comprehensive benefits package for which this position may be eligible that includes Medical, Dental, Vision, 401 (k), FSA/HSA, Life Insurance, Paid Time Off, and Wellness Program","Hadoop (Hive), SQL, Spark, Python (pandas, NumPy), Presto, Tableau, PowerBI, Tuber/Airflow"
Data Engineer (Offshore),University of California Office of the President,"University of California Office of the President • Oakland, CA •  via DevITjobs",18 days ago,143K–160K a year,"ated area and/or equivalent experience/training.- Demonstrated experience with SQL, Stored Procedures, Functions for OLTP & OLAP systems (Microsoft SQL Server, DB2, or related technology), SQL Server management studio, and Visual Studio.- Proficiency in Data Integrations & ETL development (SSIS, DataStage or related technology) and Report development (SSRS or related technology).- Strong understanding of data integration and ETL processes, with advanced SQL coding experience.- Ability to identify and write performance tuning scripts and tools for efficient data extraction and processing.- Advanced experience with Data Analysis and Development, understanding complex data models and relationships.- Strong database knowledge with a keen understanding of database structures, schemas, objects, relationships, and constraints.- Ability to troubleshoot encryption key pairing and identify/document requirements/specifications in a data environment.- Strong verbal and written communication skills, capable of conveying complex technical concepts in business terms.- Ability to work independently with cross-functional teams and prioritize multiple work streams.- Preferred qualifications include experience with source control software, Agile development methodologies, and knowledge of best practices in enterprise-wide data architectures.👩‍💻👨‍💻 Your responsibilities are:- Perform complex data analysis to determine root causes and address business issues, supporting data integration processes and existing operations.- Develop and implement code to streamline data processing and remediate data issues; assist in coding, testing, and implementing ETL processes.- Create technical documentation from business requirements for system specifications, including design documents for data-related enhancements.- Provide operational support to troubleshoot application errors and ensure stability; participate in the on-call support team.- Lead initiatives to drive strategies and administer policies while collaborating with various teams for data-related solutions.- Mentor less experienced coworkers and assist in fostering an inclusive work environment.View this job and over 500 other transparent jobs with salaries (💰💰💰) & tech stacks (🛠️) on DevITjobsCategory: Data Developer / EngineerLocation address: 16th Street 1385, Oakland, United StatesSalary: 143,000 - 160,000 USD per yearBenefits & perks that we offer:University of California Office of the President - More about us and the role:At the University of California, we are committed to excellence in teaching, research, and public service. As a Senior Data Engineer within the Technology Delivery Services (TDS) team of the Information Technology Department, you will be a key player in our data ecosystem. We pride ourselves on maintaining an innovative and supportive culture that encourages collaboration and professional growth. This position requires flexible work arrangements, and occasional travel may be needed. Our competitive salary range is between $143,000 - $160,000, and we offer a comprehensive benefits package. We are an Equal Opportunity/Affirmative Action Employer and aim to create a diverse workplace that inspires all individuals to thrive.Are you looking for Data jobs in Oakland?","Minimum 8 years of relevant work experience in Data development or a related field; Bachelor's degree in a related area and/or equivalent experience/training; Demonstrated experience with SQL, Stored Procedures, Functions for OLTP & OLAP systems (Microsoft SQL Server, DB2, or related technology), SQL Server management studio, and Visual Studio; Proficiency in Data Integrations & ETL development (SSIS, DataStage or related technology) and Report development (SSRS or related technology); Strong understanding of data integration and ETL processes, with advanced SQL coding experience; Ability to identify and write performance tuning scripts and tools for efficient data extraction and processing; Advanced experience with Data Analysis and Development, understanding complex data models and relationships; Strong database knowledge with a keen understanding of database structures, schemas, objects, relationships, and constraints; Ability to troubleshoot encryption key pairing and identify/document requirements/specifications in a data environment; Strong verbal and written communication skills, capable of conveying complex technical concepts in business terms; Ability to work independently with cross-functional teams and prioritize multiple work streams; 8 more items(s)","Salary: 143,000 - 160,000 USD per year; Our competitive salary range is between $143,000 - $160,000, and we offer a comprehensive benefits package","SQL, Stored Procedures, Functions, Microsoft SQL Server, DB2, SQL Server Management Studio, Visual Studio, SSIS, DataStage, SSRS, source control software, Agile development methodologies."
Bioinformatics Data Engineer,Zoox,"Zoox • Foster City, CA •  via Lever",Full-time,Paid time off,"pipeline to enable Zoox to develop and scale with a data-first culture.You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team.In this role, you will:• Design, build, and maintain the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company• Define and execute on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users• Establish robust data integrity monitoring so that company-wide metrics are based on accurate data• Partner with engineering and product teams to define data consumption patterns and establish best practicesQualifications• BS/MS degree in a technical field• Experience designing and building complex data infrastructure at scale• Advanced Structure Query Language (SQL) and data warehousing experience• Experience operating a workflow manager such as Airflow• Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)Bonus Qualifications• Exceptional Python or Scala skills• Basic fluency in C++• Familiarity with or exposure to experimentation platforms• A strong DataOps mindset and opinions on next-generation warehousing toolsCompensationThere are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $245,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.ABOUT ZOOXZoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.Follow us on LinkedInAbout ZooxZoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.Follow us on LinkedInAccommodationsIf you need an accommodation to participate in the application or interview process please reach out to accommodations@zoox.com or your assigned recruiter.A Final Note:You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.","BS/MS degree in a technical field; Experience designing and building complex data infrastructure at scale; Advanced Structure Query Language (SQL) and data warehousing experience; Experience operating a workflow manager such as Airflow; Experience with large scale streaming platforms (e.g; Kafka, Kinesis), processing frameworks (e.g; Spark, Hadoop) and storage engines (e.g. HDFS, HBase); Exceptional Python or Scala skills; Basic fluency in C++; Familiarity with or exposure to experimentation platforms; A strong DataOps mindset and opinions on next-generation warehousing tools; 8 more items(s)","Compensation; There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights; The salary will range from $143,000 to $245,000; A sign-on bonus may be part of a compensation package; Compensation will vary based on geographic location, job-related knowledge, skills, and experience; Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance; 3 more items(s)","SQL, Airflow, Kafka, Kinesis, Spark, Hadoop, HDFS, HBase, Python, Scala, C++"
Staff Data Platform Engineer,General Motors,"General Motors • Mountain View, CA •  via General Motors Careers",7 days ago,Full-time,"in us. Our mission? Driving forward all digital solutions for commercial fleet owners and drivers, catering to businesses of all sizes, from small and medium businesses to large enterprises.In today's dynamic landscape, the demand for intelligent, safe, and eco-friendly transportation solutions is skyrocketing. GM is at the forefront of this transformative journey, championing connected software-defined vehicles that redefine the driving experience. At the heart of this paradigm shift lies software – enabling seamless communication, bolstering security, facilitating updates, processing data, and ultimately, delivering an unparalleled user experience. These capabilities extend beyond consumer benefits, holding even greater significance for business owners.Recognizing this pivotal moment, GM has recruited a significant number of Silicon Valley engineering, product, and leaders to lead the GM Software & Services organization. As a leading OEM with a vast fleet of vehicles spanning the globe, we're uniquely positioned to leverage our comprehensive control over in-vehicle and cloud software. Our aim? Seamlessly integrating solutions such as fleet management, energy optimization, transportation logistics, safety systems, and more, all geared towards enhancing efficiency and sustainability.To realize our vision, we're actively expanding our software teams in Silicon Valley. We seek individuals who can blend a passion for technology and sustainability with curiosity, rigorous thinking and a strong customer centric approach. This is an exceptional opportunity where you can leverage the scale of GM and make a significant impact and find fulfillment in crafting innovative software solutions. We invite you to join us on this exciting journey toward a better future.Role Summary:As a Senior Data Engineer, you will be responsible for designing, developing, and maintaining efficient data workflows and associated cloud infrastructure that support GM Commercial Software’s core analytic products and services. A key focus of this role will be on developing and optimizing streaming data pipelines, ideally utilizing Spark or similar technologies. You will work closely with our data scientists, DevOps, and software engineers to automate pipelines that process streaming vehicle telemetry data, ensuring real-time data processing and transformation. Additionally, you will transform data within our data lakehouse into deployable data models that power our automated fleet insights, visualizations, and emerging machine learning, optimization, and AI applications. The ideal candidate will have extensive experience in setting up ELT pipelines to handle massive volumes of spatiotemporal data, designing enterprise data warehouse models, implementing robust data quality tests, and leveraging data pipeline-as-code integration systems. Your expertise in streaming data technologies will be crucial for ensuring the smooth and efficient orchestration of the workflows that power our advanced analytics platform.Responsibilities:• Design, implement, and maintain streaming data pipelines using spark or analogous technologies.• Design, implement, and maintain ELT pipelines in our cloud-native data lakehouse platform.• Work with data scientists to turn exploratory analyses into production data transformation workflows within our multihop data lakehouse architecture.• Support enterprise-wide use of BI tools and assist in developing internal and external analytics products (e.g., dashboards).• Generate and deploy feature engineering pipelines to power internal and customer-facing machine learning-based products.• Maintain data quality testing and monitoring tools.• Work with information security, DevOps, and DataOps to maintain data classification, auditing, and access and cost control policies.QualificationsRequired:• Bachelor's degree (Master’s preferred) or equivalent experience in computer science, data science, engineering, or related quantitative field• 5+ years of industry experience developing, implementing, and maintaining solutions for Big Data or data warehousing systems• 3+ years of industry experience working in a cloud environment (Azure preferred)• 3+ years experience working with SQL query authoring for automated data transformation (familiarity with dbt preferred, but not required).• 2+ years of experience developing streaming data processing pipelines (use of Spark/pySpark preferred)• Basic understanding of machine learning/statistical learning principles• Experience implementing and maintaining data workflow orchestration and integration tools (e.g. Airflow/Astro, Prefect, dbt cloud, etc.)• Understanding of and experience with application of data quality tools integrated with CI/CD automation frameworks in functional deployment environments (e.g., Github Actions/Azure DevOps pipelines).• Familiarity with data quality testing frameworks (Great Expectations, Deequ)• Self-driven with an interest in on-the-job learning.Preferred:• Familiarity with enterprise warehouse data modeling techniques (e.g., Kimball)• Experience integrating simulation systems with distributed, data-intensive processing or analytics applicationsDesired:• Working familiarity with terraform• Domain knowledge in automotive systems• Engagement with modern data stack community (open source and commercial)• High degree of attention to software craftsmanship and professionalism• Experience working with containerization technologies and orchestration platforms (specifically Docker and Kubernetes)Compensation: The compensation information is a good faith estimate only. It is based on what a successful applicant might be paid in accordance with applicable state laws. The compensation may not be representative for positions located outside of New York, Colorado, California, or Washington.• The salary range for this role is $152,100.00 - 232,00.00. The actual base salary a successful candidate will be offered within this range will vary based on factors relevant to the position.Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance.Benefits:• Benefits: GM offers a variety of health and wellbeing benefit programs. Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more.This job may be eligible for relocation benefits.","5+ years of industry experience developing, implementing, and maintaining solutions for Big Data or data warehousing systems; Basic understanding of machine learning/statistical learning principles; Experience implementing and maintaining data workflow orchestration and integration tools (e.g; Airflow/Astro, Prefect, dbt cloud, etc.); Understanding of and experience with application of data quality tools integrated with CI/CD automation frameworks in functional deployment environments (e.g., Github Actions/Azure DevOps pipelines); Familiarity with data quality testing frameworks (Great Expectations, Deequ); Self-driven with an interest in on-the-job learning; 4 more items(s)","Compensation: The compensation information is a good faith estimate only; The salary range for this role is $152,100.00 - 232,00.00; The actual base salary a successful candidate will be offered within this range will vary based on factors relevant to the position; Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance; Benefits: GM offers a variety of health and wellbeing benefit programs; Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more; This job may be eligible for relocation benefits; 4 more items(s)","Spark, Azure, SQL, dbt, Airflow, Astro, Prefect, dbt cloud, Github Actions, Azure DevOps pipelines, Great Expectations, Deequ, Terraform, Docker, Kubernetes"
Senior Geospatial Data Engineer,"Scribd, Inc.","Scribd, Inc. • San Francisco, CA •  via Hiregeeks",158K–268K a year,Full-time,Join Scribd as a Data Architect/Principal Data Engineer to lead data architecture and strategy. Remote role with competitive salary and benefits.,Remote role with competitive salary and benefits,"The ideal candidate will possess substantial professional experience in spatial data analysis, quality assurance/control, data engineering, and experience working with large-scale, scalable data solutions; Join a team that is focused on spatial data evaluation, automation, and data quality measurement, comprised of diverse skills and backgrounds, all with the same goal: helping to build the world’s best map; Far more than any specific experience or skill, we are looking for engineers who are driven to build robust and reliable software, eager to learn and hone their skills, and enthusiastic about facilitating growth and mentorship among team members; Bachelor’s degree or equivalent experience in Geography, Geomatics, Computer Science, or a related field and 8+ years of experience; Experience as a Geospatial Data Engineer or similar role, with expertise working with spatial data using Postgres/PostGIS, and familiarity with tools like Tableau for reporting; Experience with AWS/GCC services such as EKS, EMR, and familiarity with big data technologies like Spark; Experience with data serialization formats like protobuf/avro and data management tools like Iceberg; Strong understanding of geographic projections, spatial boolean fundamentals, and 2D/3D data processing; Knowledge of raster and vector data formats, DEM/TIN data, and associated spatial libraries and tools; 6 more items(s)","Postgres, PostGIS, Tableau, AWS, GCC, EKS, EMR, Spark, protobuf, avro, Iceberg"
"Sr. Big Data Engineer in San Francisco, CA / McLean, VA",GoodRx,"GoodRx • San Francisco, CA •  via LinkedIn",12 days ago,Full-time,"e nearly $75 billion on the cost of their prescriptions.Our goal is to help Americans find convenient and affordable healthcare. We offer solutions for consumers, employers, health plans, and anyone else who shares our desire to provide affordable prescriptions to all Americans.About The RoleGoodRx is looking for extremely smart and curious data engineers, who are deft at working with a wide variety of languages, a variety of raw data formats, such as parquet, in a fast-paced and friendly environment. You will collaborate and work with teams across GoodRx to build outstanding data pipelines and processes that stitch together complex sets of data stores in order to guide business decisions.Responsibilities• Collaborate with product managers, data scientists, data analysts and engineers to define requirements and data specifications.• Plan, design, build, test and deploy data warehouse and data mart solutions.• Leading small to medium size projects, solve data problems through the documentation, design and creation of ETL jobs, data marts.• Works to increase the usage and value of the data warehouse and ensures the integrity of the data delivered.• Develops and implements standards, and promotes their use throughout the warehouse• Develop, deploy and maintain data processing pipelines using cloud technology such as AWS, Kubernetes, Airflow, Redshift, Databricks, EMR.• Define and manage overall schedule and availability for a variety of data sets.• Work closely with other engineers to enhance infrastructure, improve reliability and efficiency.• Make smart engineering and product decisions based on data analysis and collaboration.• Act as an in house data expert and make recommendations regarding standards for code quality and timeliness.• Architect cloud-based data pipeline solutions to meet stakeholder needs.Skills & Qualifications• Bachelor’s degree in analytics, engineering, math, computer science, information technology or related discipline.• 8+ years professional experience in the big data space.• 8+ years' experience in engineering data pipelines using big data technologies (Spark, Flink etc...) on large scale data sets.• Expert knowledge in writing complex pySpark, SQL, dbt and ETL development with experience processing extremely large datasets.• Expert in applying SCD types on S3 data lake using Databricks/Delta Lake.• Experience with data model principles and data cataloging.• Experience with job scheduler Airflow or similar.• Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions.• Deep familiarity with AWS Services (S3, Event Bridge, Kinesis, Glue, EMR, Lambda).• Experience with data warehouse platforms such as Redshift, Databricks, Big Query, Snowflake.• Ability to quickly learn complex domains and new technologies.• Innately curious and organized with the drive to analyze data to identify deliverables, anomalies and gaps and propose solutions to address these findings.• Thrives in a fast-paced startup environment.Good To Have• Experience with customer data platform tools such as Segment.• Experience with data streaming such as Kafka.• Experience using Jira, GitHub, Docker, CodeFresh, Terraform.• Experience contributing to full lifecycle deployments with a focus on testing and quality.• Experience with data quality processes, data quality checks, validations, data quality metrics definition and measurement.• AWS/Kafka/Databricks or similar certifications.Engineering teams are responsible for supporting appropriate security controls, including management, operational, and technical controls in addition to general GoodRx best practices, such as reading and adhering to the security policies and procedures, being vigilant and observant of potential security threats, etc.At GoodRx, pay ranges are determined based on work locations and may vary based on where the successful candidate is hired. The pay ranges below are shown as a guideline, and the successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, and other relevant business and organizational factors. These pay zones may be modified in the future. Please contact your recruiter for additional information.San Francisco And Seattle Offices$161,000.00 - $257,000.00New York Office$147,000.00 - $235,000.00Santa Monica Office$134,000.00 - $214,000.00Other Office Locations:$121,000.00 - $193,000.00GoodRx also offers additional compensation programs such as annual cash bonuses and annual equity grants for most positions as well as generous benefits. Our great benefits offerings include medical, dental, and vision insurance, 401(k) with a company match, an ESPP, unlimited vacation, 13 paid holidays, and 72 hours of sick leave. GoodRx also offers additional benefits like mental wellness and financial wellness programs, fertility benefits, generous parental leave, pet insurance, supplemental life insurance for you and your dependents, company-paid short-term and long-term disability, and more!We’re committed to growing and empowering a more inclusive community within our company and industry. That’s why we hire and cultivate diverse teams of the best and brightest from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has a seat at the table and the tools, resources, and opportunities to excel.With that said, research shows that women and other underrepresented groups apply only if they meet 100% of the criteria. GoodRx is committed to leveling the playing field, and we encourage women, people of color, those in the LGBTQ+ communities, individuals with disabilities, and Veterans to apply for positions even if they don’t necessarily check every box outlined in the job description. Please still get in touch - we’d love to connect and see if you could be good for the role!GoodRx is committed to providing reasonable accommodations for candidates with disabilities during our recruiting process. If you need any assistance or accommodations due to a disability, please reach out to us at accommodations@goodrx.com.GoodRx is America's healthcare marketplace. The company offers the most comprehensive and accurate resource for affordable prescription medications in the U.S., gathering pricing information from thousands of pharmacies coast to coast, as well as a tele-health marketplace for online doctor visits and lab tests. Since 2011, Americans with and without health insurance have saved $60 billion using GoodRx and million consumers visit goodrx.com each month to find discounts and information related to their healthcare. GoodRx is the #1 most downloaded medical app on the iOS and Android app stores. For more information, visit www.goodrx.com.","GoodRx is looking for extremely smart and curious data engineers, who are deft at working with a wide variety of languages, a variety of raw data formats, such as parquet, in a fast-paced and friendly environment; Bachelor’s degree in analytics, engineering, math, computer science, information technology or related discipline; 8+ years professional experience in the big data space; 8+ years' experience in engineering data pipelines using big data technologies (Spark, Flink etc...); on large scale data sets; Expert knowledge in writing complex pySpark, SQL, dbt and ETL development with experience processing extremely large datasets; Expert in applying SCD types on S3 data lake using Databricks/Delta Lake; Experience with data model principles and data cataloging; Experience with job scheduler Airflow or similar; Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions; Deep familiarity with AWS Services (S3, Event Bridge, Kinesis, Glue, EMR, Lambda); Experience with data warehouse platforms such as Redshift, Databricks, Big Query, Snowflake; Ability to quickly learn complex domains and new technologies; Experience with customer data platform tools such as Segment; Experience with data streaming such as Kafka; Experience using Jira, GitHub, Docker, CodeFresh, Terraform; Experience contributing to full lifecycle deployments with a focus on testing and quality; Experience with data quality processes, data quality checks, validations, data quality metrics definition and measurement; AWS/Kafka/Databricks or similar certifications; 16 more items(s)","The pay ranges below are shown as a guideline, and the successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, and other relevant business and organizational factors; $147,000.00 - $235,000.00; $134,000.00 - $214,000.00; $121,000.00 - $193,000.00; GoodRx also offers additional compensation programs such as annual cash bonuses and annual equity grants for most positions as well as generous benefits; Our great benefits offerings include medical, dental, and vision insurance, 401(k) with a company match, an ESPP, unlimited vacation, 13 paid holidays, and 72 hours of sick leave; GoodRx also offers additional benefits like mental wellness and financial wellness programs, fertility benefits, generous parental leave, pet insurance, supplemental life insurance for you and your dependents, company-paid short-term and long-term disability, and more!; 4 more items(s)","AWS, Kubernetes, Airflow, Redshift, Databricks, EMR, Spark, Flink, pySpark, SQL, dbt, S3, Delta Lake, Segment, Kafka, Jira, GitHub, Docker, CodeFresh, Terraform, Big Query, Snowflake"
Snowflake Data Engineer,Cars Commerce,"Cars Commerce • San Francisco, CA •  via Taro",101K–140K a year,Full-time,"Cars Commerce seeks a Data Engineer III to build scalable data platforms and tools, enabling innovations with advanced analytics and machine learning.","Cars Commerce seeks a Data Engineer III to build scalable data platforms and tools, enabling innovations with advanced analytics and machine learning","8+ years of relevant experience as a Data Engineer; Snowflake (mandatory and preferably certified) - Strong implementation experience with a good understanding of Snowflake Architecture being able to design and implement solutions and having good experience in Snowflake performance optimization techniques; Advanced SQL (mandatory and preferably certified) Good understanding of the concept of Slowly Changing Dimensions (SCD), be able to write complex queries using Self Joins, Cursors also recursive, Views/Materialized, strong in PL/SQL etc; Strong experience in SQL Performance tuning especially when dealing with large datasets in millions of data; Understanding of Data semantics and data semantic models; Python (mandatory) Good experience in Python; Expert in Singlestore procedure; Experience with Cloud Computing (AWS or Google Cloud) for deployment purposes is nice to have; 5 more items(s)","Snowflake, SQL, PL/SQL, Python, Singlestore, AWS, Google Cloud"
Software Engineer II - Data Platform,InfraStaff,"InfraStaff • San Francisco, CA •  via Jobted",5 days ago,Full-time,"Required Skills & Experience• Programming in Python• NumPy• Airflow• Big Data experience, such as Hadoop, Hive, Spark• Experience working with large pools of data, especially Finance Data• AWS","Data Engineer with advanced Python skills and NumPy; Someone who can use Python scripts to pull data from Hive tables and has the ability to do analysis; Required Skills & Experience; Programming in Python; NumPy; Airflow; Big Data experience, such as Hadoop, Hive, Spark; Experience working with large pools of data, especially Finance Data; AWS; 6 more items(s)","The consultant will Develop and maintain custom, complex ETL pipelines written in Python, some Airflow, Spark, Hive; Migrate legacy data pipelines (Oracle Server, some RedShift) to newer technologies Migrate data pipelines from a legacy systems","Python, NumPy, Airflow, Hadoop, Hive, Spark, AWS, Oracle Server, RedShift"
"Data Engineer, Ads",Cars Commerce,"Cars Commerce • San Francisco, CA •  via Taro",131K–164K a year,Full-time,Sr. Data Engineer I at Cars Commerce: Build scalable data platforms and ML solutions for automotive retail innovation.,Data Engineer I at Cars Commerce: Build scalable data platforms and ML solutions for automotive retail innovation,"You will be a key member of our team, developing and executing data-driven strategies for our Ads products.To be successful in this role, you should have extensive experience designing and developing Ads-related data pipelines; You should have a strong understanding of data engineering techniques and practices, as well as a passion for data exploration; Additionally, you should have a strong desire to stay up-to-date on the latest Ads technologies and trends",Not applicable
Data Scientist/Data Engineer (1042),Unity,"Unity • San Francisco, CA •  via JobzMall",90K–150K a year,Full-time,"developing Ads-related data pipelines. You should have a strong understanding of data engineering techniques and practices, as well as a passion for data exploration. Additionally, you should have a strong desire to stay up-to-date on the latest Ads technologies and trends. If you have the skills and qualifications we are looking for, we would love to hear from you!Responsibilities:Design and develop data pipelines related to Ads products.Utilize data engineering techniques and practices to ensure optimal Ads performance.Explore and analyze data to identify trends in Ads performance.Identify and address any issues in the Ads data pipeline.Stay up-to-date on the latest Ads technologies and trends.Collaborate with other engineers and Ads team members to ensure project success.Develop and execute data-driven strategies for Ads products.Monitor Ads performance and provide recommendations for improvement.Troubleshoot Ads-related data issues and provide resolutions.Document and share best practices for Ads product development.Unity is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.","You will be a key member of our team, developing and executing data-driven strategies for our Ads products.To be successful in this role, you should have extensive experience designing and developing Ads-related data pipelines; You should have a strong understanding of data engineering techniques and practices, as well as a passion for data exploration; Additionally, you should have a strong desire to stay up-to-date on the latest Ads technologies and trends",Design and develop data pipelines related to Ads products; Utilize data engineering techniques and practices to ensure optimal Ads performance; Explore and analyze data to identify trends in Ads performance; Identify and address any issues in the Ads data pipeline; Stay up-to-date on the latest Ads technologies and trends; Collaborate with other engineers and Ads team members to ensure project success; Develop and execute data-driven strategies for Ads products; Monitor Ads performance and provide recommendations for improvement; Troubleshoot Ads-related data issues and provide resolutions; Document and share best practices for Ads product development; 7 more items(s),Unity
Principal Data Software Engineer,Meta,"Meta • San Francisco, CA •  via Indeed",134K–204K a year,Full-time,"ith some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few companies can match.Data Engineer, Product Analytics Responsibilities:• Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems• Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve• Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way• Define and manage SLA for all data sets in allocated areas of ownership• Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership• Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains• Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources• Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts• Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts• Influence product and cross-functional teams to identify data opportunities to drive impact• Mentor team members by giving/receiving actionable feedbackMinimum Qualifications:• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.• 4+ years of work experience in data engineering (a minimum of 2+ years with a Ph.D)• Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)Preferred Qualifications:• Master's or Ph.D degree in a STEM field• Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy• Experience working with terabyte to petabyte scale dataAbout Meta:Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today—beyond the constraints of screens, the limits of distance, and even the rules of physics.Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.$134,000/year to $204,000/year + bonus + equity + benefitsIndividual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about benefits at Meta.","Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience; 4+ years of work experience in data engineering (a minimum of 2+ years with a Ph.D); Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.); Meta participates in the E-Verify program in certain locations, as required by law; 1 more items(s)","$134,000/year to $204,000/year + bonus + equity + benefits; Individual compensation is determined by skills, qualifications, experience, and location; Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable; In addition to base compensation, Meta offers benefits; 1 more items(s)","SQL, ETL, Python, C++, C#, Scala"
Data Engineer (Offshore),Capgemini,"Capgemini • San Francisco, CA •  via Adzuna",Full-time,Full-time,"ining.Required Skills and Experience:You focus on building solutions and on maintaining, optimizing and improving a client’s applications and systems. You contribute to a business and technical blueprint and customize the respective Software Package Core Module. You may also be responsible for unit testing, contribute to integration testing, and/or be responsible for the design and delivery of end-user training.• Qualification\: 3-7 years (2 years min relevant experience in the role), Bachelor’s Degree.• Certification\: Should have or seeking SE Level 1.• Should be proficient in Package Configuration.• Should have progressing skills in Business Analysis, Business Knowledge, Testing, Architecture Knowledge, Technical Solution Design and Vendor ManagementCapgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.Click the following link for more information on your rights as an Applicant - http\://www.capgemini.com/resources/equal-employment-opportunity-is-the-lawAbout CapgeminiCapgemini is a global leader in consulting, digital transformation, technology and engineering services. The Group is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50-year+ heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. Today, it is a multicultural company of 270,000 team members in almost 50 countries. With Altran, the Group reported 2019 combined revenues of €17billion.Visit us at www.capgemini.com. People matter, results count.Strong data engineering experience using Java or Python programming languages or Spark on Google Cloud. Should have worked on handling big data. Strong communication skills , experience in Agile methodologies , ETL / ELT skills , Data movement skills , Data processing skills.• Experience in using Google Cloud Storge (GCS), Apache Beam (Dataflow), BigQuery, BigTable, Cloud Spanner, Dataproc or any other Google Cloud Analytical Services• Hands-on experience on native services like BigQuery, DataProc, DataFlow, DataPrep, Pub/Sub and DataLab. Cloud Storage, Cloud Data Fusion, Cloud Functions, Cloud Composer• Experience with open source distributed storage and processing utilities in the Apache Hadoop family and/or workflow orchestration products such as Apache Airflow.• Experience with machine learning technologies such as Tensorflow, and experience in Google AI Platform particularly developing and/or training models or implementing solutions which rely on invoking such models.• API Development\: Minimum of 3 years of application design and development experience in a cloud environment, including 2+ years of coding REST services and API such as\: Java, JSON, Python.• Minimum 1 year of Google Cloud Platform’s Apigee EDGE platform and API development experience through the entire Software Development Lifecycle Process.Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment.Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.","Qualification\: 3-7 years (2 years min relevant experience in the role), Bachelor’s Degree; Certification\: Should have or seeking SE Level 1; Should be proficient in Package Configuration; Should have progressing skills in Business Analysis, Business Knowledge, Testing, Architecture Knowledge, Technical Solution Design and Vendor Management; Strong data engineering experience using Java or Python programming languages or Spark on Google Cloud; Should have worked on handling big data; Strong communication skills , experience in Agile methodologies , ETL / ELT skills , Data movement skills , Data processing skills; Experience in using Google Cloud Storge (GCS), Apache Beam (Dataflow), BigQuery, BigTable, Cloud Spanner, Dataproc or any other Google Cloud Analytical Services; Hands-on experience on native services like BigQuery, DataProc, DataFlow, DataPrep, Pub/Sub and DataLab; Cloud Storage, Cloud Data Fusion, Cloud Functions, Cloud Composer; Experience with open source distributed storage and processing utilities in the Apache Hadoop family and/or workflow orchestration products such as Apache Airflow; Experience with machine learning technologies such as Tensorflow, and experience in Google AI Platform particularly developing and/or training models or implementing solutions which rely on invoking such models; API Development\: Minimum of 3 years of application design and development experience in a cloud environment, including 2+ years of coding REST services and API such as\: Java, JSON, Python; Minimum 1 year of Google Cloud Platform’s Apigee EDGE platform and API development experience through the entire Software Development Lifecycle Process; Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment; Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini; 13 more items(s)","They are responsible for software-specific design and realization, as well as testing, deployment and release management, or technical and functional application management of client-specific package based solutions (e.g; These roles also require functional and methodological capabilities in testing and training; You focus on building solutions and on maintaining, optimizing and improving a client’s applications and systems; You contribute to a business and technical blueprint and customize the respective Software Package Core Module; You may also be responsible for unit testing, contribute to integration testing, and/or be responsible for the design and delivery of end-user training; Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed; 3 more items(s)","Java, Python, Spark, Google Cloud Storage (GCS), Apache Beam, BigQuery, BigTable, Cloud Spanner, Dataproc, BigQuery, DataProc, DataFlow, DataPrep, Pub/Sub, DataLab, Cloud Storage, Cloud Data Fusion, Cloud Functions, Cloud Composer, Apache Hadoop, Apache Airflow, TensorFlow, Google AI Platform, JSON, Apigee EDGE"
Bioinformatics Data Engineer,Natera,"Natera • San Carlos, CA •  via The Muse",Full-time,11 days ago,"models to ensure business units have access to the data they need and make informed decisions. This position is responsible for analyzing data and collaborate with business analyst and developers to translate data requirements into logical data models, develop ETL/ data integration as needed and the continued evolution of data warehouses, data stores and data marts.PRIMARY RESPONSIBILITIES:• Understand business requirements, business processes and data sources.• Actively participate/interact with users and business analyst to understand requirements of the data need and reports.• Design and develop source to target mapping to support data transformation process.• Design highly scalable ETL processes or data transformations.• Design and develop SQL functions, stored procedures and views, help develop reporting data mart, provide support to reporting team.• Serve as a subject matter expert in Structured Query Language (SQL).• Perform complex data analysis with large volumes of data.• Responsible for the integrity and quality of data and information stored and flowing between business applications, the enterprise data warehouse, data marts and business intelligence applications.• Responsible for creating design specification, technical documentation, data mapping and other training documents as needed.• Produce deliverables that facilitate better understanding across all products such as data flow diagrams, data models, ERDs and other illustrations.• Keep up with industry trends and best practices, advising on new and improved data engineering strategies leading to improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance.• This role works with PHI on a regular basis both in paper and electronic form and has access to various technologies to access PHI (paper and electronic) in order to perform the job.• Employee must complete training related to HIPAA/PHI privacy, General policies and procedure for compliance training and security training as soon as possible but not later than the first 30 days of hire.• Must maintain a status on Natera training requirements.• Performs other duties as assigned.QUALIFICATIONS:• Bachelor's or Master's degree in computer science or equivalent field.• Minimum of 8 years of relevant experience.• Minimum 5 years of experience with data warehouse methodologies (Star/Snowflake Schema, OLAP, Dimensional modeling etc.)• Minimum 6 years of experience in writing extensive SQL functions and procedures and optimization techniques• Experience in data analysis, data modelling, data cleansing and reporting• Experience with Data Extraction, Transformation and Loading (ETL) using Talend and similar platform.• Experience working with Cloud Data Platform - Snowflake or similar platform• Experience on programming language (Python preferred)• Experience with cloud platform services desired (AWS preferred)KNOWLEDGE, SKILLS, AND ABILITIES:• Proven understanding of the full software development lifecycle including testing, continuous integration and deployment• Excellent problem solving, critical thinking, and communication skills• Self-motivated, proactive and solution-oriented• Commitment to the successful achievement of team and organizational goals• Excellent interpersonal and collaborative skills• Knowledge of, and experience with Healthcare domain is plus• Nice to have: Experience in handling semi /un-structured data and related tools & technologies.PHYSICAL DEMANDS & WORK ENVIRONMENT:• Duties typically performed in an office setting.• This position requires the ability to use a computer keyboard, communicate over the telephone and read printed material.• Duties may require working outside normal working hours (evenings & weekends) at timesOUR OPPORTUNITYDriven by the passion for elevating the science and utility of genetic testing, Natera is committed to helping families identify and manage genetic diseases. Natera is a rapidly-growing diagnostics company with proprietary bioinformatics and molecular technology for analyzing DNA. Our complex technology has been proven clinically and commercially in the prenatal testing space and we are actively researching its applications in the liquid biopsy space for developing products with oncology applications.The Natera team consists of highly dedicated statisticians, geneticists, doctors, laboratory scientists, business professionals, software engineers and many other professionals from world-class institutions, who care deeply for our work and each other. When you join Natera, you'll work hard and grow quickly. Working alongside the elite of the industry, you'll be stretched and challenged, and take pride in being part of a company that is changing the landscape of genetic disease management.WHAT WE OFFERCompetitive Benefits. Generous Employee Referral program. Healthy catered lunches everyday, and premium snacks! Additionally, we offer complimentary premium coffee drinks and teas, and other beverages.For more information, visit www.natera.com.Natera is proud to be an Equal Opportunity Employer. We are committed to ensuring a diverse and inclusive workplace environment, and welcome people of different backgrounds, experiences, abilities and perspectives. Inclusive collaboration benefits our employees, our community and our patients, and is critical to our mission of changing the management of disease worldwide.All qualified applicants are encouraged to apply, and will be considered without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, age, veteran status, disability or any other legally protected status. We also consider qualified applicants regardless of criminal histories, consistent with applicable laws.","Successful applicants for this position must be fully vaccinated against COVID-19 as a condition of employment; Vaccine verification will be required; Bachelor's or Master's degree in computer science or equivalent field; Minimum of 8 years of relevant experience; Minimum 5 years of experience with data warehouse methodologies (Star/Snowflake Schema, OLAP, Dimensional modeling etc.); Minimum 6 years of experience in writing extensive SQL functions and procedures and optimization techniques; Experience in data analysis, data modelling, data cleansing and reporting; Experience with Data Extraction, Transformation and Loading (ETL) using Talend and similar platform; Experience working with Cloud Data Platform - Snowflake or similar platform; Proven understanding of the full software development lifecycle including testing, continuous integration and deployment; Excellent problem solving, critical thinking, and communication skills; Self-motivated, proactive and solution-oriented; Commitment to the successful achievement of team and organizational goals; Excellent interpersonal and collaborative skills; Knowledge of, and experience with Healthcare domain is plus; Nice to have: Experience in handling semi /un-structured data and related tools & technologies; 13 more items(s)","Competitive Benefits; Generous Employee Referral program; Healthy catered lunches everyday, and premium snacks!; Additionally, we offer complimentary premium coffee drinks and teas, and other beverages; 1 more items(s)","Talend, Snowflake, SQL, Python, AWS"
Ego Synthetic Data Engineer,Karkidi,"Karkidi • San Francisco, CA •  via Ladders",6 days ago,Full-time,"imized user experiences, and collaborate with machine learning engineers to build AI/ML systems. Additionally, we work with our global business/sales team to develop business tools that optimize their workflow, significantly impacting Airbnb's revenue.We are seeking a Staff Data Engineer to define and realize our technical vision, implement best data practices, and influence key stakeholders to build high-quality data models that empower our products.A Typical Day:Build the team’s long-term vision and roadmap by working with a cross-functional team at Airbnb.Mentor and coach data/analytics engineers on the team and influence the broader data community at Airbnb.Design, build, and maintain robust and efficient data pipelines that collect, process, and store data from various sources across Airbnb.Ensure the quality, performance, and stability of data systems through thoughtful quality systems and monitoring.Tune, productionize, and optimize data systems that power Airbnb products and AI/ML use cases.Collaborate and build relationships with partner engineering teams, including backend/client, data science, analytics, and ML.Your Expertise:9+ years of relevant industry experience with a Bachelor’s and/or Master’s degree in CS/EE, or equivalent experience, or 6+ years of experience with a PhDExperience collaborating with client, backend, ml, analytics teams, product and business partnersExperience designing and deploying high performance systems with reliable monitoring and logging practices.Effectively work across team boundaries to establish overarching data architecture, data flow, and provide guidance to individual teams.Strong knowledge of relational databases and query authoring (SQL).Strong expertise with Java / Scala / Spark and operating on data at the petabyte scale.Experience working on/with end-to-end Machine Learning products is a significant plus.Excellent communication skills, both written and verbal.Your Location:This position is US - Remote Eligible. The role may include occasional work at an Airbnb office or attendance at offsites, as agreed to with your manager. While the position is Remote Eligible, you must live in a state where Airbnb, Inc. has a registered entity. Click here for the up-to-date list of excluded states. This list is continuously evolving, so please check back with us if the state you live in is on the exclusion list.Our Commitment To Inclusion & Belonging:Airbnb is committed to working with the broadest talent pool possible. We believe diverse ideas foster innovation and engagement, and allow us to attract creatively-led people, and to develop the best products, services and solutions. All qualified individuals are encouraged to apply.We strive to also provide a disability inclusive application and interview process. If you are a candidate with a disability and require reasonable accommodation in order to submit an application, please contact us at: reasonableaccommodations@airbnb.com. Please include your full name, the role you’re applying for and the accommodation necessary to assist you with the recruiting process. #J-18808-Ljbffr","Collaborate and build relationships with partner engineering teams, including backend/client, data science, analytics, and ML.Your Expertise:9+ years of relevant industry experience with a Bachelor’s and/or Master’s degree in CS/EE, or equivalent experience, or 6+ years of experience with a PhDExperience collaborating with client, backend, ml, analytics teams, product and business partners; Experience designing and deploying high performance systems with reliable monitoring and logging practices; Effectively work across team boundaries to establish overarching data architecture, data flow, and provide guidance to individual teams; Strong knowledge of relational databases and query authoring (SQL).Strong expertise with Java / Scala / Spark and operating on data at the petabyte scale; Experience working on/with end-to-end Machine Learning products is a significant plus; Excellent communication skills, both written and verbal; Your Location:This position is US - Remote Eligible; 4 more items(s)","The Difference You Will Make:We collaborate with a cross-functional team that includes product managers, frontend and backend engineers, data scientists, and machine learning engineers to influence and build company-wide product releases; Mentor and coach data/analytics engineers on the team and influence the broader data community at Airbnb.Design, build, and maintain robust and efficient data pipelines that collect, process, and store data from various sources across Airbnb; Ensure the quality, performance, and stability of data systems through thoughtful quality systems and monitoring.Tune, productionize, and optimize data systems that power Airbnb products and AI/ML use cases","SQL, Java, Scala, Spark"
Senior Staff Software Engineer (Data)  - Activision Blizzard Media,Airtable,"Airtable • San Francisco, CA •  via ZipRecruiter",16 days ago,Full-time,"at our customers can build with Airtable is virtually limitless. Data engineering plays a critical role in building the data surrounding how people use the open-ended toolkit that Airtable offers, enabling our team to improve users' experience and understanding how our business is performingAs one of the data engineers at Airtable, you'll make an enormous contribution to our data engineering efforts. You'll design and own mission-critical data pipelines to enable decision-making, partner with company leaders to create scalable data solutions, and launch innovative alerting and visualization solutions.Learn more about our Engineering team and values here.What you'll do• Work between our engineering organization and stakeholders from our data science, growth, sales, marketing, and product teams, to understand the data needs of the business and produce pipelines, data marts, and other data solutions that enable better product and growth decision-making.• Design and update our foundational business tables in order to simplify analysis across the entire company.• Continue to improve the performance and reliability of our data warehouse.• Build and enforce a pattern language across our data stack, ensuring that our data pipelines and tables are consistent, accurate, and well-understood.Who you are• You have 5+ years of professional experience designing, creating and maintaining scalable data pipelines, preferably in Airflow.• You've wrangled enough data to understand how often the complex systems that produce data can go wrong.• You are proficient in at least one programming language (preferably Python), and are willing to become effective in others as needed to get your job done.• You are highly effective with SQL and understand how to write and tune complex queries.• You're passionate and thoughtful about building systems that enhance human understanding.• You communicate with clarity and precision in written form; experience communicating with graphs and plotsWant to Learn More? Our team faces significant challenges surrounding performance & reliability, read more!Airtable is an equal opportunity employer. We embrace diversity and strive to create a workplace where everyone has an equal opportunity to thrive. We welcome people of different backgrounds, experiences, abilities, and perspectives. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status or any characteristic protected by applicable federal and state laws, regulations and ordinances. Learn more about your EEO rights as an applicantVEVRAA-Federal ContractorIf you have a medical condition, disability, or religious belief/practice which inhibits your ability to participate in any part of the application or interview process, please complete our Accommodations Request Form and let us know how we may assist you. Airtable is committed to participating in the interactive process and providing reasonable accommodations to qualified applicants.Compensation awarded to successful candidates will vary based on their work location, relevant skills, and experience.Our total compensation package also includes the opportunity to receive benefits, restricted stock units, and may include incentive compensation. To learn more about our comprehensive benefit offerings, please check out Life at Airtable.For work locations in the San Francisco Bay Area, New York City, and Los Angeles, the base salary range for this role is:$213,900-$277,600 USDFor all other work locations (including remote), the base salary range for this role is:$192,500-$249,900 USDPlease see our Privacy Notice for details regarding Airtable's collection and use of personal information relating to the application and recruitment process by clicking here.","You have 5+ years of professional experience designing, creating and maintaining scalable data pipelines, preferably in Airflow; You've wrangled enough data to understand how often the complex systems that produce data can go wrong; You are proficient in at least one programming language (preferably Python), and are willing to become effective in others as needed to get your job done; You are highly effective with SQL and understand how to write and tune complex queries; You're passionate and thoughtful about building systems that enhance human understanding; You communicate with clarity and precision in written form; experience communicating with graphs and plots; 3 more items(s)","Our total compensation package also includes the opportunity to receive benefits, restricted stock units, and may include incentive compensation; $213,900-$277,600 USD","Airtable, Airflow, Python, SQL"
Software Engineer - Data Platform - New College Grad,Cloudious LLC,"Cloudious LLC • Mountain View, CA •  via Salary.com",7 days ago,Full-time,"loping APIs using Python/FastAPI.• Good to have 1 year of experience in LLM, Generative AI (developing capabilities or dev/ops)• Good to have Experience in developing API on Google Cloud Platform/Azure/API Gateways• Good to have 1 year of experience in Vector Database, Model Development would be an added benefit.Employers have access to artificial intelligence language tools (“AI”) that help generate and enhance job descriptions and AI may have been used to create this description. The position description has been reviewed for accuracy and Dice believes it to correctly reflect the job opportunity.• Dice Id: 10114358C• Position Id: 2024-23916•","Must have strong experience in Data Engineering along with Data Science; 5 years of Python experience; 5 years of big data experience needed (Big Query, Hadoop); 3 years of experience in Machine Learning (ML); 2 years of experience in developing APIs using Python/FastAPI; Good to have 1 year of experience in LLM, Generative AI (developing capabilities or dev/ops); Good to have Experience in developing API on Google Cloud Platform/Azure/API Gateways; Good to have 1 year of experience in Vector Database, Model Development would be an added benefit; Employers have access to artificial intelligence language tools (“AI”) that help generate and enhance job descriptions and AI may have been used to create this description; 6 more items(s)","Location: Mountain View, CA - Hybrid Role (3 days onsite)","Python, FastAPI, LLM, Generative AI, Google Cloud Platform, Azure, API Gateways, Vector Database, Model Development, Big Query, Hadoop, Machine Learning"
GraphDB Data Engineer with Tiger DB,Zoox,"Zoox • Foster City, CA •  via The Muse",Full-time,No Degree Mentioned,"pipeline to enable Zoox to develop and scale with a data-first culture.You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team.Responsibilities• Designing, building, and maintaining the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company• Defining and executing on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users• Establishing robust data integrity monitoring so that company-wide metrics are based on accurate data• Partnering with engineering and product teams to define data consumption patterns and establish best practicesQualifications• Experience designing and building complex data infrastructure at scale• Exceptional Python or Scala skills• Advanced SQL and data warehousing experience• Experience operating a workflow manager such as Airflow• Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)• A strong DataOps mindset and opinions on next-generation warehousing toolsBonus Qualifications• Basic fluency in C++• Familiarity with or exposure to experimentation platformsCompensationThere are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $200,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.ABOUT ZOOXZoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.Follow us on LinkedInVaccine MandateEmployees working in this position will be required to have received a single dose of the J&J/Janssen COVID-19 vaccine OR have completed the two-dose Pfizer or Moderna vaccine series.In addition, employees will be required to receive a COVID-19 booster vaccine within two months of becoming eligible for the booster vaccine.Employees will be required to show proof of vaccination status upon receipt of a conditional offer of employment. That offer of employment will be conditioned upon, among other things, an Applicant’s ability to show proof of vaccination status. Please note the Company provides reasonable accommodations in accordance with applicable state, federal, and local laws.About ZooxZoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.Follow us on LinkedInA Final Note:You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.Apply for this job","Experience designing and building complex data infrastructure at scale; Exceptional Python or Scala skills; Advanced SQL and data warehousing experience; Experience operating a workflow manager such as Airflow; Experience with large scale streaming platforms (e.g; Kafka, Kinesis), processing frameworks (e.g; Spark, Hadoop) and storage engines (e.g. HDFS, HBase); A strong DataOps mindset and opinions on next-generation warehousing tools; Basic fluency in C++; Familiarity with or exposure to experimentation platforms; Employees working in this position will be required to have received a single dose of the J&J/Janssen COVID-19 vaccine OR have completed the two-dose Pfizer or Moderna vaccine series.In addition, employees will be required to receive a COVID-19 booster vaccine within two months of becoming eligible for the booster vaccine; That offer of employment will be conditioned upon, among other things, an Applicant’s ability to show proof of vaccination status; 9 more items(s)","Compensation; There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights; The salary will range from $143,000 to $200,000; A sign-on bonus may be part of a compensation package; Compensation will vary based on geographic location, job-related knowledge, skills, and experience; Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance; 3 more items(s)","Python, Scala, SQL, Airflow, Kafka, Kinesis, Spark, Hadoop, HDFS, HBase, C++"
Senior Data Engineer - BPR Insight,Xcel life sciences,"Xcel life sciences • Sunnyvale, CA •  via Glassdoor",4 days ago,65 an hour,"ule:• 8 hour shiftExperience:• Informatica: 3 years (Required)• SQL: 6 years (Required)• Data warehouse: 8 years (Required)• Graph databases: 8 years (Required)• Tiger DB: 6 years (Required)Ability to Commute:• Sunnyvale, CA 94085 (Required)Ability to Relocate:• Sunnyvale, CA 94085: Relocate before starting work (Required)Work Location: Hybrid remote in Sunnyvale, CA 94085","Primary Skill - Graph DB- TigerDB and NO SQL( MongoDB and Casandra); Informatica: 3 years (Required); SQL: 6 years (Required); Data warehouse: 8 years (Required); Graph databases: 8 years (Required); Tiger DB: 6 years (Required); Sunnyvale, CA 94085 (Required); Sunnyvale, CA 94085: Relocate before starting work (Required); 5 more items(s)",Pay: Up to $65.00 per hour; Expected hours: 40 per week; 401(k); Dental insurance; Health insurance; 8 hour shift; 3 more items(s),"Informatica, SQL, Tiger DB, MongoDB, Casandra"
"Data Engineer, Data Platform (Contract)",Apple,"Apple • Elk Grove, CA •  via Careers At Apple",13 days ago,Full-time,"vide solutions to real business problems? Or perhaps you thrive on performing data discovery and creating proof of concepts? Do you want to be part of an energetic team critical to the success of Apple? The Insight Analytics Engineering team is looking for a Senior Data Engineer that can quickly summarize complex problems, and soar in a dynamic, high-energy environment. Your rock solid integrity, teamwork and ability to work independently are valued and encouraged. Having the ability to quantify your decisions, critical thinking and using your gut instinct are all key qualities that make ourAnalytics Team successful and fun.DescriptionDescriptionDesign, develop and maintain robust pipelines to support various LOBs analytics needs in multi-petabyte cloud environment.Perform in-depth analysis on our Analytics platform to help solve hard business questions.Developing maintain data models and architecture to support analytics reporting.Work side by side with exceptional people that run, optimize and scale our environment and help to tell meaningful stories with data.Stay current with latest trends and technologies in data analytics, and cloud computingPerform complex data analytics in new and innovative ways and leveraging your creativity to go beyond current tools to deliver amazing solutions.Work with high degree of independence, but supported by a broad technical team.Minimum QualificationsMinimum Qualifications• 8+ years of hands-on experience with data analytics in large scale volumes in Cloud environment AWS and GCP.• Experience with Spark, SQL, Trino and Big Data systems and tools• Programming skills in Scala, Java, Python, or similar languages• Experience with Iceberg and high performant queries on S3Key QualificationsKey QualificationsPreferred QualificationsPreferred Qualifications• Ability to work with various cross functional teams to gather input and provide guidance• Ability to present information to peers and senior management• Ability to bring creative, simple and logical approaches to complex problems• Technical minded with the aptitude to learn new technologies• Excellent problem solving and communication (verbal/written) skills• Ability to tell meaningful stories with data• Experience with Jupyter Notebooks, and BigTable• Bachelors in technical field (math, statistics, engineering, computer science, analytics, or similar)Education & ExperienceEducation & ExperienceAdditional RequirementsAdditional RequirementsPay & BenefitsPay & Benefits• At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $187,000 and $280,800, and your base pay will depend on your skills, qualifications, experience, and location.Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.More• Apple is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, Veteran status, or other legally protected characteristics. Learn more about your EEO rights as an applicant.","The Insight Analytics Engineering team is looking for a Senior Data Engineer that can quickly summarize complex problems, and soar in a dynamic, high-energy environment; Your rock solid integrity, teamwork and ability to work independently are valued and encouraged; Having the ability to quantify your decisions, critical thinking and using your gut instinct are all key qualities that make ourAnalytics Team successful and fun; 8+ years of hands-on experience with data analytics in large scale volumes in Cloud environment AWS and GCP; Experience with Spark, SQL, Trino and Big Data systems and tools; Programming skills in Scala, Java, Python, or similar languages; Experience with Iceberg and high performant queries on S3; 4 more items(s)","Pay & Benefits; Pay & Benefits; At Apple, base pay is one part of our total compensation package and is determined within a range; This provides the opportunity to progress as you grow and develop within a role; The base pay range for this role is between $187,000 and $280,800, and your base pay will depend on your skills, qualifications, experience, and location; Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs; Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan; You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition; Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation; Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program; 7 more items(s)","AWS, GCP, Spark, SQL, Trino, Scala, Java, Python, Iceberg, S3, Jupyter Notebooks, BigTable"
Oracle Data Engineer,Rhizome Data,"Rhizome Data • San Francisco, CA •  via ZipRecruiter",130K–170K a year,Full-time,"al databases, CSVs, and Excel at the enterprise scale. Successful candidates will also have practical experience building large scale ETL pipelines on AWS or GCP for data engineering, feature extraction, statistical analysis, and correlations.About RhizomeRhizome is at the forefront of developing decision intelligence technology at the intersection of climate science and infrastructure systems. Our team pursues this endeavor with the wisdom and steadiness of industry veterans, and the curiosity, grit, and energy of startup and technology enthusiasts.Our climate resilience SaaS platform helps utilities, governments, and industries plan for greater resilience to climate change and extreme weather by applying AI to a vast amount of information that characterizes infrastructure assets and their vulnerability to extreme weather. Focused on the $500B resilience investment gap in the grid today, our mandate is simple: Help electric utilities proactively adapt to climate change by integrating cutting-edge climate-asset intelligence into their existing planning workflows. As the world experiences record-breaking climate-related impacts, especially related to grid failures, our platform identifies future extreme weather vulnerabilities on utility assets at high resolutions and empowers planners to optimize investment deployments that keep society safe during natural hazard events.Roles and Responsibilities• Design, construct, and maintain data pipelines to combine large volumes of geospatial, climate + weather, and electric utility datasets.• Work with a cross-functional team to deliver data in support of analytic and ML pipelines.• Develop deep familiarity with electric utility datasets and take ownership of integration of new datasets into our existing environments.• Contribute to ML model development in the context of understanding future extreme weather impacts on the power grid.• Optimize storage and ETL pipelines.• Develop versioned, scalable, repeatable and reliable pipelines for utility data that is in GIS and Tabular format to Delta Lake format.• Scale & Automate data pipelines for statistical analysis for internal and external use-cases.• Standardize and scale multi-tenant data storage.• Exceptional ability to diagnose data issues and discrepancies.• Ability to modularize different stages of data ingestion and verification.• Ability to write algorithms for data sanity checks and classification of different data elements.• Ability to develop heuristics and suggestions for missing data items.• Ability to validate and test pipelines and write functional test to validate the pipelines.Qualifications• Exceptional Python programming skills.• Exceptional programming skills with NumPy, SciPy, Xarrays.• Exceptional programming skills with frameworks like Dagster or Airflow or Prefect.• Exceptional programming skills with Databricks or Apache Spark or Amazon EMR or Cloudera.• Deep expertise in storage optimization and partitioning on RDS, Postgres, PostGIS, Delta Lake.• Hands on with GIS dataset and QGIS or ESRI.• Hands of Experience with multi-dimensional Climate or Weather data.• Familiarity or hands on experience with Secure Cloud Development.We'll pay extra close attention if you have:• Exposure or experience with electric utility tech stacks (AMS, OMS, GIS, etc.).• Exposure to applied ML and Data Engineering in the context of electric utilities.Culture and Core ValuesAt Rhizome, we lead with compassion and empathy, aiming to understand before we help. Our thesis as technologists is that, in order to fulfill our mission to protect society from the impacts of climate change through intentional, intelligent infrastructure planning, we need to embark on a journey of respectfully listening, learning, and then problem-solving. This sentiment is represented through our core values:Empathy: Understanding and relating to problems, customers, and each other, with humility.Creativity: Exploring with curiosity and building with intention.Aspiration: Striving for societal impact, personal fulfillment, and simply doing good work.Tenacity: Pushing past barriers and the status quo with a sense of optimism and determination.Service Excellence: Delivering high-quality outcomes for our customers, colleagues, and communities.Compensation and BenefitsRhizome offers salaries and an excellent package of benefits and stock options. Compensation is based on a variety of factors including experience, role, and location.Salary Range: $130,000 - $170,000Benefits: Unlimited time off, stock options, excellent health, dental, and vision, and 401k.","The ideal candidate will have a strong background in data processing pipelines, DAGs, ETLs, feature extraction, and statistical analytics using Python and AWS cloud; The ideal candidate will have deep expertise in working with GIS data, relational databases, CSVs, and Excel at the enterprise scale; Successful candidates will also have practical experience building large scale ETL pipelines on AWS or GCP for data engineering, feature extraction, statistical analysis, and correlations; Ability to modularize different stages of data ingestion and verification; Exceptional Python programming skills; Exceptional programming skills with NumPy, SciPy, Xarrays; Exceptional programming skills with frameworks like Dagster or Airflow or Prefect; Exceptional programming skills with Databricks or Apache Spark or Amazon EMR or Cloudera; Deep expertise in storage optimization and partitioning on RDS, Postgres, PostGIS, Delta Lake; Hands on with GIS dataset and QGIS or ESRI; Hands of Experience with multi-dimensional Climate or Weather data; Familiarity or hands on experience with Secure Cloud Development; Exposure or experience with electric utility tech stacks (AMS, OMS, GIS, etc.); Exposure to applied ML and Data Engineering in the context of electric utilities; Empathy: Understanding and relating to problems, customers, and each other, with humility; Creativity: Exploring with curiosity and building with intention; Aspiration: Striving for societal impact, personal fulfillment, and simply doing good work; Tenacity: Pushing past barriers and the status quo with a sense of optimism and determination; 15 more items(s)","Rhizome offers salaries and an excellent package of benefits and stock options; Compensation is based on a variety of factors including experience, role, and location; Salary Range: $130,000 - $170,000; Benefits: Unlimited time off, stock options, excellent health, dental, and vision, and 401k; 1 more items(s)","CSVs, Excel, AWS, GCP, Python, NumPy, SciPy, Xarrays, Dagster, Airflow, Prefect, Databricks, Apache Spark, Amazon EMR, Cloudera, RDS, Postgres, PostGIS, Delta Lake, QGIS, ESRI, Secure Cloud Development"
Senior Data Engineer (Remote - US),Robert Half,"Robert Half • Anaheim, CA •  via Robert Half",22 hours ago,100K–147K a year,"associated with the job function.• Perform data migrations and automate data enrichment tasks to enhance efficiency.• Develop system integrations across marketing, finance, sales, and relationship management systems.• Utilize expertise in relational databases, data mining and transformation, integration patterns, systems administration, reporting, risk management, and application security.• Collaborate with team members to create effective, scalable solutions and coach them to develop their skills.• Make strategic decisions and implement best practices to ensure projects are completed on schedule.• Stay up-to-date with novel integration capabilities and patterns.• Create high-quality data products that are well-documented and user-friendly.• Build, maintain, and improve the infrastructure for extracting, transforming, and loading data from various sources using SQL and Oracle technologies.• Design analytics tools to utilize the data pipeline to produce actionable insights.• Analyze business needs to create scalable solutions.• Identify methods to streamline processes and automate repetitive tasks.• Lead projects and collaborate with partners from different functional departments.• Maintain security data across multiple data centers and regions.","In this role, the ideal candidate is a passionate and highly skilled professional with expertise in analytics tools and cloud technologies like AWS, Azure or similar technologies; They should be proficient in programming languages such as SQL, NoSQL, and Python, capable of processing large data sets to deliver high-quality, customer-facing data solutions and insights; Education: A bachelor's degree in computer science or information technology plus 8 years minimum of relevant experience; Programming Proficiency: High proficiency in programming languages commonly used in ETL development, such as PLSQL, SQL, Python; Ability to write efficient SQL queries, SQL store procedures, develop scripts for data transformations, and utilize programming frameworks and libraries to create/enhance ETL mappings and workflows; AWS Services: Expertise in utilizing AWS services, including but not limited to Amazon s3, glue, data catalog, Amazon redshift, redshift spectrum and Amazon Athena; Ability to leverage these services to build scalable, reliable, and performant data pipelines and analytics solutions; Proficiency in working with relational databases such as Postgres, Oracle, MySQL, or SQL Server; Knowledge of database design, optimization techniques, and advanced querying capabilities; Experience in performance tuning and optimizing database operations; Familiarity with data governance frameworks and data security best practices; Passion for learning new technologies, staying up to date with industry trends, and exploring innovative approaches to ETL development; Information will be requested to perform the compulsory background check; A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment; 11 more items(s)","Compensation to commensurate with experience with the pay band of $135,000 - $165,000/Annually; Compensation is commensurate with experience and includes a generous retirement package; Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP); Office Locations and a Remote Workforce; 1 more items(s)","SQL, Oracle, AWS, Azure, NoSQL, Python, PLSQL, Amazon S3, Glue, Data Catalog, Amazon Redshift, Redshift Spectrum, Amazon Athena, Postgres, MySQL, SQL Server"
Senior Data Engineer (Full-Time),Energy Solutions - USA,"Energy Solutions - USA • Oakland, CA •  via Indeed",2 days ago,135K–165K a year,"performance-based solutions for our utility, government, and institutional customers.We are currently seeking a Senior Data Engineer to join our Information Systems team to design, develop, and maintain data platforms that support the data needs across Energy Solutions. In this role, the ideal candidate is a passionate and highly skilled professional with expertise in analytics tools and cloud technologies like AWS, Azure or similar technologies. They should be proficient in programming languages such as SQL, NoSQL, and Python, capable of processing large data sets to deliver high-quality, customer-facing data solutions and insights. This unique position is perfect for individuals with technical prowess in data field that want to have an impact on energy efficiency markets and greenhouse gas reductions through our work for major North American utilities and other clients around the country.Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange California as well as Boston, New York and Chicago for those that wish to work from one of our offices.Responsibilities include but are not limited to:• Build, automate, and manage near-real-time scalable data ingestion pipelines for master data management, deep-learning, and predictive analytics.• Build and maintain cloud native big data environments on AWS, that are highly secure, scalable, flexible, and highly performant using appropriate SQL, NoSQL and NewSQL technologies.• Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage.• Provide technical input into build/buy/partner decisions for all components of the data infrastructure.• Partner closely with Data Scientists, BI developers, and Product Managers to design and implement data models, database schemas, data structures, and processing logic to support various data science, analytics, machine learning, and BI initiatives.• Design and develop ETL (extract-transform-load) processes to validate and transform data, calculate metrics, and model features, populate data models etc., using Spark, Python, SQL, and other technologies in the AWS.• Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage.• Lead by example, demonstrating best practices for code development and optimization, unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting, and incident response to ensure data availability, data quality, and usability.• Define SLAs for data availability and correctness. Automate data availability and quality monitoring and respond to alerts when data delivery SLAs are not being met.• Communicate progress across organizations and levels from individual contributor to executive. Identify and clarify the critical few issues that need action and drive appropriate decisions and actions. Communicate results clearly.Minimum Qualifications:• Education: A bachelor's degree in computer science or information technology plus 8 years minimum of relevant experience.• Programming Proficiency: High proficiency in programming languages commonly used in ETL development, such as PLSQL, SQL, Python. Ability to write efficient SQL queries, SQL store procedures, develop scripts for data transformations, and utilize programming frameworks and libraries to create/enhance ETL mappings and workflows.• AWS Services: Expertise in utilizing AWS services, including but not limited to Amazon s3, glue, data catalog, Amazon redshift, redshift spectrum and Amazon Athena. Ability to leverage these services to build scalable, reliable, and performant data pipelines and analytics solutions.• Proficiency in working with relational databases such as Postgres, Oracle, MySQL, or SQL Server. Knowledge of database design, optimization techniques, and advanced querying capabilities.• Experience in performance tuning and optimizing database operations.• Familiarity with data governance frameworks and data security best practices.• Passion for learning new technologies, staying up to date with industry trends, and exploring innovative approaches to ETL development.Compensation to commensurate with experience with the pay band of $135,000 - $165,000/AnnuallyCompensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).Office Locations and a Remote WorkforceEnergy Solutions is a predominantly remote workforce with offices in six different locations. At this time, we are not accepting applications from the following states (Alaska, Delaware, Kentucky, Mississippi, Montana, Nebraska, North Dakota, Wyoming).Inclusion, Diversity, Equity, AccessibilityWe live out our mission day in and day out by helping our clients and communities generate clean energy solutions the world needs. Our success is dependent on developing a team of creative thinkers, innovators, and a workforce that reflects the diversity of our world. To that end, we're committed to hiring, promoting, and retaining a diversity of talent, and infusing inclusion, diversity, equity, and accessibility (IDEA) throughout our business. We encourage people from underrepresented backgrounds and all walks of life to apply. Come grow with us at Energy Solutions!Background Check InformationInformation will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.Reasonable AccommodationsEnergy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require accommodations in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.Privacy Notice for Job Applicants","In this role, the ideal candidate is a passionate and highly skilled professional with expertise in analytics tools and cloud technologies like AWS, Azure or similar technologies; They should be proficient in programming languages such as SQL, NoSQL, and Python, capable of processing large data sets to deliver high-quality, customer-facing data solutions and insights; Education: A bachelor's degree in computer science or information technology plus 8 years minimum of relevant experience; Programming Proficiency: High proficiency in programming languages commonly used in ETL development, such as PLSQL, SQL, Python; Ability to write efficient SQL queries, SQL store procedures, develop scripts for data transformations, and utilize programming frameworks and libraries to create/enhance ETL mappings and workflows; AWS Services: Expertise in utilizing AWS services, including but not limited to Amazon s3, glue, data catalog, Amazon redshift, redshift spectrum and Amazon Athena; Ability to leverage these services to build scalable, reliable, and performant data pipelines and analytics solutions; Proficiency in working with relational databases such as Postgres, Oracle, MySQL, or SQL Server; Knowledge of database design, optimization techniques, and advanced querying capabilities; Experience in performance tuning and optimizing database operations; Familiarity with data governance frameworks and data security best practices; Passion for learning new technologies, staying up to date with industry trends, and exploring innovative approaches to ETL development; Information will be requested to perform the compulsory background check; A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment; 11 more items(s)","Compensation to commensurate with experience with the pay band of $135,000 - $165,000/Annually; Compensation is commensurate with experience and includes a generous retirement package; Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP); Office Locations and a Remote Workforce; 1 more items(s)","AWS, Azure, SQL, NoSQL, Python, PLSQL, Amazon S3, Glue, Data Catalog, Amazon Redshift, Redshift Spectrum, Amazon Athena, Postgres, Oracle, MySQL, SQL Server"
"Senior Director, Data Engineering",CDL (B2),"CDL (B2) • San Mateo, CA •  via LinkedIn",9 days ago,Full-time,"Company Introduction ""ALWAYS PAMOJA"" – At Zola Electric, we are a global team united by one mission: achieving Energy Equality through Enterprise Technology. Originating near the Serengeti, Tanzania, our diverse team has grown across 4 continents, embracing over 13 nationalities. Were committed to...","We are looking for an experienced and technically skilled individual to lead the team that provides data engineering and analytics functionality for the entire GoFundMe/Classy organization; 7+ years industry experience designing and building distributed data systems,; 5+ years of experience supporting or managing a business intelligence or analytics team, including employee development and performance management; A track record of recruiting, leading, and growing high performing technical teams in a demanding talent market; Strong proficiency in SQL and ETL processes; Modern data architectures including Data Lake, Data Mesh; Experience with data visualization tools such as Tableau, Looker, Quicksight, or similar; Proficiency in statistical/machine learning software such as R, Python, SAS, or Matlab; Completed bachelor's degree in Computer Science, Data Science, Statistics, or a related field; 6 more items(s)","Market competitive pay; Rich healthcare benefits including employer paid premiums for medical/dental/vision (100% for employee-only plans and 85% for employee + dependent plans) and employer HSA contributions; 401(k) retirement plan with company matching; Hybrid workplace with fully remote flexibility for many roles; Monetary support for new hire setup, hybrid work & wellbeing, family planning, and commuting expenses; A variety of mental and wellness programs to support employees; Generous paid parental leave and family planning stipend; Supportive time off policies including vacation, sick/mental health days, volunteer days, company holidays, and a floating holiday; Learning & development and recognition programs; ""Gives Back"" Program where employees can nominate a fundraiser every week for a donation from the company; Inclusion, diversity, equity, and belonging are vital to our priorities and we continue to evolve our strategy to ensure DEI is embedded in all processes and programs at GoFundMe; The total annual salary for this full-time position is $300,000-$410,000 + equity + benefits; The salary range was determined by role, level, and location requirement the US; Individual pay is determined by work location and additional factors including job-related skills, experience, and relevant education or training; Your recruiter can share more about the specific salary range based on your location during the hiring process; 12 more items(s)","SQL, ETL, Tableau, Looker, Quicksight, R, Python, SAS, Matlab"
Portfolio Data Engineer,Unreal Gigs,"Unreal Gigs • San Francisco, CA •  via Palagrit.com",23 days ago,Full-time,"tforms. Join us and lead our efforts in shaping the future of cloud data engineering.Position Overview: As the Lead Cloud Data Engineer, you'll spearhead our initiatives in designing, building, and optimizing cloud data solutions. You'll lead a team of skilled engineers, collaborating closely with cross-functional teams to deliver high-quality, scalable, and cost-effective data solutions. If you're a seasoned engineer with expertise in cloud data technologies and a proven track record of leadership in delivering successful data projects, we invite you to lead our team in this exciting opportunity.RequirementsKey Responsibilities:• Technical Leadership: Provide strategic guidance, mentorship, and technical leadership to a team of cloud data engineers, fostering a culture of excellence, innovation, and collaboration.• Cloud Data Architecture: Lead the design and implementation of cloud data architectures on platforms such as AWS, Azure, or Google Cloud Platform (GCP), leveraging cloud-native services to build scalable, reliable, and cost-effective data solutions.• Data Ingestion: Architect and develop robust data ingestion pipelines to collect data from diverse sources, including databases, APIs, files, and streaming sources, ensuring efficient and reliable data ingestion into cloud storage.• Data Storage: Design and optimize data storage solutions using cloud-native storage services such as Amazon S3, Azure Data Lake Storage, or Google Cloud Storage, balancing performance, cost, and scalability requirements.• Data Processing: Architect and implement data processing pipelines using cloud-native data processing frameworks such as Apache Spark, Apache Flink, or Google Dataflow, enabling real-time and batch data processing and analysis.• Data Integration: Architect and implement data integration solutions to seamlessly integrate data from diverse sources and systems into cloud data solutions, ensuring interoperability and data consistency.• Data Governance: Establish and enforce data governance policies and procedures to ensure data quality, security, and compliance with regulatory requirements in cloud data environments.• Performance Optimization: Lead efforts to optimize data pipelines and processing workflows for performance, scalability, and cost-effectiveness, leveraging cloud-native features and techniques.• Monitoring and Alerting: Implement robust monitoring and alerting solutions to track data pipeline performance and health, proactively identifying and resolving issues to minimize downtime and data loss.• Documentation and Best Practices: Define and promote best practices for cloud data engineering, ensuring clear and comprehensive documentation to facilitate understanding and collaboration among team members.• Collaboration: Collaborate closely with cross-functional teams, including data scientists, business analysts, and infrastructure teams, to understand requirements and deliver cloud data solutions that meet business needs.• Mentorship and Development: Mentor and coach junior engineers, providing guidance, support, and opportunities for skill development and career growth, and foster a culture of continuous learning and improvement within the team.Qualifications:• Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field.• 8+ years of experience in data engineering, with a focus on cloud data technologies.• Proven leadership experience, with a track record of successfully leading cloud data engineering teams and delivering complex data projects.• Expertise in cloud platforms such as AWS, Azure, or GCP, and proficiency in cloud-native services and tools for data storage, processing, and analytics.• Strong programming skills in languages such as Python, Java, or Scala, with experience in data processing frameworks like Apache Spark or Apache Flink.• Experience with cloud-native data processing services such as AWS Glue, Azure Data Factory, or Google Dataflow.• Strong understanding of data integration concepts and techniques, with experience integrating data from diverse sources and systems in cloud environments.• Strong problem-solving skills and analytical thinking, with the ability to design and troubleshoot complex cloud data solutions.• Excellent communication and collaboration skills, with the ability to effectively communicate technical concepts to non-technical stakeholders.Benefits• Competitive salary: The industry standard salary for Lead Cloud Data Engineers typically ranges from $200,000 to $300,000 per year, depending on experience and qualifications.• Comprehensive benefits package, including health insurance, retirement plans, and wellness programs.• Flexible work arrangements, including remote work options and flexible hours.• Generous vacation and paid time off.• Professional development opportunities, including access to training programs, conferences, and workshops.• State-of-the-art technology environment with access to cutting-edge tools and resources.• Vibrant and inclusive company culture with opportunities for growth and advancement.• Exciting projects with real-world impact at the forefront of data-driven innovation.Join Us: Ready to lead the charge in cloud data engineering? Apply now to join our team and be part of the data revolution!","Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field; 8+ years of experience in data engineering, with a focus on cloud data technologies; Proven leadership experience, with a track record of successfully leading cloud data engineering teams and delivering complex data projects; Expertise in cloud platforms such as AWS, Azure, or GCP, and proficiency in cloud-native services and tools for data storage, processing, and analytics; Strong programming skills in languages such as Python, Java, or Scala, with experience in data processing frameworks like Apache Spark or Apache Flink; Experience with cloud-native data processing services such as AWS Glue, Azure Data Factory, or Google Dataflow; Strong understanding of data integration concepts and techniques, with experience integrating data from diverse sources and systems in cloud environments; Strong problem-solving skills and analytical thinking, with the ability to design and troubleshoot complex cloud data solutions; Excellent communication and collaboration skills, with the ability to effectively communicate technical concepts to non-technical stakeholders; 6 more items(s)","Competitive salary: The industry standard salary for Lead Cloud Data Engineers typically ranges from $200,000 to $300,000 per year, depending on experience and qualifications; Comprehensive benefits package, including health insurance, retirement plans, and wellness programs; Flexible work arrangements, including remote work options and flexible hours; Generous vacation and paid time off; Professional development opportunities, including access to training programs, conferences, and workshops; State-of-the-art technology environment with access to cutting-edge tools and resources; Vibrant and inclusive company culture with opportunities for growth and advancement; 4 more items(s)","AWS, Azure, Google Cloud Platform (GCP), Amazon S3, Azure Data Lake Storage, Google Cloud Storage, Apache Spark, Apache Flink, Google Dataflow, AWS Glue, Azure Data Factory, Python, Java, Scala"
Data Engineer Manager,Addepar,"Addepar • Mountain View, CA •  via Jobs - Towards AI",28 days ago,Full-time,"ontributing to the organization's goal of offering streamlined data insights to clients. This results in less manual effort for both clients and internal teams, enhancing overall operational efficiency.","In the role, the individual will develop and maintain data integrations that enhance the efficiency of the Addepar platform's data processing capabilities; The position is pivotal in executing automated processes that improve data throughput and accuracy, contributing to the organization's goal of offering streamlined data insights to clients; This results in less manual effort for both clients and internal teams, enhancing overall operational efficiency","Experience in building data pipelines to serve reporting needs; Experience owning a team roadmap; Experience leading a matrixed team supporting many stakeholders; Ability to prioritize requests from multiple stakeholders in disparate domains; Ability to effectively communicate complex projects to non-technical stakeholders; BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field; 5+ year of experience in SQL or similar languages; 3+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc); 2+ years of experience leading a team; Experience in ETL / Data application development; Hands on experience with Google BigQuery; Experience in version control systems such as Git; Data architecture and warehousing experience; Experience leading a small team of data or software engineers; Experience with Airflow; 12 more items(s)","Addepar, SQL, Python, Java, Scala, Google BigQuery, Git, Airflow"
Data Engineer-GA4/Tableau,Snap,"Snap • Mountain View, CA •  via The Muse",Full-time,2 days ago,"CA, Mountain View, CA, or Seattle, WA, you’ll collaborate with teams across the organization (engineering, finance, sales, marketing, and strategy) to build pipelines and systems to deliver the data necessary for making the right decision in the moment. Our team strives to improve decision quality across the company by ensuring metrics are trustworthy, discoverable, and easily consumable.• What you’ll do:• Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner• Develop data pipelines adhering with privacy and governance principles• Become familiar with our data consumption portals and their capabilities• Build expertise and ownership of data quality for supported domains• Establish and implement data quality standards and controls• Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate• Drive adoption of the data sets you’ve produced• Knowledge, Skills & Abilities:• Experience in building data pipelines to serve reporting needs• Experience owning a team roadmap• Experience leading a matrixed team supporting many stakeholders• Ability to prioritize requests from multiple stakeholders in disparate domains• Ability to effectively communicate complex projects to non-technical stakeholders• Minimum Qualifications:• BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field• 5+ year of experience in SQL or similar languages• 3+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc)• 2+ years of experience leading a team• Experience in ETL / Data application development• Preferred Qualifications:• Hands on experience with Google BigQuery• Experience in version control systems such as Git• Data architecture and warehousing experience• Experience leading a small team of data or software engineers• Experience with AirflowAt Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets. If you have a disability or special need that requires accommodation, please don’t be shy and contact us at accommodations-ext@snap.com.","Experience in building data pipelines to serve reporting needs; Experience owning a team roadmap; Experience leading a matrixed team supporting many stakeholders; Ability to prioritize requests from multiple stakeholders in disparate domains; Ability to effectively communicate complex projects to non-technical stakeholders; BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field; 5+ year of experience in SQL or similar languages; 3+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc); 2+ years of experience leading a team; Experience in ETL / Data application development; Hands on experience with Google BigQuery; Experience in version control systems such as Git; Data architecture and warehousing experience; Experience leading a small team of data or software engineers; Experience with Airflow; 12 more items(s)","Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner; Develop data pipelines adhering with privacy and governance principles; Become familiar with our data consumption portals and their capabilities; Build expertise and ownership of data quality for supported domains; Establish and implement data quality standards and controls; Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate; Drive adoption of the data sets you’ve produced; 4 more items(s)","SQL, Python, Java, Scala, Google BigQuery, Git, Airflow"
"Principal, Data Engineer",Jobot Consulting,"Jobot Consulting • Santa Clara, CA •  via LinkedIn",2 days ago,65–90 an hour,"UsBased in Santa Clara, CA we are a fast-growing company that is revolutionizing the insurance space! We are an Insurtech company, located in the heart of Silicon Valley, revolutionizing the way travelers search, compare, purchase, and manage their travel insurance. Imagine a place where buying travel insurance is as easy as ordering an item from your favorite online retailer. You know exactly what the benefits are and what each word on the coverage document means, and you are able to zip through the checkout process. We are obsessed with simplifying Travel Insurance! We are currently looking for a Data Engineer-Marketing Background to join our team.Why join us?• Extremely Competitive Benefits!• Work / Life Balance!• Accelerated Career Growth!Job DetailsResponsibilities• Aggregate traffic and revenue data accurately in GA4 from the following sources Google Ads, Microsoft Ads, Meta, Reddit and additional online paid/organic sources.• Fix any data discrepancies due to Google Tag Manager• Audit Google Tag Manager’s triggers and tags for accuracy and it’s data feed to other platforms• Work with the team to QA tags and data streams for accuracy• Work with Engineering and Marketing teams to align on tracking KPIsExperience & Qualifications• 3 years of experience in tracking Support and Management using Google Tag Manager• 3 years of experience integrating data from key marketing platforms, including Meta, Google Ads, and TikTok Ads, to guarantee accurate data flow into Google Analytics 4 (GA4), and Tableau• 1 year experience working with Tableau, BigQuery, Python, and SQL• Significant experience in management of Google Analytics (GA4) architecture and delivery of web analytics insights and reporting from Google Analytics (required) - Especially tracking ‘key events’ and ‘revenue’ tracking• Significant understanding of attribution models and how this functions for tracking website sales• Proven history of working on technical site integrations or projectsPreferred Qualifications• Experience with Looker Studio, Funnel IO and Google Sheets• Excellent communication and presentation skillsWhat do we need in the next 60-90 days• Get data to a clean and accurate state in a singular location• Fix discrepancies between various marketing tools’ data for revenue and sales tracking across below platforms• GA4• Meta• Google Ads• Reddit• Bing• Clean up Google Tag Manager data/metadata• Export aggregated data to Google SheetsInterested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.Want to learn more about this role and Jobot Consulting?Click our Jobot Consulting logo and follow our LinkedIn page!","3 years of experience in tracking Support and Management using Google Tag Manager; 3 years of experience integrating data from key marketing platforms, including Meta, Google Ads, and TikTok Ads, to guarantee accurate data flow into Google Analytics 4 (GA4), and Tableau; 1 year experience working with Tableau, BigQuery, Python, and SQL; Significant experience in management of Google Analytics (GA4) architecture and delivery of web analytics insights and reporting from Google Analytics (required) - Especially tracking ‘key events’ and ‘revenue’ tracking; Significant understanding of attribution models and how this functions for tracking website sales; Proven history of working on technical site integrations or projects; Export aggregated data to Google Sheets; 4 more items(s)",Salary $65 - $90 per hour; Extremely Competitive Benefits!; Work / Life Balance!; Accelerated Career Growth!; 1 more items(s),"GA4, Google Ads, Microsoft Ads, Meta, Reddit, Google Tag Manager, Tableau, BigQuery, Python, SQL, Looker Studio, Funnel IO, Google Sheets, TikTok Ads"
Data Engineer / Quality Assurance - 81629,Marsh,"Marsh • Los Angeles, CA •  via Careers At Marsh McLennan",3 days ago,Full-time,"nnan Agency (MMA) provides business insurance, employee health & benefits, retirement, and private client insurance solutions to organizations and individuals seeking limitless possibilities. With 180 offices across North America, we combine the personalized service model of a local consultant with the global resources of the world’s leading professional services firm, Marsh McLennan (NYSE: MMC).A day in the life.As a Senior Platform Engineer on the Data Platform National Team, you will design, implement, and maintain Azure infrastructure that supports our data operations. You will play a key role in enabling our data teams to deliver insights efficiently and effectively.Responsibilities• Infrastructure Development: Design, build, and maintain secure and scalable Azure infrastructure using Terraform. Ensure that the architecture supports our evolving data processing needs.• CI/CD Implementation: Develop and maintain CI/CD pipelines for infrastructure deployment. Automate infrastructure provisioning and configuration to improve deployment speed and reliability.• Monitoring and Maintenance: Implement monitoring solutions to track system performance and health. Proactively address and troubleshoot infrastructure issues to ensure high availability and reliability.• Collaboration and Support: Work closely with data engineers and analysts to understand their infrastructure needs. Provide technical support and guidance to ensure optimal deployment and functioning of data applications and tools.• Best Practices and Innovation: Keep up-to-date with industry best practices in DevOps and cloud infrastructure. Recommend and implement improvements to our infrastructure management processes.• Documentation: Create and maintain detailed documentation for infrastructure setups, configurations, and procedures to ensure consistency and knowledge sharing within the team.Qualifications• 5+ years of experience in platform engineering, with a proven track record of deploying and managing Azure cloud environments.• DevOps Principles: Solid understanding of DevOps practices including automated testing, deployment, and orchestration.• Problem Solving: Excellent analytical and problem-solving abilities.Preferred Skills• Certifications in Azure are highly desirable.• Knowledge of data engineering and big data technologies is advantageous.• Communication: Strong communication skills with the ability to convey complex technical information clearly to non-technical stakeholders.Technical Skills:o Strong expertise in Azure services and management, particularly Azure data serviceso Proficient in IaC tools, particularly Terraform.o Experience in setting up and managing CI/CD pipelines.o Familiarity with monitoring tools and practices.Valuable benefits.We value and respect the impact our colleagues make every day both inside and outside our organization. We’ve built a culture that promotes colleague well-being through robust benefit programs and resources, encourages professional and personal development, and celebrates opportunities to pursue the projects and causes that give colleagues fulfilment outside of work.Some benefits included in this role are:• Generous time off, including personal and volunteering• Remote work• Tuition reimbursement and professional development opportunities• Charitable contribution match programs• Stock purchase opportunitiesTo learn more about a career at MMA, check us out online: http://marshmma.com/careers or flip through our recruiting brochure: https://bit.ly/3QpcjmwFollow us on social media to meet our colleagues and see what makes us tick:• https://www.instagram.com/lifeatmma/• https://www.facebook.com/LifeatMMA• https://twitter.com/marsh_mma• https://www.linkedin.com/company/marsh-mclennan-agency/The applicable base salary range for this role is $95,130-$166,425. The base pay offered will be determined on factors such as experience, skills, training, location, certifications, education, and any applicable minimum wage requirements. Decisions will be determined on a case-by-case basis. In addition to the base salary, this position may be eligible for performance-based incentives.We are excited to offer a competitive total rewards package which includes health and welfare benefits, tuition assistance, 401K savings and other wellbeing programs as well as employee assistance programs.Applications will be accepted until: September 23rd, 2024Who you are is who we are.We embrace a culture that celebrates and promotes the many backgrounds, heritages and perspectives of our colleagues and clients. We are always seeking those with ethics, talent, and ambition who are interested in joining our client-focused teams.Marsh McLennan and its affiliates are EOE Minority/Female/Disability/Vet/Sexual Orientation/Gender Identity employers.","5+ years of experience in platform engineering, with a proven track record of deploying and managing Azure cloud environments; DevOps Principles: Solid understanding of DevOps practices including automated testing, deployment, and orchestration; Problem Solving: Excellent analytical and problem-solving abilities; Strong expertise in Azure services and management, particularly Azure data services; Proficient in IaC tools, particularly Terraform; Experience in setting up and managing CI/CD pipelines; Familiarity with monitoring tools and practices; 4 more items(s)","We value and respect the impact our colleagues make every day both inside and outside our organization; We’ve built a culture that promotes colleague well-being through robust benefit programs and resources, encourages professional and personal development, and celebrates opportunities to pursue the projects and causes that give colleagues fulfilment outside of work; Generous time off, including personal and volunteering; Remote work; Tuition reimbursement and professional development opportunities; Charitable contribution match programs; Stock purchase opportunities; The applicable base salary range for this role is $95,130-$166,425; The base pay offered will be determined on factors such as experience, skills, training, location, certifications, education, and any applicable minimum wage requirements; In addition to the base salary, this position may be eligible for performance-based incentives; We are excited to offer a competitive total rewards package which includes health and welfare benefits, tuition assistance, 401K savings and other wellbeing programs as well as employee assistance programs; 8 more items(s)","Azure, Terraform"
Senior Staff Data Engineer,Swoon,"Swoon • Roseville, CA •  via LinkedIn",7 hours ago,Full-time,"lutions using AI tools and Machine Learning models• Document and define data QA processes and guidelinesThings to Love about this company• Established company offering stability and job security• Collaborative and team-oriented culture• Amazing mission transforming the lives of millions who are hard-of-hearing bringing them true meaningful connections again.• Competitive benefits including medical, dental, vision and 401K with company matchWhat’s required?• 4+ years of experience as a data engineer or data analyst• 4+ years of experience working in Quality Assurance• Experience in Python, Scala, SQL, and no SQL databases• Experience in designing and implementing QA framework for large-scale data environmentsWhat else should you know?Direct-Hire Role – Full-time Opportunity – NO contracting or consultingLocation – Roseville, CAFully Remote – Must reside in the USWhat’s Next?• Apply Now!• Email questions to Kathryn.Jackson@swoontech.com","4+ years of experience as a data engineer or data analyst; 4+ years of experience working in Quality Assurance; Experience in Python, Scala, SQL, and no SQL databases; Experience in designing and implementing QA framework for large-scale data environments; 1 more items(s)","Established company offering stability and job security; Collaborative and team-oriented culture; Amazing mission transforming the lives of millions who are hard-of-hearing bringing them true meaningful connections again; Competitive benefits including medical, dental, vision and 401K with company match; Direct-Hire Role – Full-time Opportunity – NO contracting or consulting; 2 more items(s)","Python, Scala, SQL, noSQL databases"
Data engineer,ALTOS LABS,"ALTOS LABS • San Francisco, CA •  via Local Job Network",28 days ago,Full-time,"c innovation and inquiry.We are building a company where exceptional scientists and industry leaders from around the world work side by side to advance a shared mission.Our intentional focus is on Belonging, so that all employees know that they are valued for their unique perspectives.At Altos, we are all accountable for sustaining a diverse and inclusive environment.What You Will Contribute To AltosThe Altos Labs Scientific Computing & Data group consists of software, data, and machine learning engineers implementing scalable data engineering solutions enabling Altos Labs' scientific mission. We are currently hiring a Senior Staff Data Engineer (curation) to lead the data curation and operations team. This role will be responsible for designing ontologies and information models capturing multimodal genomics, imaging, mass spectrometry, and clinical data generating unique insights on cell rejuvenation and health. This pivotal role involves leading the operational management and integration of multimodal data sources to support Altos Labs' pioneering scientific mission in cell rejuvenation and health.Responsibilities• Lead the scientific data curation and quality control of internal and externally generated data at Altos Labs.• Lead a centralized platform for managing requests related to data ingestion, curation, and wrangling. This includes the adoption or implementation of technologies and processes including from SDKs to LLMs to optimize data ingestion.• Lead the design and operational management of ontologies and information models to capture and integrate multimodal genomics, imaging, mass spectrometry, and clinical data.• Work with teams to drive the standardization and harmonization of scientific datasets across Altos Labs, utilizing established ontologies and controlled vocabularies to ensure consistency and facilitate data sharing.• Collaborate with data generation hubs, CROs and internal labs to ingest multi-modal datasets into Altos data platform.• Engage with research teams to understand data access & analysis patterns and help design appropriate tools and frameworks to address those needs.• Manage the strategy and execution of semantic data operations across various modalities, ensuring robust data quality and accessibility to fuel AI/ML research initiatives.• Work closely with researchers across Altos Science & Medical Institutes, ensuring that data operations align with scientific objectives and support accelerated research outcomes through effective semantic data integration and inference.Who You AreMinimum Qualifications• PhD in Computer Science, Bioinformatics, or a related field, with a strong emphasis on data and knowledge modeling.• Minimum of 10 years of experience in an academic or industrial setting, managing, or analyzing research datasets in genomics, imaging, and related fields.• Experience in knowledge graph development, ontology management, and programming in languages such as Python or R.• Proven track record in leading data operations from concept through implementation, including the ability to manage large-scale data integration projects.• Strong familiarity with biological data analysis methods and experience collaborating with biologists or bioinformaticians.• Experience with biomedical large language models is advantageous.• Demonstrated ability to work effectively in cross-functional and geographically dispersed teams.The salary range for Redwood City, CA:• Senior Staff Data Engineer: $237,150 - $320,850The salary range for San Diego, CA:• Senior Staff Data Engineer: $222,700 - $301,300Exact compensation may vary based on skills, experience, and location.For UK applicants, before submitting your application:- Please click here to read the Altos Labs EU and UK Applicant Privacy Notice (bit.ly/eu_uk_privacy_notice)- This Privacy Notice is not a contract, express or implied and it does not set terms or conditions of employment.What We Want You To KnowWe are a culture of collaboration and scientific freedom, and we believe in the values of diversity, inclusion and belonging to inspire innovation.Altos Labs provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Altos currently requires all employees to be fully vaccinated against COVID-19, subject to legally required exemptions (e.g., due to a medical condition or sincerely-held religious belief).Thank you for your interest in Altos Labs where we strive for a culture of scientific freedom, learning, and belonging.Note: Altos Labs will not ask you to download a messaging app for an interview or outlay your own money to get started as an employee. If this sounds like your interaction with people claiming to be with Altos, it is not legitimate and has nothing to do with Altos. Learn more about a common job scam at https://www.linkedin.com/pulse/how-spot-avoid-online-job-scams-biron-clark/","PhD in Computer Science, Bioinformatics, or a related field, with a strong emphasis on data and knowledge modeling; Minimum of 10 years of experience in an academic or industrial setting, managing, or analyzing research datasets in genomics, imaging, and related fields; Experience in knowledge graph development, ontology management, and programming in languages such as Python or R; Proven track record in leading data operations from concept through implementation, including the ability to manage large-scale data integration projects; Strong familiarity with biological data analysis methods and experience collaborating with biologists or bioinformaticians; Experience with biomedical large language models is advantageous; Demonstrated ability to work effectively in cross-functional and geographically dispersed teams; 4 more items(s)","Senior Staff Data Engineer: $237,150 - $320,850; Senior Staff Data Engineer: $222,700 - $301,300","Python, R"
Data Engineer III 🏆,VirtualVocations,"VirtualVocations • Fremont, CA •  via Talent.com",20 hours ago,Full-time,Last updated : 2024-09-07,"Familiarity with battery cell design and materials is also advantageous; Proficiency in using modeling and simulation tools such as MATLAB, Simulink, and other specialized tools; Experience in writing efficient code to analyze large data sets, along with familiarity in Git and the code review process; Attention to detail and the ability to develop high-quality code documentation are essential; Experience in dynamic modeling, statistical and machine learning algorithm design; I look for experience in developing solutions using control theories like Kalman filters and state observers; I value experience in mathematical modeling of linear and nonlinear dynamical systems and distributed parameter systems, along with the ability to solve optimization problems; 4 more items(s)","Salary: 166,400 - 176,800 USD per year; Benefits & perks that we offer:; The pay rate is between $80 to $85 per hour, with the specific range offered based on several factors, including education, work experience, location, job duties, and certifications; Volt offers benefits (based on eligibility) that include health, dental, vision, term life, short-term disability, AD&D, 401(k), sick time, and other types of paid leaves as required by law, along with an Employee Assistance Program (EAP); 1 more items(s)","MATLAB, Simulink, Git"
AWS Data Engineer with Snowflake,Volt Workforce Solutions,"Volt Workforce Solutions • Cupertino, CA •  via DevITjobs",9 days ago,166K–177K a year,"geous.- Proficiency in using modeling and simulation tools such as MATLAB, Simulink, and other specialized tools.- Experience in writing efficient code to analyze large data sets, along with familiarity in Git and the code review process. Attention to detail and the ability to develop high-quality code documentation are essential.- Experience in dynamic modeling, statistical and machine learning algorithm design.- I look for experience in developing solutions using control theories like Kalman filters and state observers.- I value experience in mathematical modeling of linear and nonlinear dynamical systems and distributed parameter systems, along with the ability to solve optimization problems.- Expertise in C/C++ programming is a plus.👩‍💻👨‍💻 Your responsibilities are:In this position, I expect you to utilize a combination of advanced mathematics, machine learning, electrical engineering, and control engineering knowledge to:- Develop advanced battery models and control algorithms to enhance battery performance.- Create models that estimate and optimize cell and pack performance during the design phase.- Analyze large volumes of test data to understand cell performance differences and propose design improvements.- Formulate algorithms to estimate the State of Charge and State of Power of batteries.- Develop machine learning algorithms or mathematical models to predict cell performance over time.- Create simulation models and tools to evaluate new algorithms.View this job and over 500 other transparent jobs with salaries (💰💰💰) & tech stacks (🛠️) on DevITjobsCategory: Data Developer / EngineerLocation address: Great America Py 5201, San-Jose, United StatesSalary: 166,400 - 176,800 USD per yearBenefits & perks that we offer:Volt Workforce Solutions - More about us and the role:This is a Full-Time, Contract opportunity for a duration of 12 months, with a preference for candidates located in Cupertino, CA. The pay rate is between $80 to $85 per hour, with the specific range offered based on several factors, including education, work experience, location, job duties, and certifications.We encourage qualified candidates to apply now for immediate consideration. Please complete the required information, and we will be in touch as soon as possible.Volt offers benefits (based on eligibility) that include health, dental, vision, term life, short-term disability, AD&D, 401(k), sick time, and other types of paid leaves as required by law, along with an Employee Assistance Program (EAP).As an Equal Opportunity Employer, we prohibit any form of unlawful discrimination and harassment and are committed to creating a diverse and inclusive work environment. In accordance with various laws, we consider qualified applicants regardless of citizenship status or arrest and conviction records. If you require a reasonable accommodation for your job application, please inform us of your needs.Are you looking for Data jobs in San-Jose?","Familiarity with battery cell design and materials is also advantageous; Proficiency in using modeling and simulation tools such as MATLAB, Simulink, and other specialized tools; Experience in writing efficient code to analyze large data sets, along with familiarity in Git and the code review process; Attention to detail and the ability to develop high-quality code documentation are essential; Experience in dynamic modeling, statistical and machine learning algorithm design; I look for experience in developing solutions using control theories like Kalman filters and state observers; I value experience in mathematical modeling of linear and nonlinear dynamical systems and distributed parameter systems, along with the ability to solve optimization problems; 4 more items(s)","Salary: 166,400 - 176,800 USD per year; Benefits & perks that we offer:; The pay rate is between $80 to $85 per hour, with the specific range offered based on several factors, including education, work experience, location, job duties, and certifications; Volt offers benefits (based on eligibility) that include health, dental, vision, term life, short-term disability, AD&D, 401(k), sick time, and other types of paid leaves as required by law, along with an Employee Assistance Program (EAP); 1 more items(s)","MATLAB, Simulink, Git, C/C++"
Electrical/Data Engineer,Cygnus Professionals Inc,"Cygnus Professionals Inc • Palo Alto, CA •  via Indeed",Contractor,No Degree Mentioned,"tries and geographies with our industry-focused business excellence.Cygnus Professionals Inc. has been named by the US Pan Asian American Chamber of Commerce Education Foundation (USPAACC) as one of the “Fast 100 Asian American Businesses” – joining the country’s fastest-growing Asian American-owned companies, based on percentage revenue growth over the immediate past two years.Job DescriptionGreetings from Cygnus Professionals,Hope you’re doing great !I have a requirement for you Kindly go through it, If you feel comfortable and would like to apply for this role so kindly provide me your updated profile along with your contact details and rate part.Role: Big Data Engineer w/SparkLocation: Palo Alto, CADuration: Potential Contract to HireMode of Interview: Skype After Phone!! Need Green Card OR US Citizen OR EAD GC Candidates Only !!Position Description:• Experience with Java, Spark and Hadoop• A minimum of 2 years of experience working with distributed systems• Knowledge in distributed system design, data pipelining, and implementation• Knowledge in machine learning algorithms• Knowledge and experience in building large scale applications using various software design patterns and OO design principles• Experience with either distributed computing (Hadoop/Spark/Cloud) or parallel processing (CUDA/threads/MPI)• Expertise in design pattern (UML diagrams) and data modeling of large scale analytic systems• Experience in research, analysis, and the conversion of large amount of raw collected data and content into new sets of data that is structured and does not reduce data context in order to enable the Productization of new products• Worked with data warehousing and distributed/parallel processing of large data sets using parallel computing system to map/reduce computation and Linux clusters (e.g. Hadoop/Cloud technologies, HDFS); cluster;• Experienced in modern development methodology such as Agile, Scrum and SDLC• Ability to work in a research oriented, fast pace, and highly technical environment• Quick thinker and a fast learner, collaborative spirit, and excellent communication and interpersonal skillsAdditional Information• * U.S. Citizens and those who are authorized to work independently in the United States are encouraged to apply. We are unable to sponsor at this time.• * All your information will be kept confidential according to EEO guidelines.","Need Green Card OR US Citizen OR EAD GC Candidates Only !!; Experience with Java, Spark and Hadoop; A minimum of 2 years of experience working with distributed systems; Knowledge in distributed system design, data pipelining, and implementation; Knowledge in machine learning algorithms; Knowledge and experience in building large scale applications using various software design patterns and OO design principles; Experience with either distributed computing (Hadoop/Spark/Cloud) or parallel processing (CUDA/threads/MPI); Expertise in design pattern (UML diagrams) and data modeling of large scale analytic systems; Experience in research, analysis, and the conversion of large amount of raw collected data and content into new sets of data that is structured and does not reduce data context in order to enable the Productization of new products; Worked with data warehousing and distributed/parallel processing of large data sets using parallel computing system to map/reduce computation and Linux clusters (e.g. Hadoop/Cloud technologies, HDFS); cluster;; Experienced in modern development methodology such as Agile, Scrum and SDLC; Ability to work in a research oriented, fast pace, and highly technical environment; Quick thinker and a fast learner, collaborative spirit, and excellent communication and interpersonal skills; U.S. Citizens and those who are authorized to work independently in the United States are encouraged to apply; 11 more items(s)",All your information will be kept confidential according to EEO guidelines,"Java, Spark, Hadoop, Cloud, CUDA, threads, MPI, UML diagrams, Agile, Scrum, SDLC"
"Senior Data Engineer, Data Ingestion",Creamos,"Creamos • Fremont, CA •  via Learn4Good",5 days ago,Full-time,"efficiency. Perform data migration, ingest, egress, and transform data from multiple sources.Skills Required:Big Data, Azure, Tableau, Data Stage, Snowflake, Pyspark, Hadoop, Hive, Sqoop, ETL, PL/SQL and Jira. Bachelors degree in Science, Technology, or Engineering (any);Business Administration with 2 years of experience in the job offered or related occupation is required.Work location:Fremont, CA and various unanticipated locations throughout the U.S. Send Resume to HR Dept., Creamos Solutions, Inc., 43195 Mission Blvd., Suite A1, Fremont, CA 94539.","Big Data, Azure, Tableau, Data Stage, Snowflake, Pyspark, Hadoop, Hive, Sqoop, ETL, PL/SQL and Jira; Bachelors degree in Science, Technology, or Engineering (any);; Business Administration with 2 years of experience in the job offered or related occupation is required","Design and develop data models and database architecture to ensure data accuracy, consistency, and integrity; Create databases and database objects like Tables, Views, Indexed Views, Stored Procedures, DDL/DML Triggers, and Cursors; Develop PL/SQL scripts for data reconciliation and data accuracy; Create user stories and utilize reusable design components to tune code for efficiency; Perform data migration, ingest, egress, and transform data from multiple sources; 2 more items(s)","Big Data, Azure, Tableau, Data Stage, Snowflake, Pyspark, Hadoop, Hive, Sqoop, ETL, PL/SQL, Jira"
Senior Data Engineer (4064),VirtualVocations,"VirtualVocations • Fremont, CA •  via Talent.com",3 days ago,Full-time,Last updated : 2024-09-05,"The ideal candidate will come in with knowledge in multiple cloud and database technologies to hit the ground running from a technology perspective; Bachelor’s Degree or higher in Computer Science, Information Technology, or related field and 5+ years of work experience as a data engineer; or a Master’s degree with 3+ years of experience; or a PhD with 1+ years of experience; Proven experience as a data engineer with data technologies for on-prem and Cloud environments, specifically GCP; Experience managing and prioritizing multiple projects simultaneously; Experience with API creation and management skills are needed, at least on GCP; Experience with data analysis and problem-solving skills who can handle the project delivery with minimal managerial intervention; Experience building and maintaining strong relationships with business users, software developers, other cross-functional co-workers, and outside partners; Experience in SQL\NO-SQL is required; Experience translating between technical and non-technical stakeholders; Experience with hybrid (cloud/on-premises) data topologies; Experience designing data lake and data warehouse architectures using modern cloud solutions; Hands-on experience implementing data transformation (ETL/ELT, etc.) best practices at scale, preferably using modern cloud solutions; Experience with SQL and database architecture; Experience with both open source and off the shelf technologies; Experience with communicating across various levels, both written and verbally; This includes listening skills and an open attitude toward constructive feedback; Experience working both with a global team and individually; 14 more items(s)","Amyris is committed to fair and equitable compensation practices. ; The annual pay range for this role is $140,500 to $207,000; Salary offers are made based on internal equity and market analysis, and will vary based on the candidate’s skills, depth of experience and specific work location.  ; Amyris is committed to providing a diverse array of inclusive benefits and perks to support employees and their families’ wellbeing including access to robust healthcare, mental wellness benefits, family leave, discounted fitness memberships, flexible paid time off, 17 paid holidays, 10 paid sick days, and additional paid time off including 3 volunteer days, and bereavement; 1 more items(s)","GCP, SQL, NO-SQL"
Data Engineer- Data Bricks- Hybrid CA,Moss Adams LLP,"Moss Adams LLP • Woodland Hills, CA •  via WhatJobs",2 days ago,Full-time,"lture make this a reality. Join a values-driven firm where you’ll have fun while solving complex and interesting business challenges.Introduction to the Team:The Senior Data Engineer on the Data Platform Team is primarily responsible for the development, testing and maintenance of the firm's analytical data assets to support DW/BI applications, workflows, reporting, integrations and other solutions. This role collaborates with data scientists, report writers, product group customers, stakeholders and other IT resources to define business needs and build scalable data processing pipelines. This individual is expected to provide solution design leadership, as well as mentor junior engineers and team members. Expertise in SQL and Python, solid experience and understanding on Snowflake data cloud and Azure data assets, and a strong understanding of data integration, data warehousing and dimensional modeling are required. Experience with SQL Server 2019 is desired to support migration from on-prem SQL Server to Snowflake. Strong analytical skills as well as good customer service is preferred.Individuals who thrive at Moss Adams exhibit the following success skills – Collaboration, Critical Thinking, Emotional Intelligence, Executive Presence, Growth Mindset, Intellectual Curiosity, and Results Focus.Responsibilities:In collaboration with the data analytics & reporting team, manage and support Azure Machine Learning environmentDevelop ETL using data factory, SSIS toolsDesign, construct, and manage the data lake environment including: Data ingestion, staging, data quality monitoring, and business modelingDevelop, construct, test and maintain architectures (databases and large-scale processing systems)Design, develop and implement technical solutions in SQL environment (databases, SQL, data marts, integrations)Develop visual reports in Power BIMaintain and develop queries and stored procedures using T-SQLSupport data integrationsAnalyze data and identify gaps, data quality and integrity issuesWork in all aspects of development, design, reviews, troubleshooting and post-production issue resolutionFollow SDLC and create and maintain required documentation of newly developed and refactored applicationsQualifications:Bachelor’s degree or equivalent experience required; emphasis in Computer Science, Computer Engineering or related field preferredMinimum of 3 years of related experience required; experience in a professional services environment preferredHighly skilled in SQL programming in T-SQL building stored procedures and jobsStrong functional understanding of Azure Machine LearningAbility to manage large volume data integration/ETL; experience with SQL Server Integration Services (SSIS) including ability to configure SQL Server agentExpertise in identifying and mapping data elements across multiple systemsKnowledge of Power BI report visualizations, Data Analysis Expressions (DAX), Power Query (M), and Power BI data modelingHands-on experience in writing DAX queries and custom calculationsStrong understanding of data integration, quality and validationStrong understanding of MDM, Data Warehouse/Data mart and relational database designExcellent written and verbal communication skills and able to interact with multiple IT teams and business partners both formally and informallyShould possess sound knowledge of SDLC using Azure DevOpsSelf-motivated with the ability to work both independently and in a team environmentEffective time management, ability to multi-task with changing priorities while continuing to provide exceptional client serviceAbility to travel as needed, approximately 5%Moss Adams is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation, gender identity or any other characteristic protected by law.Moss Adams complies with federal and state disability laws and makes reasonable accommodations for applicants and employees with disabilities. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contactSome local/state regulations require employers to disclose the pay range in job postings. While this is the typical range of pay for the position, actual pay may vary based on internal equity, knowledge, experience, skillset, and geographic location among other factors. It’s uncommon for an individual to be hired at the top end of the pay range. This position may be eligible for an annual discretionary bonus. For more information about our benefit offerings and other total rewards, visit our careers page.#LI-AC1Compensation Range (Denver Market ONLY): Compensation range for California: $106,000 -$152,000 Compensation range for Washington: $99,000 -$133,000Primary Location Tacoma, WAOther Locations Pasadena, CA, Woodland Hills, CA, Spokane, WA, San Diego, CA, Bellingham, WA, Orange County, CA, Eugene, OR, Walnut Creek, CA, El Segundo, CA, Wenatchee, WA, San Francisco, CA, Everett, WA, Medford, OR, Yakima, WA, Tri-Cities, WA, Stockton, CA, Seattle, WA, Santa Rosa, CA, Silicon Valley, CA, Sacramento, CA, Portland, OR, Fresno, CAEmployee Status: RegularSchedule: Full TimeReq ID: 27392","Expertise in SQL and Python, solid experience and understanding on Snowflake data cloud and Azure data assets, and a strong understanding of data integration, data warehousing and dimensional modeling are required; Individuals who thrive at Moss Adams exhibit the following success skills – Collaboration, Critical Thinking, Emotional Intelligence, Executive Presence, Growth Mindset, Intellectual Curiosity, and Results Focus; Highly skilled in SQL programming in T-SQL building stored procedures and jobs; Strong functional understanding of Azure Machine Learning; Ability to manage large volume data integration/ETL; experience with SQL Server Integration Services (SSIS) including ability to configure SQL Server agent; Expertise in identifying and mapping data elements across multiple systems; Knowledge of Power BI report visualizations, Data Analysis Expressions (DAX), Power Query (M), and Power BI data modeling; Hands-on experience in writing DAX queries and custom calculations; Strong understanding of data integration, quality and validation; Strong understanding of MDM, Data Warehouse/Data mart and relational database design; Excellent written and verbal communication skills and able to interact with multiple IT teams and business partners both formally and informally; Should possess sound knowledge of SDLC using Azure DevOps; Self-motivated with the ability to work both independently and in a team environment; Effective time management, ability to multi-task with changing priorities while continuing to provide exceptional client service; Ability to travel as needed, approximately 5%; 12 more items(s)","It’s uncommon for an individual to be hired at the top end of the pay range; This position may be eligible for an annual discretionary bonus; For more information about our benefit offerings and other total rewards, visit our careers page; Compensation Range (Denver Market ONLY): Compensation range for California: $106,000 -$152,000 Compensation range for Washington: $99,000 -$133,000; 1 more items(s)","SQL, Python, Snowflake, Azure, SQL Server 2019, Azure Machine Learning, Data Factory, SSIS, Power BI, T-SQL, DAX, Power Query, Azure DevOps"
"Senior Data Engineer at Fiserv Berkeley, CA",Robert Half,"Robert Half • Torrance, CA •  via Robert Half",58–63 an hour,No Degree Mentioned,"lytics and business intelligence efforts and perform ETL functions.• Data Engineer is responsible for building and maintaining the organization's data infrastructure. This role involves designing, implementing, and managing robust and scalable data pipelines to collect, store, and process large volumes of data efficiently.• The Data Engineer plays a crucial role in data availability and quality to support data analytics and business intelligence efforts within the organization.","6-10 years of overall industry experience, with at least 6+ years' experience in building large-scale big data applications development, and a bachelor’s degree in computer science or a related field; Possess strong technical leadership skills, demonstrating expertise in developing data solutions, building frameworks, and designing solutions for processing large volumes of data using data processing tools and Big Data platforms; Hands-on experience building Data Lake, EDW, and data applications on Azure cloud; Proficiency in major programming/scripting languages like Java, and/or Python; Strong understanding of cluster and parallel architecture, experience with high-scale databases and SQL, and exposure to NoSQL databases like Cassandra, HBase, DynamoDB, and Elastic Search; Experience in Banking, Financial domain; Advanced certifications in data engineering or related fields; Familiarity with machine learning frameworks and data science workflows; Knowledge of containerization technologies like Docker and orchestration tools like Kubernetes; Experience working with PCI Data and collaborating with data scientists; Knowledge of data governance, security, and privacy principle; This role is not eligible to be performed in Colorado, California, Hawaii, New York or Washington; 9 more items(s)","You will be responsible for building and taking ownership over large-scale data engineering, integration, and warehousing projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by using the Data integration tool with coding across several languages such as Java, Python, and SQL; Additional responsibilities include, but are not limited to architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create; Collaborate with cross-functional teams to design scalable data architecture and create robust data processing pipelines; Design and implement data models that align with business requirements, enabling seamless data access and analytics; Identify opportunities to enhance data processing efficiency and implement performance optimizations for our data pipelines; Implement data quality checks and validation processes to ensure the accuracy and integrity of our data; Work closely with data scientists, analysts, and software engineers to understand data needs, provide technical support, and troubleshoot data-related issues; Stay up to date with the latest data engineering technologies and tools and recommend improvements to our existing systems; Maintain comprehensive documentation of data engineering processes, data flows, and system configurations; Conduct code reviews and strive for improvement in software engineering quality; Experience in real-time data processing and streaming technologies like Kafka and Apache Beam, as well as a successful track record in delivering big data projects using Kafka and Spark; 8 more items(s)","Azure, Java, Python, SQL, NoSQL, Cassandra, HBase, DynamoDB, Elastic Search, Docker, Kubernetes, Kafka, Apache Beam, Spark"
Senior Data Engineer (REMOTE),Tenstorrent,"Tenstorrent • Santa Clara, CA •  via Simplify",100K–500K a year,Full-time,"share a passion for AI and a deep desire to build the best AI platform possible. We value collaboration, curiosity, and a commitment to solving hard problems. We are growing our team and looking for contributors of all seniorities.As a Staff Data Engineer in our Data Science and Engineering team, you will help grow the reach and scope of Tenstorrent’s data engineering and data science workflows, including batch processing, streaming, metadata management, and machine learning. The ideal candidate for this role excels at building end-to-end data processing automations running on a modern and scalable hybrid-cloud data infrastructure. You are very experienced in writing Python data processing code, Airflow DAGs, and working with SQL and no-SQL databases. You view self-serve data analytics and ML applications leveraging the curated datasets you produce as customers. You highly appreciate the importance of and are very familiar with IaC, CI, monitoring and logging, security and documentation to achieve the highest levels of automation, quality and functionality.This role is Hybrid, based out of Santa Clara, CA, Austin, TX, or Toronto, ON.Responsibilities:• Help envision and build the future of our data platform and data pipelines. You will have a remarkable impact on the overall tech stack we use to process the rich data available at Tenstorrent.• Design and implement end-to-end data pipelines to process batch and streaming data at small to medium scale, employing cutting-edge data processing and orchestration methodologies, including distributed computing solutions when applicable.• Lead collaboration with data producers and data consumers to identify product requirements and structure effective implementation of solutions.• Lead the integration of a wide range of technologies constituting our data infrastructure. Architect and build new ones when available solutions fall short.• Identify, evolve, and champion data/software engineering best practices, as they best fit a small and agile team.• Help define and implement scalable data governance, security and privacy solutions, including an AI-powered data catalog.• Build automation for IaC.• Contribute to our data engineering roadmap, working cross-functionally to define priorities and establish effective collaborationExperience & Qualifications:• Excellent Python programming skills, with a focus on data processing.• Experience designing, managing and optimizing SQL databases and schemas.• Experience working with AWS, such as S3, EC2, IAM, SFTP, Cloudwatch, Lambda and other services.• Advanced SQL queries and functions programming.• Experience working with no-SQL (Elasticsearch) and time-series databases (TimescaleDB).• Experience building RESTful APIs.• Proficiency in containerization technologies, such as Docker and Kubernetes.• Familiarity with IaC technologies, such as Terraform.• Experience deploying and working with data analytics, dashboarding and visualization tools, such as Apache Superset, Grafana, Kibana.• Experience with distributed processing and querying solutions is a plus, such as Trino and Spark.• Comfortable using Agile development methodologies and industry standard software development lifecycle processes (Jira, Git, documentation).• Self-motivated and independent individual with a strong desire to learn, explore, and excel in a fast-paced, innovative environment.Compensation for all engineers at Tenstorrent ranges from $100k - $500k including base and variable compensation targets. Experience, skills, education, background and location all impact the actual offer made.Tenstorrent offers a highly competitive compensation package and benefits, and we are an equal opportunity employer.Due to U.S. Export Control laws and regulations, Tenstorrent is required to ensure compliance with licensing regulations when transferring technology to nationals of certain countries that have been licensing conditions set by the U.S. government.Our engineering positions and certain engineering support positions require access to information, systems, or technologies that are subject to U.S. Export Control laws and regulations, please note that citizenship/permanent residency, asylee and refugee information and/or documentation will be required and considered as Tenstorrent moves through the employment process.If a U.S. export license is required, employment will not begin until a license with acceptable conditions is granted by the U.S. government. If a U.S. export license with acceptable conditions is not granted by the U.S. government, then the offer of employment will be rescinded.","Technical Skills Expert in SQL and/or SQL based languages and performance tuning of SQL queries Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques; Strong Experience with cloud-based data warehouses – e.g., Snowflake, Big Query, Synapse, RedShift, etc; Experienced with ETL/ELT in Databricks, with Medallion architecture and with Delta Lake, Unity Catalog, Delta Sharing, Delta Live Tables (DLT); Experience with CI/CD on Databricks using tools such as GitHub Actions, and Databricks CLI; Strong Grasp of data management principles: Data Lake, Data Mesh, Data Catalog, Data Quality, etc; QUALIFICATIONS: 5+ years of experience in Data Warehousing and development using data technologies such as Relational & NoSQL databases, open data formats, building data pipelines (ETL and ELT) with batch or streaming ingestion, loading and transforming data; Expert in SQL and/or SQL based languages and performance tuning of SQL queries Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques; Strong experience working with ETL/ELT concepts of data integration, consolidation, enrichment, and aggregation in petabyte scale data sets; Experience with message queuing, stream processing (Kafka, Flink, Spark Streams) Strong Grasp of data management principles: Data Lake, Data Mesh, Data Catalog, Master Data, Data Quality, etc; Experience in BI tooling such as Qlik, MicroStrategy, Tableau, PowerBI or Looker Experience with orchestration tools (Control-M, Airflow etc.); Strong communication skills across different mediums to craft compelling messages to drive action and alignment; Comfort with agile delivery methodologies in a fast-paced complex environment – Scrum, SAFe, utilizing tools such as Jira, Confluence, and GitHub Ideal candidates will have experience working with one of the following industries: Retail, Supply Chain, Logistics, Manufacturing or Marketing Proficient in Linux/Unix environments #LI-JN1 Targeted Pay Range: $83,000 - $138,200; 9 more items(s)","This is part of a competitive total rewards package that could include other components such as: incentive, equity and benefits; Individual pay is determined by a number of factors including experience, location, internal pay equity, and other relevant business considerations; We review all teammate pay regularly to ensure competitive and equitable pay; DICK'S Sporting Goods complies with all state paid leave requirements; We also offer a generous suite of benefits; 2 more items(s)","Python, Airflow, SQL, no-SQL databases, AWS (S3, EC2, IAM, SFTP, Cloudwatch, Lambda), Elasticsearch, TimescaleDB, Docker, Kubernetes, Terraform, Apache Superset, Grafana, Kibana, Trino, Spark, Jira, Git, Snowflake, BigQuery, Synapse, RedShift, Databricks, Delta Lake, Unity Catalog, Delta Sharing, Delta Live Tables (DLT), GitHub Actions, Databricks CLI, Kafka, Flink, Spark Streams, Qlik, MicroStrategy, Tableau, PowerBI, Looker, Control-M"
"AWS Data Engineer (Mountainview, CA; )","DICK'S Sporting Goods, Inc.","DICK'S Sporting Goods, Inc. • Remote, OR •  via Workday",5 days ago,Full-time,"ts team, apply to join our team today! OVERVIEW: At DICK’S Sporting Goods, we believe in how positively sports can change lives. On our team, everyone plays a critical role in creating confidence and excitement by personally equipping all athletes to achieve their dreams. We are committed to creating an inclusive and diverse workforce, reflecting the communities we serve. We are creating the future of sports, driven by powerful data products and platforms that serve our Athletes and Teammates. We are looking for a Senior Data Engineer to join our passionate team, adding your background and experience to make us even stronger. In this role, you will build dataset and make it accessible to our partner teams by writing great code to simplify the complexity and ensure quality. Your work will enable product teams, data scientists, and decision-makers across the company to bring together insights and inform our business. We believe that trusted, easy to consume data is critical and as a Senior Data Engineer your work will help to build that foundation. You will also be responsible for the daily operations inclusive of troubleshooting and job monitoring. You will be a part of the growing Data team reporting to the Sr. Director, Data Analytics. The impact you will have: Design/Strategy: You will design and support the business’s database and table schemas for new and existing data sources for the data warehouse. Creates and supports the ETL to facilitate data accommodation into the warehouse. In this capacity, the Data Engineer designs and develops systems for the maintenance of the business’s data warehouse, ETL processes, and business intelligence. Collaboration: You will be collaborative - working closely with analysts, data scientists, and other data consumers within the business to gather and deliver high quality data for business cases. The Data Engineer also works closely with other disciplines/departments and teams across the business in coming up with simple, functional, and elegant solutions that balance data needs across the business Analytics: You will play an analytical role in quickly and thoroughly analyzing business requirements and subsequently translating the emanating results into good technical data designs. In this capacity, the Data Engineer establishes the documentation of the data solutions, develops, and maintains technical specification documentation for all reports and processes. What You Will Do You’ll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needs Your work will provide teams with visibility into how DICKs products are being used and how we can better serve our customers Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across DICKs. Experience in one or more of the following: Python (Preferred), Scala, C++, or Java. Design, develop, reliable data models and extremely efficient pipelines to build quality data and provide intuitive analytics to our partner teams. Help the Data Analytics & Data Science team apply and generalize statistical and econometric models on large datasets Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the DICKs product evolves Develop strong subject matter expertise and manage the SLAs for those data pipelines Participate in design sessions and code reviews to elevate the quality of data engineering across the organization. Participate in an on-call rotation for support during and after business hours. Lead design sessions and code reviews to elevate the quality of data engineering across the organization. Technical Skills Expert in SQL and/or SQL based languages and performance tuning of SQL queries Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques. Experience in one or more of the programming languages are required: Python (Preferred), Scala, C++, or Java, Go, Kotlin. Strong Experience with cloud-based data warehouses – e.g., Snowflake, Big Query, Synapse, RedShift, etc. Experienced with ETL/ELT in Databricks, with Medallion architecture and with Delta Lake, Unity Catalog, Delta Sharing, Delta Live Tables (DLT). Experience with CI/CD on Databricks using tools such as GitHub Actions, and Databricks CLI. Strong Grasp of data management principles: Data Lake, Data Mesh, Data Catalog, Data Quality, etc. QUALIFICATIONS: 5+ years of experience in Data Warehousing and development using data technologies such as Relational & NoSQL databases, open data formats, building data pipelines (ETL and ELT) with batch or streaming ingestion, loading and transforming data. Expert in SQL and/or SQL based languages and performance tuning of SQL queries Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques. Experience in one or more of the programming languages are required: Python (Preferred), Scala, C++, or Java, Go, Kotlin. Strong experience working with ETL/ELT concepts of data integration, consolidation, enrichment, and aggregation in petabyte scale data sets. Experience with at least one of the following cloud platforms: Microsoft Azure (Preferred), Amazon Web Services (AWS), or Google Cloud Platform (GCP) Strong Experience with cloud-based data warehouses – e.g., Snowflake, Big Query, Synapse, RedShift, etc. Experience with message queuing, stream processing (Kafka, Flink, Spark Streams) Strong Grasp of data management principles: Data Lake, Data Mesh, Data Catalog, Master Data, Data Quality, etc. Experience in BI tooling such as Qlik, MicroStrategy, Tableau, PowerBI or Looker Experience with orchestration tools (Control-M, Airflow etc.) Strong communication skills across different mediums to craft compelling messages to drive action and alignment. Comfort with agile delivery methodologies in a fast-paced complex environment – Scrum, SAFe, utilizing tools such as Jira, Confluence, and GitHub Ideal candidates will have experience working with one of the following industries: Retail, Supply Chain, Logistics, Manufacturing or Marketing Proficient in Linux/Unix environments #LI-JN1 Targeted Pay Range: $83,000 - $138,200. This is part of a competitive total rewards package that could include other components such as: incentive, equity and benefits. Individual pay is determined by a number of factors including experience, location, internal pay equity, and other relevant business considerations. We review all teammate pay regularly to ensure competitive and equitable pay.DICK'S Sporting Goods complies with all state paid leave requirements. We also offer a generous suite of benefits. To learn more, visit www.benefityourliferesources.com. ACHIEVING SUCCESS AT DICK’S SPORTING GOODS Everything we do is focused on helping athletes achieve their dreams. Accomplishing this takes a team of people committed to being their best. That means learning, growing, advancing and exploring new experiences. Advancing in our career is much like a climbing wall; it isn't always a vertical path. Climbers plan their path, make moves in every direction and reach further. Whatever direction you want to explore, we encourage you to be brave, curious and hungry for new experiences. We are the athletes, the coaches, the energy in the stands, and the fans that fill them. The referees that call the games and the neighbor that can’t wait until the next one. The sports enthusiasts that wake up and strive to reach our personal best. We are DICK’S Sporting Goods – a community united by our passion for sport, standing together in our pursuit of our best. What we bring to the game is what we believe will change it. By upholding the values that strengthen our performance, fosters collaboration, and ignites our desire to win, we actively demonstrate the impact and change we inspire when we work as one.","Technical Skills Expert in SQL and/or SQL based languages and performance tuning of SQL queries Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques; Strong Experience with cloud-based data warehouses – e.g., Snowflake, Big Query, Synapse, RedShift, etc; Experienced with ETL/ELT in Databricks, with Medallion architecture and with Delta Lake, Unity Catalog, Delta Sharing, Delta Live Tables (DLT); Experience with CI/CD on Databricks using tools such as GitHub Actions, and Databricks CLI; Strong Grasp of data management principles: Data Lake, Data Mesh, Data Catalog, Data Quality, etc; QUALIFICATIONS: 5+ years of experience in Data Warehousing and development using data technologies such as Relational & NoSQL databases, open data formats, building data pipelines (ETL and ELT) with batch or streaming ingestion, loading and transforming data; Expert in SQL and/or SQL based languages and performance tuning of SQL queries Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques; Strong experience working with ETL/ELT concepts of data integration, consolidation, enrichment, and aggregation in petabyte scale data sets; Experience with message queuing, stream processing (Kafka, Flink, Spark Streams) Strong Grasp of data management principles: Data Lake, Data Mesh, Data Catalog, Master Data, Data Quality, etc; Experience in BI tooling such as Qlik, MicroStrategy, Tableau, PowerBI or Looker Experience with orchestration tools (Control-M, Airflow etc.); Strong communication skills across different mediums to craft compelling messages to drive action and alignment; Comfort with agile delivery methodologies in a fast-paced complex environment – Scrum, SAFe, utilizing tools such as Jira, Confluence, and GitHub Ideal candidates will have experience working with one of the following industries: Retail, Supply Chain, Logistics, Manufacturing or Marketing Proficient in Linux/Unix environments #LI-JN1 Targeted Pay Range: $83,000 - $138,200; 9 more items(s)","This is part of a competitive total rewards package that could include other components such as: incentive, equity and benefits; Individual pay is determined by a number of factors including experience, location, internal pay equity, and other relevant business considerations; We review all teammate pay regularly to ensure competitive and equitable pay; DICK'S Sporting Goods complies with all state paid leave requirements; We also offer a generous suite of benefits; 2 more items(s)","Snowflake, Big Query, Synapse, RedShift, Databricks, Delta Lake, Unity Catalog, Delta Sharing, Delta Live Tables (DLT), GitHub Actions, Databricks CLI, Kafka, Flink, Spark Streams, Qlik, MicroStrategy, Tableau, PowerBI, Looker, Control-M, Airflow, Jira, Confluence, GitHub"
Staff Data Engineer (IRIS),CEDENT,"CEDENT • Mountain View, CA •  via ZipRecruiter",2 days ago,Contractor,"AWS Redshift as a data warehousing solution Advanced knowledge in designing, developing, implementing and managing data pipelines to deliver data or data insights for application, reporting, or analytics Strong experience creating and maintaining functional and technical specifications documents Strong experience creating test plans, test data sets, and automated testing ot ensure all components of the system meet specifications Strong SQL technical experience such as linking IT applications to databases and creating and handling metadata Strong programming skill in Python or Scala Bonus skills: Strong experience in NoSQL database (i.e., MongoDB) Strong experience in streaming technology (i.e., Kafka, data bricks streaming) Strong experience in working in the healthcare industry including PHI, HIPAA regulations, and BAA processes ? AWS DATA ENGINEER RESPONSIBILITIES Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics Support the creation of the new cloud infrastructure and data ecosystem in the cloud Department: Preferred Vendors This is a contract position","5+ years experience with at least 1 development language like JAVA, NodeJS; 3+ year working on REST services and API Manager like Apigee, Vordel, Mulesoft, Kong API Gateway; 2+ years of hands-on and robust experience in Python, Unix Shell scripting, SQL and handling of JSON, XML data; 4+ years of experience in building and deploying applications in GCP; 3+ years of experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT); Strong communication skills are required as the position will interact with business owners; Working in a fluid environment, defining, and owning priorities that adapt to our larger goals; Bachelor's degree or equivalent work experience in Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering, or related discipline; 5 more items(s)","Pay Range; $130,295.00 – $260,590.00; This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls; The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors; This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above; This position also includes an award target in the company’s equity award program; In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities; The Company offers a full range of medical, dental, and vision benefits; Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees; The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits; CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners; As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year; Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies; 10 more items(s)","AWS Redshift, Python, Scala, NoSQL, MongoDB, Kafka, JAVA, NodeJS, Apigee, Vordel, Mulesoft, Kong API Gateway, Unix Shell, SQL, JSON, XML, GCP (Google Cloud Platform), Jenkins, GIT"
Azure Data Engineer ADF - Maharashtra,CVS Health Corporation,"CVS Health Corporation • Sacramento, CA •  via Data Engineering Jobs",26 days ago,"130,295 a year","loying applications in GCP• 3+ years of experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT)• Strong communication skills are required as the position will interact with business owners• Working in a fluid environment, defining, and owning priorities that adapt to our larger goals. You can bring clarity to ambiguity while remaining open-minded to new information.Preferred Qualifications• GCP engineering technologies such as Google Kubernetes Engine, Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, and Health care API (FHIR store)• Knowledge and Experience in Big Query is strongly preferable.• Experience working with health care data and understanding of analytics and how it is leveraged within the healthcare industries.• Prior experience creating FHIR API, oAuth2 authentication, or mapping healthcare data to FHIR resources will be a strong plus• GCP Cloud Architect Or GCP Data engineering certification.EducationBachelor's degree or equivalent work experience in Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering, or related discipline.Master's degree strongly preferred.Pay RangeThe typical pay range for this role is:$130,295.00 – $260,590.00This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above. This position also includes an award target in the company’s equity award program.In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefitsWe anticipate the application window for this opening will close on: 08/13/2024Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.","5+ years experience with at least 1 development language like JAVA, NodeJS; 3+ year working on REST services and API Manager like Apigee, Vordel, Mulesoft, Kong API Gateway; 2+ years of hands-on and robust experience in Python, Unix Shell scripting, SQL and handling of JSON, XML data; 4+ years of experience in building and deploying applications in GCP; 3+ years of experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT); Strong communication skills are required as the position will interact with business owners; Working in a fluid environment, defining, and owning priorities that adapt to our larger goals; Bachelor's degree or equivalent work experience in Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering, or related discipline; 5 more items(s)","Pay Range; $130,295.00 – $260,590.00; This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls; The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors; This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above; This position also includes an award target in the company’s equity award program; In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities; The Company offers a full range of medical, dental, and vision benefits; Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees; The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits; CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners; As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year; Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies; 10 more items(s)","Jenkins, GIT, Google Kubernetes Engine, Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, Health care API (FHIR store), JAVA, NodeJS, Apigee, Vordel, Mulesoft, Kong API Gateway, Python, Unix Shell scripting, SQL"
Data Scientist and AI Engineer,Fugetron Corporation,"Fugetron Corporation • Los Angeles, CA •  via Glassdoor",18 days ago,Full-time,"we believe that work should be meaningful and fulfilling. As part of our team, you’ll be at the forefront of cutting-edge solutions, enabling businesses to thrive in the digital age, embracing the challenges with top talent finding purpose in solving complex problems, and making a difference to our enterprise clients.Empowering InnovationInnovation is in our DNA. We foster a culture of continuous learning and encourage our team members to push the boundaries of technology. Whether you’re a seasoned expert or just starting your career, you’ll have access to a wealth of resources, training programs, and mentorship opportunities to expand your skillset.Teamwork Makes the Dream WorkSuccess at Fugetron is a team effort. Join a community of like-minded individuals who are passionate about technology sharing a common goal of delivering exceptional IT services. We believe in the power of collaboration, diverse perspectives, and supportive teamwork. Together, we celebrate achievements and overcome challenges creating an environment where everyone can thrive.Work-Life BalanceA healthy work-life balance is essential for personal well-being and professional growth. At Fugetron, we prioritize the well-being of our team members. Enjoy flexible work arrangements, opportunities for remote work, and comprehensive benefits that cater to your needs. Take the time to pursue your passions outside of work, and bring your best self to the table.Career AdvancementYour career growth matters to us. We are committed to nurturing talent from within and offering a clear path for advancement. Through personalized development plans and exposure to diverse projects, you’ll have the opportunity to progress your career within the organization. Join us, and unlock a world of possibilities for your future in the IT services industry.Azure Data Engineer ADF - MaharashtraDevelops and maintains scalable Azure Data Factory (ADF) data pipelines and/or builds out new API integrations to support continuing increases in data volume and complexity across our source data systems.Collaborates with data modelers, data scientists, business analysts and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Writes unit/integration tests and documents work.Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.Designs data integrations and data quality framework.REQUIREMENTS: -Experience designing, building, and deploying scalable Azure cloud-based solution architectures.Minimum 5+ years of relevant hands-on experience in Data Ingestion, Integration and TransformationStrong working knowledge of ETL, BI, Data Lake and Data warehousing tools and technologiesStrong experience with data systems and infrastructure on Azure, especially Azure Data Factory (ADF)Knowledge of Azure Synapse Analytics, Azure Data Lake, Azure SQL, Azure Databricks or equivalent tools and technologiesStrong Expertise in SQL and PythonKnowledge of streaming mechanismsExperience with error exception handling mechanisms, monitoring and alerting techniquesKnowledge of Azure DevOps, or Continuous Integration/DeliveryExperience with FiveTran for SalesForce IntegrationKnowledge of containerization and container orchestration technologies.Familiarity with standard IT security practices such as identity and access management, SSO, data protection, encryption, certificate, and key managementPerform data analysis to support strategic initiatives and clearly communicate and present insights to leadership.Strong analytical problem-solving skillsExcellent written and verbal communication skills.","Experience designing, building, and deploying scalable Azure cloud-based solution architectures; Minimum 5+ years of relevant hands-on experience in Data Ingestion, Integration and Transformation; Strong working knowledge of ETL, BI, Data Lake and Data warehousing tools and technologies; Strong experience with data systems and infrastructure on Azure, especially Azure Data Factory (ADF); Knowledge of Azure Synapse Analytics, Azure Data Lake, Azure SQL, Azure Databricks or equivalent tools and technologies; Strong Expertise in SQL and Python; Knowledge of streaming mechanisms; Experience with error exception handling mechanisms, monitoring and alerting techniques; Knowledge of Azure DevOps, or Continuous Integration/Delivery; Experience with FiveTran for SalesForce Integration; Knowledge of containerization and container orchestration technologies; Familiarity with standard IT security practices such as identity and access management, SSO, data protection, encryption, certificate, and key management; Perform data analysis to support strategic initiatives and clearly communicate and present insights to leadership; Strong analytical problem-solving skills; Excellent written and verbal communication skills; 12 more items(s)","Whether you’re a seasoned expert or just starting your career, you’ll have access to a wealth of resources, training programs, and mentorship opportunities to expand your skillset; Work-Life Balance; A healthy work-life balance is essential for personal well-being and professional growth; Enjoy flexible work arrangements, opportunities for remote work, and comprehensive benefits that cater to your needs; Career Advancement; Through personalized development plans and exposure to diverse projects, you’ll have the opportunity to progress your career within the organization; 3 more items(s)","Azure Data Factory (ADF), Azure Synapse Analytics, Azure Data Lake, Azure SQL, Azure Databricks, SQL, Python, Azure DevOps, FiveTran for SalesForce Integration"
Lead Data Engineer,KPI Partners,"KPI Partners • Dublin, CA •  via LinkedIn",120K–160K a year,Contractor,"r and Architect - Designs, develops and implements AWS eco-system-based data pipeline/engineering applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation. Experience/Skills Required:• Expert in Cloud Big Data Solutions using AWS, Snowflake, and SQL. DBT preferred and highly desirable.• Experience in GitLab and CI/CD implementation in Data Engineering• Expert in Data modeling and Data Warehousing• Understand and convert business requirements into Data Models & Schema Designs• Ability to communicate well with business users to understand needs and requirements, IT Stakeholders to understand data sources and access, and manage client expectations and delivery","Expert in Cloud Big Data Solutions using AWS, Snowflake, and SQL","Senior Hands-on Data Engineer and Architect - Designs, develops and implements AWS eco-system-based data pipeline/engineering applications to support business requirements; Follows approved life cycle methodologies, creates design documents, and performs program coding and testing; Resolves technical issues through debugging, research, and investigation","AWS, Snowflake, SQL, DBT, GitLab, CI/CD"
"Senior Azure Data engineers at Starcom consulting limited Los Angeles, CA",Inland Empire Health Plan,"Inland Empire Health Plan • Rancho Cucamonga, CA •  via DevITjobs","118,602–157,144 a year",Full-time,"ing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse, and Cosmos DB.- Eight (8) years implementing software development methodologies.- Bachelor’s degree in a quantitative discipline such as Computer Science, Statistics, Mathematics, or Engineering from an accredited institution required.- Strong knowledge and understanding in areas such as DevOps, Python or Java or Json, Applicable data privacy practices and laws, common SDLC models, Non-relational database (NoSQL) designs, Relational databases like MS SQL Server, and more.👩‍💻👨‍💻 Your responsibilities are:Under the direction of the Department Leadership, the Data Engineer III is responsible for the design, planning, and development of IEHP data solutions. The Data Engineer III will lead the design and development of data transformation, be involved in data architecture, and is expected to lead by example in following coding standards throughout all aspects of solution development. Additionally, collaboration with inter-departments to ensure member needs are met while simultaneously building strong peer relationships is part of the role.View this job and over 500 other transparent jobs with salaries (💰💰💰) & tech stacks (🛠️) on DevITjobsCategory: Data Developer / EngineerLocation address: Civic Center Drive 10500, San-Bernardino, United StatesSalary: 118,602 - 157,144 USD per yearBenefits & perks that we offer:Inland Empire Health Plan - More about us and the role:This position will entail a hybrid work schedule with remote work on Mondays and Fridays and onsite work required on Tuesdays, Wednesdays, and Thursdays. The compensation package includes bonuses and a competitive salary range. Additionally, the company offers various benefits such as a 401(k), health insurance, parental leave, professional development assistance, and more. The job location is in Rancho Cucamonga, CA, and candidates must be willing to relocate if not already based in the area.Are you looking for Data jobs in San-Bernardino?","Minimum of eight (8) years of experience in provisioning, configuring, and developing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse, and Cosmos DB; Eight (8) years implementing software development methodologies; Bachelor’s degree in a quantitative discipline such as Computer Science, Statistics, Mathematics, or Engineering from an accredited institution required; Strong knowledge and understanding in areas such as DevOps, Python or Java or Json, Applicable data privacy practices and laws, common SDLC models, Non-relational database (NoSQL) designs, Relational databases like MS SQL Server, and more; 1 more items(s)","Salary: 118,602 - 157,144 USD per year; This position will entail a hybrid work schedule with remote work on Mondays and Fridays and onsite work required on Tuesdays, Wednesdays, and Thursdays; The compensation package includes bonuses and a competitive salary range; Additionally, the company offers various benefits such as a 401(k), health insurance, parental leave, professional development assistance, and more; 1 more items(s)","Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse, Cosmos DB, Python, Java, Json, MS SQL Server"
Senior Devops/ Data Engineer,Starcom consulting limited,"Starcom consulting limited • Los Angeles, CA •  via StarPoint Screening",16 days ago,Full-time,"uired• Large scale data migrations• Azure synapse over many projects• Pyspark over many projectsFor a senior role involving Azure and Databricks, especially with a focus on large-scale data migrations and working with Azure Synapse and PySpark, you'll want to pursue certifications and experiences that align with these requirements. Here's a breakdown of what you should consider:### Azure Certifications1. **Azure Solutions Architect Expert**& && & This is the most relevant certification for a senior role involving architectural design. It covers advanced topics such as infrastructure, security, and solutions design. The relevant exams are:& & - **Exam AZ-305: Designing Microsoft Azure Infrastructure Solutions**&2. **Azure Data Engineer Associate**& && & This certification could also be valuable given the focus on data migration and management. It covers data storage, data processing, and data security. The relevant exam is:& & - **Exam DP-203: Data Engineering on Microsoft Azure**### Databricks Certifications1. **Databricks Certified Data Engineer Associate**& && & This certification demonstrates foundational knowledge in data engineering and is useful for working with Databricks.2. **Databricks Certified Data Engineer Professional**& && & For a senior role, this certification would be more appropriate as it covers more advanced topics and practical skills.3. **Databricks Certified Machine Learning Professional**& && & If your role involves machine learning, this certification can be beneficial.& & **OR**4. **Oil and Gas Industry Experience**& && & Since you can substitute Databricks certification with relevant industry experience, having a strong background in the oil and gas sector with data engineering or analytics roles would be valuable.### Experience1. **Large-Scale Data Migrations**& && & Demonstrating experience with large-scale data migrations is crucial. Be prepared to discuss specific projects where you’ve managed or contributed to substantial data migration efforts.2. **Azure Synapse Analytics**& && & Experience with Azure Synapse is critical, especially if you've worked on multiple projects involving this technology. Highlight your familiarity with its integration with other Azure services and how you've leveraged it in past projects.3. **PySpark**& && & Given the requirement to have experience with PySpark, ensure you have significant hands-on experience with it. Being able to discuss various projects where you’ve used PySpark for data processing, transformation, or analytics will be important.### SummaryTo be well-prepared for this senior role:- **Achieve** the **Azure Solutions Architect Expert** certification and consider the **Azure Data Engineer Associate** certification.- **Obtain** relevant Databricks certifications, ideally the **Certified Data Engineer Professional** or **Certified Machine Learning Professional**.- **Showcase** your experience with large-scale data migrations, Azure Synapse, and PySpark through detailed project descriptions and results in your resume or during interviews.Balancing these certifications and experiences will position you strongly for a senior role in this domain.","NEED A SUPER SENIOR CANDIDATE& WITH ENERGY CLIENT EXPERIENCE IF POSSIBLE; Azure certifications (architect certification needed for this senior role); Databricks certifications ( can have EITHER Databrick cert OR Oil and Gas Experience)& One is Required; Large scale data migrations; For a senior role involving Azure and Databricks, especially with a focus on large-scale data migrations and working with Azure Synapse and PySpark, you'll want to pursue certifications and experiences that align with these requirements; ### Azure Certifications; **Azure Solutions Architect Expert**& &; & & This is the most relevant certification for a senior role involving architectural design; It covers advanced topics such as infrastructure, security, and solutions design; & & This certification could also be valuable given the focus on data migration and management; & & - **Exam DP-203: Data Engineering on Microsoft Azure**; ### Databricks Certifications; **Databricks Certified Data Engineer Associate**& &; & & This certification demonstrates foundational knowledge in data engineering and is useful for working with Databricks; **Databricks Certified Data Engineer Professional**& &; & & For a senior role, this certification would be more appropriate as it covers more advanced topics and practical skills; **Databricks Certified Machine Learning Professional**& &; **Oil and Gas Industry Experience**& &; & & Since you can substitute Databricks certification with relevant industry experience, having a strong background in the oil and gas sector with data engineering or analytics roles would be valuable; **Large-Scale Data Migrations**& &; & & Demonstrating experience with large-scale data migrations is crucial; & & Experience with Azure Synapse is critical, especially if you've worked on multiple projects involving this technology; Highlight your familiarity with its integration with other Azure services and how you've leveraged it in past projects; & & Given the requirement to have experience with PySpark, ensure you have significant hands-on experience with it; Being able to discuss various projects where you’ve used PySpark for data processing, transformation, or analytics will be important; **Achieve** the **Azure Solutions Architect Expert** certification and consider the **Azure Data Engineer Associate** certification; **Obtain** relevant Databricks certifications, ideally the **Certified Data Engineer Professional** or **Certified Machine Learning Professional**; **Showcase** your experience with large-scale data migrations, Azure Synapse, and PySpark through detailed project descriptions and results in your resume or during interviews; Balancing these certifications and experiences will position you strongly for a senior role in this domain; 26 more items(s)","Azure synapse over many projects; Pyspark over many projects; & & - **Exam AZ-305: Designing Microsoft Azure Infrastructure Solutions**&; **Azure Data Engineer Associate**& &; It covers data storage, data processing, and data security; Be prepared to discuss specific projects where you’ve managed or contributed to substantial data migration efforts; **Azure Synapse Analytics**& &; 4 more items(s)","Azure Synapse, PySpark"
Data Engineer Sr,Noralogic Inc,"Noralogic Inc • Santa Clara, CA •  via LinkedIn",Full-time,No Degree Mentioned,"sk/Fast/Django API, Middleware, Scheduler, SQL, Databases.• Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).• Good to have some experience in AWS/Azure.• Capability of developing highly-scalable RESTful APIs.• Excellent team player and can work well in an individual capacity as well.• Detail-oriented and possess strong analytical skills.• Pays strong attention to detail and delivers work that is of a high standard.• Highly goal-driven and work well in fast-paced environments. Location - Santa ClaraRegards,Ajay DuttaTechnical RecruiterNoralogic Inc.109 E 17th St, Cheyenne WY 82001+1. (307) 274-3105| (Ajay@noralogic.com) |www.noralogic.comUSA: WY, MD, NJwww.noralogic.comMexico: Guadalajara, MonterreyIndia: Noida UP• *WBE and MBE company**• * ISO 9001:2015**• *WY Top 50 Minority owned growing company**","NOTE: Candidate must be ready for coding test for this role; Client data engineer with at least 4-7 years' experience, ideally within a Data Engineer role; Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data; Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases; Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.); Good to have some experience in AWS/Azure; Capability of developing highly-scalable RESTful APIs; Excellent team player and can work well in an individual capacity as well; Detail-oriented and possess strong analytical skills; Pays strong attention to detail and delivers work that is of a high standard; Highly goal-driven and work well in fast-paced environments; 8 more items(s)","Bachelor's degree in computer science, data science, or a related field with five (5) or more years of working as a data engineer, ETL developer, and/or data warehouse DBA; Experience with Cloud-based data services and solutions (Azure Synapse / Data Lake, AWS RedShift, Snowflake, GCP Big Query); Experience partnering with Analytics and Data Science teams in building out production-grade GenAI/ML solutions; Familiarity with Change Data Capture solutions (Qlik Replicate, Striim, Debeezium, etc.); Experience with big data tools and architectures, such as Cloudera Hadoop, HDFS, Hive, and Spark; Working knowledge of telematics interfaces and streaming solutions (MQTT, NiFi, Kafka, etc.); Automotive or heavy-duty on or off-road vehicle, digital data bus, including Ethernet or Controller Area Network (CAN) experience; Highly organized and detail-oriented, with strong critical thinking, analytical, and problem-solving skills; Ability to handle multiple tasks in a fast-paced environment, both independently and as part of a team; Display excellent interpersonal skills and the ability to effectively present information and respond to questions from leadership and peers; Strong proficiency with SQL, Bash, and Python (and experience with the Anaconda distribution); 8 more items(s)","Django API, Middleware, Scheduler, SQL, Databases, Python, Django, Flask, AWS, Azure, Pandas, Cloud-based data services, Azure Synapse, Data Lake, AWS RedShift, Snowflake, GCP Big Query, Qlik Replicate, Striim, Debeezium, Cloudera Hadoop, HDFS, Hive, Spark, MQTT, NiFi, Kafka, Bash, Anaconda distribution"
Decision Support Analyst (Data Engineer),Avanade,"Avanade • Los Angeles, CA •  via The Muse",25 days ago,Full-time,"ION LOCATION: 1003 E 4th Place, 8th Floor, Los Angeles, CA 90013SALARY: $121,368.00 to $128,500.00/ year(40 hours per week)JOB DESCRIPTION: [MULTIPLE POSITIONS]: Avanade Inc. is seeking Azure Data Engineers in Los Angeles, CA to deliver analytics and business intelligence solutions to our clients. In this role, you will: design and set-up projects that bring together information from a variety of sources to enable analysis and decision-making; utilize personal knowledge to plan and deliver data warehouse and storage; take part in designing and running bespoke data services to individual projects; design, develop, adapt, and maintain data warehouse architecture and relational databases to support data mining; customize storage and extraction, meta-data, and information repositories; create and use effective metrics and monitoring processes; monitor key performance indicators to determine where current operations can be improved and identify the causes of any problems; create the building blocks for transforming enterprise data solutions; design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs); and create and maintain report forms and formats, information dashboards, data generators, and scanned reports, as well as other information portals and resources. Periodic travel to client sites (domestic) is expected up to 80% of the time. Must live w/in reasonable commuting distance of Los Angeles, CA. Benefits information available here: https://www.avanade.com/en/career/benefitsREQUIREMENTS: The position requires at least a bachelor's degree, or its foreign equivalent, in Computer Science, Applied Mathematics, Statistics, or Machine Learning, or closely related field, and at least three (3) years of experience as an Azure Data Engineer, or in a similar role in a technology or IT consulting environment. **Alternatively, in lieu of a bachelor's degree and three (3) years of experience, the employer will accept no degree and five (5) years of experience as an Azure Data Engineer, or in a similar role in a technology or IT consulting environment.** A qualified applicant's experience must include: 3 years of experience performing data engineering, warehousing, publishing and visualization throughout the full data lifecycle; 3 years of experience defining ETA architecture and ETL process design; 3 years of experience performing end-to-end implementation of data warehousing analytics solutions built on MS or Azure platforms; 2 years of experience working with Azure; and 3 years of experience using DPM specific tooling, including Python, Databricks, Azure Synapse, SQL Server, Azure Data Lake, and/or Azure Data Factory (ADF).Applicants who are interested in this position should select ""Apply"" below.If offered employment, must have legal right to work in U.S. EOE.This notice is being provided as a result of the filing of a permanent alien labor certification application for this job opportunity. Any person may provide documentary evidence bearing on the application to the Certifying Officer of the Department of Labor at:U.S. Department of LaborEmployment and Training AdministrationOffice of Foreign Labor Certification200 Constitution Avenue, NWRoom N-5311Washington, DC 20210","Strong technical skills in mainframe, PC, and Internet-based technologies; Proficiency in report generation products such as Power BI; Ability to develop programs using SQL, SAS, and ETL tools like Alteryx or Dataiku; Excellent communication skills, comfortable presenting findings to senior management; Strong analytical and problem-solving abilities; 3+ years of relevant experience in Data Engineering or related field; Experience and demonstrated proficiency as a Data Engineer or equivalent training in a business or clinical environment; Bachelor’s Degree in Computer/Information Science, Mathematics/Statistics, Economics, or related field; Relevant experience may be considered in lieu of a bachelor’s degree; 3 years of progressive experience in Data Engineering or related field; Experience in applying technology in a business or clinical environment; 8 more items(s)","Competitive salary ranging from \(50,200.00 to \)90,300.00 based on experience; Health, dental, and vision insurance; Retirement savings plan with company matching; Generous vacation and paid time off policy; Opportunities for professional development and continuous learning; Flexible working hours; 3 more items(s)","Azure Synapse, SQL Server, Azure Data Lake, Azure Data Factory (ADF), Power BI, SQL, SAS, Alteryx, Dataiku"
Data Engineer Lead,Highmark Health,"Highmark Health • Carson City, NV •  via Get.It",4 days ago,Full-time,"utilize appropriate decision support and reporting tools. You will also recommend innovative approaches to analytical investigations and perform modeling of data sources and flows, collaborating with data management staff to define an operational framework that ensures precise and secure delivery of knowledge and information.Compensation and Benefits• Competitive salary ranging from \(50,200.00 to \)90,300.00 based on experience• Health, dental, and vision insurance• Retirement savings plan with company matching• Generous vacation and paid time off policy• Opportunities for professional development and continuous learning• Flexible working hoursWhy you should apply for this position todayThis position offers a unique opportunity to significantly impact data-driven decision-making within a leading organization. You will collaborate with a diverse range of internal and external partners, contributing to innovative solutions that enhance business operations. Joining our team means being part of a supportive environment that values mentorship and professional growth.Skills• Strong technical skills in mainframe, PC, and Internet-based technologies• Proficiency in report generation products such as Power BI• Ability to develop programs using SQL, SAS, and ETL tools like Alteryx or Dataiku• Excellent communication skills, comfortable presenting findings to senior management• Strong analytical and problem-solving abilitiesResponsibilities• Develop and foster internal relationships across the corporation• Provide analytic and consultative support to internal customer areas and external business partners• Mentor colleagues on technical and business aspects of projects• Determine how decision support systems will provide necessary information for business decisions• Propose solutions to complex business problems and lead implementation efforts• Define issues and methodologies for studies and projects• Perform modeling of data sources and flows, ensuring secure delivery of information• Assist in interpreting raw data and preparing reports or presentations• Develop and maintain complex computer programs using 4GL programming languagesQualifications• 3+ years of relevant experience in Data Engineering or related field• Experience and demonstrated proficiency as a Data Engineer or equivalent training in a business or clinical environmentEducation Requirements• Bachelor’s Degree in Computer/Information Science, Mathematics/Statistics, Economics, or related fieldEducation Requirements Credential Category• Relevant experience may be considered in lieu of a bachelor’s degreeExperience Requirements• 3 years of progressive experience in Data Engineering or related field• Experience in applying technology in a business or clinical environmentWhy work in Carson City, NVCarson City offers a unique blend of natural beauty, rich history, and a vibrant community. With its proximity to stunning outdoor recreational areas, residents enjoy a variety of activities year-round. The city provides a welcoming environment for families and professionals alike, with a lower cost of living compared to larger metropolitan areas. Carson City is also home to a growing tech scene, offering numerous opportunities for career advancement and professional development.","Strong technical skills in mainframe, PC, and Internet-based technologies; Proficiency in report generation products such as Power BI; Ability to develop programs using SQL, SAS, and ETL tools like Alteryx or Dataiku; Excellent communication skills, comfortable presenting findings to senior management; Strong analytical and problem-solving abilities; 3+ years of relevant experience in Data Engineering or related field; Experience and demonstrated proficiency as a Data Engineer or equivalent training in a business or clinical environment; Bachelor’s Degree in Computer/Information Science, Mathematics/Statistics, Economics, or related field; Relevant experience may be considered in lieu of a bachelor’s degree; 3 years of progressive experience in Data Engineering or related field; Experience in applying technology in a business or clinical environment; 8 more items(s)","Competitive salary ranging from \(50,200.00 to \)90,300.00 based on experience; Health, dental, and vision insurance; Retirement savings plan with company matching; Generous vacation and paid time off policy; Opportunities for professional development and continuous learning; Flexible working hours; 3 more items(s)","Power BI, SQL, SAS, Alteryx, Dataiku"
Sr. Test Data Engineer,ASR Analytics,"ASR Analytics • Rancho Cordova, CA •  via Ladders",150K–170K a year,Full-time,"ating in the WIC program• Supports first responders in reducing opioid overdoses within their communities.• Empowers colleges and universities to identify and thwart financial aid fraud• Equips teachers with valuable insights to identify students requiring additional support• Enhances efficiency for state tax agencies, leading to 99% faster return processing and quicker refunds for taxpayersWith a focus on Tax & Revenue, Health & Human Services, and Justice & Public Safety, Voyatek combines the scale to support large complex projects with the agility and accessibility of a boutique solutions provider. Together, Voyatek and its customers work to improve population wellbeing, create safer communities, and foster a thriving economy.We're more than a technology company -- we're an outcomes company.We encourage our employees to think differently, ask tough questions, and relentlessly pursue what's best for our customers and the residents they serve.We believe that the value of technology is defined by its human impact. If you agree, you've come to the right place.Voyatek is seeking applicants to occupy the position of Data Engineer Lead within our team. This role offers remote work opportunities.Key Responsibilities:• Experience managing a team of Data Engineers, ETL Developers, and/or Conversion Developers.• Experience working in a multi-vendor, multi-team environment.• Experience leading and coordinating with team members across multiple physical locations and an ability to manage remote workers.• Implement business and IT data requirements through new data strategies and designs across all data platforms (relational, dimensional, and NoSQL) and data tools (reporting, visualization, and, analytics).• Work with business and application/solution teams to implement data strategies, build data flows, and develop conceptual/logical/physical data models• Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models.• Identify the architecture, infrastructure, and interfaces to data sources, tools supporting automated data loads, security concerns, analytic models, and data visualization.• Hands-on modeling, design, configuration, installation, performance tuning, and sandbox POC.• Work proactively and independently to address project requirements and articulate issues/challenges to reduce project delivery risks.• Analyzing and translating business needs into long-term solution data models.• Evaluating existing data systems.• Working with the development team to create conceptual data models and data flows.• Developing best practices for data coding to ensure consistency within the system.• Reviewing modifications of existing systems for cross-compatibility.• Implementing data strategies and developing physical data models.• Updating and optimizing local and metadata models.• Evaluating implemented data systems for variances, discrepancies, and efficiency.• Troubleshooting and optimizing data systems.Mandatory Qualifications• 5+ years of team leadership and/or management experience.• 5+ years of hands-on relational, dimensional, and/or analytic experience (using RDBMS, dimensional, NoSQL data platform technologies, and ETL and data ingestion protocols).• Experience with data warehouse, data lake, and enterprise big data platforms in multi-data-center contexts required .• 3 years + of hands-on experience with physical and relational data modeling.• Good knowledge of metadata management, data modeling, and related tools (Erwin or ER Studio or others) required .Desired Qualifications• Bachelor's or master's degree in computer/data science technical or related experience.• Experience in team management, communication, and presentation.• Expert knowledge of metadata management and related tools.• Knowledge of mathematical foundations and statistical analysis.• Strong interpersonal skills.• Experience with team management and leadership.• Excellent communication and presentation skills.• Advanced troubleshooting skills.The wage range for this role considers the wide range of factors that are considered in making compensation decisions. This includes, but is not limited to, skill sets, experience and training, licensure and certifications, and geographic location. At Voyatek, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current annual range is $150,000 - $170,000.If you think you are a good fit for us, we encourage you to apply. Check out our career website for all open positions!Voyatek provides equal employment opportunities to all employees and applicants for employment without regard to race, color, creed, ancestry, national origin, citizenship, sex or gender (including pregnancy, childbirth, and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status or domestic violence victim status, religion, age, disability, genetic information, service in the military, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Equal employment opportunity applies to all terms and conditions of employment, including recruitment and hiring, job assignment/placement, promotion, upgrading, demotion, termination, layoff, recall, transfer, leave of absence, rates of pay or other compensation, internship, and training.","Experience working in a multi-vendor, multi-team environment; 5+ years of team leadership and/or management experience; 5+ years of hands-on relational, dimensional, and/or analytic experience (using RDBMS, dimensional, NoSQL data platform technologies, and ETL and data ingestion protocols); Experience with data warehouse, data lake, and enterprise big data platforms in multi-data-center contexts required ; 3 years + of hands-on experience with physical and relational data modeling; Good knowledge of metadata management, data modeling, and related tools (Erwin or ER Studio or others) required ; 3 more items(s)","The wage range for this role considers the wide range of factors that are considered in making compensation decisions; This includes, but is not limited to, skill sets, experience and training, licensure and certifications, and geographic location; A reasonable estimate of the current annual range is $150,000 - $170,000","Erwin, ER Studio"
"Data Engineer, Palo Alto",Databricks,"Databricks • Mountain View, CA •  via LinkedIn",1 day ago,Full-time,"and customer obsessed — we leap at every opportunity to solve technical challenges, from designing next-gen UI/UX for interfacing with data to scaling our services and infrastructure across millions of virtual machines. And we're only getting started.As a software engineer with a backend focus, you will work with your team to build infrastructure and products for the Databricks platform at scale.The Impact You'll Have:Our backend teams span many domains, from our core compute fabric resource management infrastructure, to service platforms, to machine learning infrastructure. For instance, you might work on challenges such as:• Resource management infrastructure powering the big data and machine learning workloads on the Databricks platform in a scalable, secure, and cloud-agnostic way• Develop reliable, scalable services and client libraries that work with massive amounts of data on the cloud, across geographic regions and Cloud providers• Build tools to allow Databricks engineers to operate their services across different clouds and environments• Build services, products and infrastructure at the intersection of machine learning and distributed systems.What We Look For:• BS (or higher) in Computer Science, or a related field• 5+ years of production level experience in one of: Java, Scala, C++, or similar language.• Experience developing large-scale distributed systems.• Experience working on a SaaS platform or with Service-Oriented Architectures.• Experience with cloud technologies, e.g. AWS, Azure, GCP, Docker, or Kubernetes.• Experience with security and systems that handle sensitive data.• Good knowledge of SQL.Benefits• Comprehensive health coverage including medical, dental, and vision• 401(k) Plan• Equity awards• Flexible time off• Paid parental leave• Family Planning• Gym reimbursement• Annual personal development fund• Work headphones reimbursement• Employee Assistance Program (EAP)• Business travel accident insurancePay Range TransparencyDatabricks is committed to fair and equitable compensation practices. The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location. Based on the factors above, Databricks utilizes the full width of the range. The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above. For more information regarding which range your location is in visit our page here.Local Pay Range$166,000—$225,000 USDAbout DatabricksDatabricks is the data and AI company. More than 10,000 organizations worldwide — including Comcast, Condé Nast, Grammarly, and over 50% of the Fortune 500 — rely on the Databricks Data Intelligence Platform to unify and democratize data, analytics and AI. Databricks is headquartered in San Francisco, with offices around the globe and was founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow. To learn more, follow Databricks on Twitter, LinkedIn and Facebook.BenefitsAt Databricks, we strive to provide comprehensive benefits and perks that meet the needs of all of our employees. For specific details on the benefits offered in your region, please visit https://www.mybenefitsnow.com/databricks.Our Commitment to Diversity and InclusionAt Databricks, we are committed to fostering a diverse and inclusive culture where everyone can excel. We take great care to ensure that our hiring practices are inclusive and meet equal employment opportunity standards. Individuals looking for employment at Databricks are considered without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, veteran status, and other protected characteristics.ComplianceIf access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone.","BS (or higher) in Computer Science, or a related field; 5+ years of production level experience in one of: Java, Scala, C++, or similar language; Experience developing large-scale distributed systems; Experience working on a SaaS platform or with Service-Oriented Architectures; Experience with cloud technologies, e.g; AWS, Azure, GCP, Docker, or Kubernetes; Experience with security and systems that handle sensitive data; Good knowledge of SQL; If access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone; 6 more items(s)","Comprehensive health coverage including medical, dental, and vision; 401(k) Plan; Equity awards; Flexible time off; Paid parental leave; Family Planning; Gym reimbursement; Annual personal development fund; Work headphones reimbursement; Employee Assistance Program (EAP); Business travel accident insurance; Pay Range Transparency; Databricks is committed to fair and equitable compensation practices; The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles; Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location; The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above; $166,000—$225,000 USD; At Databricks, we strive to provide comprehensive benefits and perks that meet the needs of all of our employees; For specific details on the benefits offered in your region, please visit https://www.mybenefitsnow.com/databricks; 16 more items(s)","Java, Scala, C++, AWS, Azure, GCP, Docker, Kubernetes, SQL"
Senior Data Engineer-AWS (Onsite or Remote) (NP – 19233),AltaMed Health Services,"AltaMed Health Services • Montebello, CA •  via Learn4Good",29 days ago,80K–100K a year,"wThe Senior Data Engineer position works closely with business leaders, managers, staff and vendor to accurately gather and interpret requirements. Develop technical specifications and recommend, design, develop, test, implement, and support innovative and optimal data solutions. Serves as a coach and mentor to the data engineering team.Minimum Requirements• High School diploma required. Associates degree or Bachelor’s degree in a health related field preferred.• Minimum 7 years of experience in a data warehouse ata engineering role for a mid to large size organization required. Health Care industry experience highly desired.• Minimum of 4 years of hands on experience with ETL ata Integration, BI, data mining (preferred) and modeling preferred.• Proficient in 3rd normal form (TNF) DW Architecture, data flow best practices, data modeling, metadata, and master data management.• Expertise in SQL and RDBMS systems such as Microsoft SQL Server.• Stay up-to-date with established and industry emerging data technologies.• Demonstrated ability to produce high quality technical documentation.• Experience in implementing a data architecture with separation between storage and compute preferred.• Deep understanding of data architecture and ability to coordinate with the enterprise data architect is highly desired• Experience with data quality and ETL platform, such as Talent is preferred.• Experience using Azure Dev Ops for source control and CICD.• Experience working in both waterfall and agile project managed teams.• Experience using Talent as an ETL tool is a big plus.• Familiarity of Data Virtualization concept/architecture is highly desired.• Hand on technical experience with MS SQL 2019 Big Data Cluster or other Big Data technologies is preferred.Benefits & Career Development• Medical, Dental and Vision insurance• 403(b) Retirement savings plans with employer matching contributions• Flexible Spending Accounts• Commuter Flexible Spending• Career Advancement & Development opportunities• Paid Time Off & Holidays• Paid CME Days• Malpractice insurance and tail coverage• Tuition Reimbursement Program• Corporate Employee Discounts• Employee Referral Bonus Program• Pet Care Insurance#J-18808-Ljbffr",High School diploma required,"Benefits & Career Development; Medical, Dental and Vision insurance; 403(b) Retirement savings plans with employer matching contributions; Flexible Spending Accounts; Commuter Flexible Spending; Career Advancement & Development opportunities; Paid Time Off & Holidays; Paid CME Days; Malpractice insurance and tail coverage; Tuition Reimbursement Program; Corporate Employee Discounts; Employee Referral Bonus Program; Pet Care Insurance; #J-18808-Ljbffr; 11 more items(s)","SQL, Microsoft SQL Server, Azure Dev Ops, Talent, MS SQL 2019 Big Data Cluster"
"Data Engineer, Consultant",MEO STAFFING,"MEO STAFFING • Reno, NV •  via MEO Staffing",Full-time,119K–178K a year,"ering lifecycle including research, proof of concepts, architecture, design, development, test, deployment, and maintenance. This role will focus on AWS optimization with some work occurring in other cloud-based environments.· Oversee the development of novel data pipelines that integrate and normalize large data from a variety of sources (e.g., electronic health record, claims, wearable device, publicly available data, etc.) to enable learning health, machine learning model development, and deployment.· Design, direct and implement ETL processes, including data capture, data quality, testing and validation methods.· Layer in instrumentation in the development process so that data pipelines can be monitored. Measurements are used to detect internal problems before they result into user visible outages or data quality issues.· Build processes and diagnostic tools to troubleshoot, maintain and optimize engineering environments and respond to production issues.· Provide subject matter expertise and hands on delivery of data capture, curation, and consumption pipelines for AWS.· Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.· Develop documentation, such as data dictionaries, guides, or data flow diagrams that assists staff in identifying, locating, and using the organization’s data.Qualifications:Incumbent Must Possess:· Minimum of 3 years of SQL programming experience and associated SQL tools (SSIS, SSMS, SSRS, etc.).· Experience with Visual Studio is preferred.· At least 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions.· Minimum of 3 years of RDBMS experience.· Extensive hands-on experience implementing data migration and data processing using Amazon Web Services. Includes knowledge of Amazon Connect and Amazon Lambda.· Knowledge of medical terminology, especially ICD-10 codes, CPT codes, DRG codes, and an understanding of adjudicated claims data.· Excellent verbal and written communication. An applicant may be asked to provide examples of written work to demonstrate technical writing proficiency.Education:Master’s degree with 10 years’ experience preferred; Bachelor’s degree with 13 years of equivalent experience will be considered in place of master’s degree requirement.Experience:Requires a minimum of 10 years’ experience with at least five (5) years working in data management, data engineering, or data architecture. Enterprise Data Warehouse development preferred; Requires at least five (5) years working with healthcare data; more is preferred. SQL proficiency is required.License(s):NoneCertification(s):Certification work related to Epic or Amazon Cloud may be required within 1 year of starting employment.Why is This a Great Opportunity?Remote or Onsite/Hybrid! Great benefits!","Minimum of 3 years of SQL programming experience and associated SQL tools (SSIS, SSMS, SSRS, etc.); At least 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions; Minimum of 3 years of RDBMS experience; Extensive hands-on experience implementing data migration and data processing using Amazon Web Services; Includes knowledge of Amazon Connect and Amazon Lambda; Knowledge of medical terminology, especially ICD-10 codes, CPT codes, DRG codes, and an understanding of adjudicated claims data; Excellent verbal and written communication; An applicant may be asked to provide examples of written work to demonstrate technical writing proficiency; Requires a minimum of 10 years’ experience with at least five (5) years working in data management, data engineering, or data architecture; SQL proficiency is required; Certification work related to Epic or Amazon Cloud may be required within 1 year of starting employment; 8 more items(s)",Remote or Onsite/Hybrid!,"AWS, SQL, SSIS, SSMS, SSRS, Visual Studio, RDBMS, Amazon Connect, Amazon Lambda"
Java Big Data Engineer,TikTok,"TikTok • San Jose, CA •  via Indeed",201K–395K a year,Full-time,"k possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always.At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve.Join us.About the Team:The success of TikTok's data business model hinges on the supply of a large volume of high quality labeled data that will grow exponentially as our business scales up. The Data Solutions team is built to understand data strategically at scale for all Global Business Solution (GBS) business needs. Data Solutions Team uses structured and unstructured data to guide and uncover insights, turning our findings into real products to power exponential growth. Data Solutions Team's responsibilities include infrastructure construction, recognition capabilities management, and global labeling delivery management.The Role:As a Lead Data Engineer, you will be responsible for 0-to-1 build of both the team and the data pipeline. You will be working on cutting-edge challenges in the big data and AI industry which requires strong passion and capability of innovation. You will collaborate closely with cross-functional teams to understand business requirements and translate them into technical solutions. Additionally, you will provide technical guidance, mentorship, and support to junior team members.Responsibilities:• Architect efficient, scalable, and reliable data pipelines and infrastructure for ingesting, processing, and transforming large volumes of data• Define the technical strategy and roadmap for data engineering projects in alignment with business objectives, actively evaluate and bring in industry best practices and state-of-the-art technical approaches, and timely update the strategy according to the rapid change of the industry• Build and lead a high-performing data engineering team with a clear strategy, providing business, technical, and personal development coaching• Own and drive data engineering projects by leveraging both internal and cross-functional resources, setting meaningful and challenging targets, and achieving them with innovative approaches• Foster a collaborative and inclusive team culture within and across teams, collaborate with data scientistsQualificationsMinimum Qualifications:• Bachelor's or Master's degree in Computer Science, Engineering, or a related field.• 5 years of experience in data engineering, with demonstrated expertise in building scalable data pipelines and infrastructure.• 5 years of experience in programming languages such as Python, SQL, and Java.• Strong understanding of database systems, data warehousing, and distributed computing concepts.• Excellent leadership, communication, and interpersonal skills.• Proven track record of leading teams and delivering complex data engineering projects.• Ability to thrive in a fast-paced, dynamic environment and manage multiple priorities effectively.Preferred qualifications:• 3 years of experience with big data technologies such as Apache Spark, Hadoop, and Kafka.TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2Job InformationThe base salary range for this position in the selected city is $201000 - $395000 annually.​Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.​Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees:​We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.​Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.​We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice.​For Los Angeles County (unincorporated) Candidates:​Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state, and local laws including the Los Angeles County Fair Chance Ordinance for Employers and the California Fair Chance Act. Our company believes that criminal history may have a direct, adverse and negative relationship on the following job duties, potentially resulting in the withdrawal of the conditional offer of employment:​1. Interacting and occasionally having unsupervised contact with internal/external clients and/or colleagues;​2. Appropriately handling and managing confidential information including proprietary and trade secret information and access to information technology systems; and​3. Exercising sound judgment.","Bachelor's or Master's degree in Computer Science, Engineering, or a related field; 5 years of experience in data engineering, with demonstrated expertise in building scalable data pipelines and infrastructure; 5 years of experience in programming languages such as Python, SQL, and Java; Strong understanding of database systems, data warehousing, and distributed computing concepts; Excellent leadership, communication, and interpersonal skills; Proven track record of leading teams and delivering complex data engineering projects; Ability to thrive in a fast-paced, dynamic environment and manage multiple priorities effectively; Interacting and occasionally having unsupervised contact with internal/external clients and/or colleagues;; Appropriately handling and managing confidential information including proprietary and trade secret information and access to information technology systems; and; 6 more items(s)","The base salary range for this position in the selected city is $201000 - $395000 annually; Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location; Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units; We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match; As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans; In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care; Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability; A 401K company match, gym and cellphone service reimbursements; 5 more items(s)","Python, SQL, Java, Apache Spark, Hadoop, Kafka"
Lead Data Engineer – R01539793,Jobsbridge,"Jobsbridge • Cupertino, CA •  via ZipRecruiter",5 days ago,Full-time,"hnologies (Hadoop, Hive, Spark, Flume, Oozie, HBase, Solr, Cassandra)Experience with 1 or more Hadoop distributions such as Cloudera, Hortonworks, Apache etc.5 years of hands-on experience in building enterprise software using Java in the space of Big Data Analytics.Strong database fundamentals including SQL, performance and schema design.Strong programming skills in JavaStrong experience designing and building REST style web servicesStrong written and communication skillsExperience working with authentication models is a plusExperience in using and managing change management tool Git and build server software JenkinsExperience with testing tools and techniques, ex. JunitAbility to work in a fast paced, constantly changing Agile environmentScripting skills in at least one of the following: Python, Shell, BashQualificationsHadoop, Hive, Spark, Flume, Oozie, HBase, Solr, CassandraAdditional InformationAll your information will be kept confidential according to EEO guidelines.","Proficiency with Big Data processing technologies (Hadoop, Hive, Spark, Flume, Oozie, HBase, Solr, Cassandra); Experience with 1 or more Hadoop distributions such as Cloudera, Hortonworks, Apache etc; 5 years of hands-on experience in building enterprise software using Java in the space of Big Data Analytics; Strong database fundamentals including SQL, performance and schema design; Strong programming skills in Java; Strong experience designing and building REST style web services; Strong written and communication skills; Experience in using and managing change management tool Git and build server software Jenkins; Experience with testing tools and techniques, ex; Junit; Ability to work in a fast paced, constantly changing Agile environment; Scripting skills in at least one of the following: Python, Shell, Bash; Hadoop, Hive, Spark, Flume, Oozie, HBase, Solr, Cassandra; 10 more items(s)","Lead Data EngineerPrimary SkillsETL Fundamentals, HDFS, Hive, MapReduce, Modern Data Platform Fundamentals, Oozie, PLSQL, Python, Spark - Pyspark, Spark - Scala, SQL Secondary SkillsJava SpecializationBig Data Engineering: Associate BI Engineer Job requirements; Previous experience as a data engineer or in a similar role Strong and hands-on Knowledge of programming languages (e.g. Java and Python); Experience with popular Python frameworks such as Django, Flask or Pyramid; In-depth understanding of the Python software development stacks, ecosystems, frameworks and tools such as Numpy, Scipy, Pandas, Dask, spaCy, NLTK, sci-kit-learn and PyTorch; Experience with popular Python frameworks pysaprk,databricks,flink nosql db could be added advantages; Familiarity with database technologies such as SQL and NoSQL; Excellent problem-solving ability with solid communication and collaboration skills; Knowledge of data science and machine learning concepts and tools; A working understanding of cloud platforms such as AWS, Google Cloud or Azure; 6 more items(s)","Hadoop, Hive, Spark, Flume, Oozie, HBase, Solr, Cassandra, Cloudera, Hortonworks, Apache, Java, SQL, Git, Jenkins, Junit, Python, Shell, Bash, ETL Fundamentals, HDFS, MapReduce, Modern Data Platform Fundamentals, PLSQL, Pyspark, Scala, Django, Flask, Pyramid, Numpy, Scipy, Pandas, Dask, spaCy, NLTK, sci-kit-learn, PyTorch, AWS, Google Cloud, Azure"
Senior ML/Data Engineer,VISART,"VISART • Claremont, CA •  via OPTnation",107K–108K a year,"Full-time, Part-time, and Contractor",erience using Map/Reduce and Hive. Experience using Scoop and Flume is a plus. Understanding of MPP systems HDFS Map/Reduce HBase.Key Skills Visualization. Machine Learning and AI. NoSQL. Data Pipelines. Hyper Automation. Programming. DevOps.,Requirements Strong experience with object-oriented design coding and testing patterns; Experience in architecting building and maintaining software platforms and large-scale data infrastructures; Experience building big data solutions using Hadoop technologies; 2+ years of Java/Scala development experience; Extensive knowledge of UNIX/Linux; Solid understanding of SQL and experience; Hands on experience using Map/Reduce and Hive; Understanding of MPP systems HDFS Map/Reduce HBase; Key Skills Visualization; Machine Learning and AI; 7 more items(s),"We are looking for a highly skilled Senior ML/Data Engineer with extensive experience in Python and Go programming languages, and a strong background in designing and optimizing AI/ML systems, data pipelines, integrations, and low-latency programming; The ideal candidate will be passionate about building high-performance ML/AI and data-rich backend systems and eager to tackle complex technical challenges; Proven experience as a ML/AI/Data/Backend Engineer or similar role with a focus on Python and Go programming languages; Strong proficiency in designing and managing data pipelines and integrations; Expertise in developing and optimizing web APIs for high performance and scalability; Experience with low-latency programming techniques and performance tuning; Solid understanding of data structures, algorithms, and software design principles; Excellent problem-solving skills and attention to detail; Strong communication and teamwork abilities; Bachelor’s degree in Computer Science, Engineering, or a related field, or equivalent practical experience; 7 more items(s)","Map/Reduce, Hive, Scoop, Flume, HDFS, HBase, Java, Scala, UNIX/Linux, SQL, Python, Go"
"URGENT: Sr. Data Engineer, Azure Data Factory, Databricks, Lakehouse - Hybrid LA (W2 ONLY,NO C2C)",Meta,"Meta • San Francisco, CA •  via ZipRecruiter",17 hours ago,Full-time,"ith some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few companies can match.Data Engineer, Product Analytics Responsibilities Manage and execute data warehouse plans for a product or a group of products to solve well-scoped problems Identify the data needed for a business problem and implement logging required to ensure availability of data, while working with data infrastructure to triage issues and resolve Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights in a meaningful way Build data expertise and leverage data controls to ensure privacy, security, compliance, data quality, and operations for allocated areas of ownership Design, build and launch new data models and visualizations in production, leveraging common development toolkits Independently design, build and launch new data extraction, transformation and loading processes in production, mentoring others around efficient queries Support existing processes running in production and implement optimized solutions with limited guidance Define and manage SLA for data sets in allocated areas of ownership Minimum Qualifications Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience. 2+ years of work experience in data engineering Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.) Preferred Qualifications Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy Experience working with terabyte to petabyte scale data For those who live in or expect to work from California if hired for this position, please click here for additional information. Locations About Meta Meta builds technologies that help people connect, find communities, and grow businesses.When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology.People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today-beyond the constraints of screens, the limits of distance, and even the rules of physics. Meta is committed to providing reasonable support (called accommodations) in our recruiting processes for candidates with disabilities, long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need support, please reach out to accommodations-ext@fb.com.$110,000/year to $171,000/year + bonus + equity + benefits Individual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits.Learn more about benefits at Meta.","All candidates must already be Authorized to work in the United States, via Citizenship, Green Card, Employment Authorization Card, Practical Training etc; Seven years of applying Enterprise Architecture principles, with at least five (5) years in a lead capacity; Five years of hands-on experience with Azure Data Factory, Databricks, API management, and managing Azure resources; Ability to automate Azure data resources using DataOps principles and tools; Five years of experience developing data models and pipelines using Python; Five years of experience working with Lakehouse platforms; Three years of experience in CI/CD pipelines and infrastructure automation using tools such as Terraform; Data lake: 5 years (Required); Azure Data Factory: 5 years (Required); Python: 5 years (Required); Azure Data Bricks: 5 years (Required); Los Angeles, CA 90012: Relocate before starting work (Required); 9 more items(s)","Pay: From $80,000.00 per year; 8 hour shift","SQL, ETL, Python, C++, C#, Scala, Azure Data Factory, Databricks, API management, Terraform"
Analytics Engineer II,Activision Blizzard,"Activision Blizzard • Santa Monica, CA •  via JobzMall",150K–200K a year,Full-time,"s a strong background in software engineering and data management. We are looking for a driven individual who thrives in a fast-paced, collaborative environment and is dedicated to delivering high-quality products. If you are a self-starter with a creative mindset and a passion for innovation, then we want you on our team!Design and develop data-driven solutions for our games.Maintain and improve existing data systems and processes.Collaborate with cross-functional teams to understand business requirements and design effective solutions.Identify and address technical issues and provide solutions to improve performance and efficiency.Stay updated on industry trends and best practices to continuously improve our data management processes.Mentor and provide technical guidance to junior engineers.Work closely with game developers to integrate data into game features and systems.Ensure data integrity and security by implementing proper data governance practices.Participate in code reviews and contribute to improving coding standards and best practices.Proactively identify and propose new solutions to improve data collection, analysis, and reporting.Communicate effectively with team members and stakeholders to provide progress updates and address any issues or concerns.Continuously optimize data systems and processes to improve performance and scalability.Collaborate with external partners and vendors to integrate and maintain data solutions.Ensure timely delivery of high-quality products by adhering to project timelines and deadlines.Act as a subject matter expert on data management and provide guidance to non-technical team members.Activision Blizzard is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",This is an excellent opportunity for someone who is passionate about gaming and has a strong background in software engineering and data management,"As a Staff Software Engineer, you will play a critical role in designing, developing, and maintaining data-driven solutions for our games; Design and develop data-driven solutions for our games; Maintain and improve existing data systems and processes; Collaborate with cross-functional teams to understand business requirements and design effective solutions; Identify and address technical issues and provide solutions to improve performance and efficiency; Stay updated on industry trends and best practices to continuously improve our data management processes; Mentor and provide technical guidance to junior engineers; Work closely with game developers to integrate data into game features and systems; Ensure data integrity and security by implementing proper data governance practices; Participate in code reviews and contribute to improving coding standards and best practices; Proactively identify and propose new solutions to improve data collection, analysis, and reporting; Communicate effectively with team members and stakeholders to provide progress updates and address any issues or concerns; Continuously optimize data systems and processes to improve performance and scalability; Collaborate with external partners and vendors to integrate and maintain data solutions; Ensure timely delivery of high-quality products by adhering to project timelines and deadlines; Act as a subject matter expert on data management and provide guidance to non-technical team members; 13 more items(s)",The job description provided does not specifically list the names of tools or software.
Graph & Semantic Data Engineer,VISART,"VISART • Claremont, CA •  via OPTnation",107K–107K a year,"Full-time, Part-time, and Contractor",age to profile the incoming data for new sources. Configuration and usage of advanced queries Workflow management User Management and UI Config. Experience with LCA configuration for customizations. Expert in Cloud & Data Technology and the market trends within Azure and AWS ecosystems. Work with project management to develop the overall implementation solution roadmap and plan.Requirements 7+ years of overall IT experience. Experience with hands-on implementation of Master Data Management solutions using one of the major MDM platforms. 5+ years’ Experience working as either Software Engineer/Data Engineer query tuning performance tuning troubleshooting and debugging big data solutions.Key Skills Programming and Scripting Languages. Cloud Platform Skills. Data Integration and ETL Tools. Data Pipeline Development. Data Quality and Testing. Data Security and Compliance. Database Administration Skills.,Experience with LCA configuration for customizations; Expert in Cloud & Data Technology and the market trends within Azure and AWS ecosystems; Requirements 7+ years of overall IT experience; Experience with hands-on implementation of Master Data Management solutions using one of the major MDM platforms. 5+ years’ Experience working as either Software Engineer/Data Engineer query tuning performance tuning troubleshooting and debugging big data solutions; Key Skills Programming and Scripting Languages; Cloud Platform Skills; Data Integration and ETL Tools; Data Security and Compliance; Database Administration Skills; 6 more items(s),Responsibilities Work with Users and Business Analysts to understand and translate functional requirements to technical design; Analysis and resolution of sophisticated data relationships and data cleansing/profiling scenarios; Solid understanding of high-level enterprise architecture patterns for data ingesting storing processing and publishing Experience configuring External matches and their usage to profile the incoming data for new sources; Configuration and usage of advanced queries Workflow management User Management and UI Config; Work with project management to develop the overall implementation solution roadmap and plan; 2 more items(s),"Azure, AWS"
Technical Data Scientist/ETL Engineer,AstraZeneca GmbH,"AstraZeneca GmbH • Long Beach, CA •  via Learn4Good",2 days ago,100K–125K a year,"We operate out of Operations hub sites including in the UK, Sweden, US, and China; and out of our Global Technology Centres in India and Mexico.Here our work has a direct impact on patients – transforming our ability to develop life-changing medicines. We empower the business to perform at its peak and lead a new way of working, combining cutting-edge science with leading digital technology platforms and data. All with a passion to impact lives through data, analytics, AI, machine learning and more.It’s a dynamic and challenging environment to work in – but that’s why we like it. There are countless opportunities to learn and grow, whether that’s exploring new technologies in hackathons, or transforming the roles and work of colleagues, forever. This is your chance to be part of a team that has the backing to innovate, disrupt an industry and change lives.THIS IS WHAT YOU’LL DOWe are seeking an experienced Knowledge Graph and Semantic Data Engineer Lead to join our Data, Analytics & AI group for Operations IT. This group supports various business areas such as Develop, Manufacture, Supply Chain, Quality and Procurement with data-driven solutions and insights.As a Knowledge Graph & Semantic Data Engineering Lead, your role is to establish and lead a Knowledge Graph and semantic capability. You’ll develop relationships with the business and start to create a backlog of use cases; in many circumstances you will need to educate the business on the value of semantics and Knowledge graphs.The ideal candidate possesses a strong background in data analytics, semantics, and knowledge graph technologies with experience in leading semantic and/or knowledge graph teams and able to perform at a strategic level. They should have a background in building and maintaining ontologies and Knowledge graphs.In this role, you’ll contribute to the success of Operations by designing and implementing solutions involving graph and semantic technologies, as well as supporting our AI and Digital deliveries.Typical accountabilities include:Strategy• Provide strategy & Vision for Knowledge graph and semantic engineering:• Partner with the AI team to develop a strategy for a Gen AI chatbot to query Operations data products via a knowledge graph.• Guide the development, execution, and implementation of a roadmap for the strategies above.• Collaborate with cross-functional teams and stakeholders to understand business requirements, identify knowledge gaps, and define data-driven strategies to inform decision-making and drive business growth.Knowledge Graph Platform• Leverage your expertise in graph databases and data engineering to design, develop, and maintain advanced knowledge-based systems and solutions that support our digital initiatives.• Develop and implement advanced data engineering pipelines, including data ingestion, processing, storage, and analysis, ensuring the optimal performance, scalability, and reliability of our data infrastructure.• Apply cutting-edge computer science and data analytics techniques to derive valuable insights from large, complex datasets, and translate these insights into actionable intelligence for our business stakeholders.• Utilize your expertise in Graph DB and Graph query languages to continue developing and maintain existing data models and graph databases, enabling efficient querying and analysis of complex data relationships.• Actively contribute to the continuous improvement of our knowledge engineering processes, tools, and methodologies, fostering a culture of innovation and excellence within the team.• Stay abreast of emerging technologies and industry trends to continuously enhance the organization's Knowledge and semantic capabilities.Semantic Engineering• You will maintain, evolve, and grow our knowledge models (ontologies), presenting, selling them, annotate them to provide common understanding.• Collaborate with cross-functional teams to define and document data models using ontological principles.• Conduct ontology mapping and integration to facilitate data interoperability and integration across systems.• Provide training and guidance to stakeholders on ontology usage and best practices for data organization.• Stay updated on industry trends and advancements in ontology development to enhance data management processes.Stakeholder Collaboration• Collaborate with stakeholders across departments to understand requirements and ensure business needs are met.• Act as a liaison between IT and Business Units, fostering strong communication and understanding.• Oversight of partnerships with multiple third parties, academic or outsourced partners to shape and drive project outcomes and…","The ideal candidate possesses a strong background in data analytics, semantics, and knowledge graph technologies with experience in leading semantic and/or knowledge graph teams and able to perform at a strategic level; They should have a background in building and maintaining ontologies and Knowledge graphs; Knowledge Graph Platform; Leverage your expertise in graph databases and data engineering to design, develop, and maintain advanced knowledge-based systems and solutions that support our digital initiatives; Utilize your expertise in Graph DB and Graph query languages to continue developing and maintain existing data models and graph databases, enabling efficient querying and analysis of complex data relationships; Actively contribute to the continuous improvement of our knowledge engineering processes, tools, and methodologies, fostering a culture of innovation and excellence within the team; Stay abreast of emerging technologies and industry trends to continuously enhance the organization's Knowledge and semantic capabilities; 4 more items(s)","This group supports various business areas such as Develop, Manufacture, Supply Chain, Quality and Procurement with data-driven solutions and insights; As a Knowledge Graph & Semantic Data Engineering Lead, your role is to establish and lead a Knowledge Graph and semantic capability; You’ll develop relationships with the business and start to create a backlog of use cases; in many circumstances you will need to educate the business on the value of semantics and Knowledge graphs; In this role, you’ll contribute to the success of Operations by designing and implementing solutions involving graph and semantic technologies, as well as supporting our AI and Digital deliveries; Provide strategy & Vision for Knowledge graph and semantic engineering:; Partner with the AI team to develop a strategy for a Gen AI chatbot to query Operations data products via a knowledge graph; Guide the development, execution, and implementation of a roadmap for the strategies above; Collaborate with cross-functional teams and stakeholders to understand business requirements, identify knowledge gaps, and define data-driven strategies to inform decision-making and drive business growth; Develop and implement advanced data engineering pipelines, including data ingestion, processing, storage, and analysis, ensuring the optimal performance, scalability, and reliability of our data infrastructure; Apply cutting-edge computer science and data analytics techniques to derive valuable insights from large, complex datasets, and translate these insights into actionable intelligence for our business stakeholders; You will maintain, evolve, and grow our knowledge models (ontologies), presenting, selling them, annotate them to provide common understanding; Collaborate with cross-functional teams to define and document data models using ontological principles; Conduct ontology mapping and integration to facilitate data interoperability and integration across systems; Provide training and guidance to stakeholders on ontology usage and best practices for data organization; Stay updated on industry trends and advancements in ontology development to enhance data management processes; Collaborate with stakeholders across departments to understand requirements and ensure business needs are met; Act as a liaison between IT and Business Units, fostering strong communication and understanding; Oversight of partnerships with multiple third parties, academic or outsourced partners to shape and drive project outcomes and…; 15 more items(s)","- AI
- Machine learning
- Graph DB
- Graph query languages"
Sr Data Engineer,GovCIO,"GovCIO • Sacramento, CA •  via Adzuna",3 days ago,Full-time,"els that impact productivity, decision making, and provide strategic mission impact. Data Integration - Applies data wrangling tools including ETL, ELT, and programming languages to collect and blend data from operational and relevant external systems. Data Analysis -- Applies data mining, machine learning, and statistical analysis on data to create predictive and descriptive models. Applies and integrates these models to develop segmentation, clustering, forecasting, classification and other models. Data Visualization -- Applies Data discovery and data visualization tools to interpret and present the findings in a compelling and usable manner. Maintains and integrates analytical systems with operational systems, verifies the accuracy of the data and analytics. Interacts with both business and data SMEs.- Generates new business insights through data extraction, storage, transformation, analysis, and visualization of diverse data sets.- Collects and transforms structured, unstructured, relational, and NoSQL data using ETL and ELT tools as well as develops custom code using programming languages. Understands and uses distributed (e.g. MapReduce) methods that scale to multi-Terabyte sized data collections.- Analyzes data using data mining, machine learning, and statistical algorithms available in COTS tools (e.g. SAS, SPSS, and Oracle); builds analytical solutions using programming languages (e.g. R, Python, SAS), and programming libraries (e.g. Python SciKit, R Caret, PostgreSQL MADlib, Apache Spark MLlib).- Interprets and evaluates accuracy of results through iterative, agile methods.- Applies data discovery and data visualization tools (e.g. Tableau, Trifacta) to develop compelling, actionable, useful data stories.- Works closely with data SMEs, business, and management to prioritize business and information needs.QualificationsHS Diploma with 9+ years experience/professionalClearance: SecretWill accept a SECRET clearance but must be able to obtain a Top Secret clearance.Required Skills and Experience- Must have or obtain IAT level II/III certification (i.e., CompTIA Security+(CE))- Demonstrated experience or certifications in Linux and/or AWS GovCloud technologiesPreferred Skills and Experience- Strong interpersonal skills to collaborate with customers and internal cross-functional teams- Experience with virtual and/or cloud based servers and applications (AWS GovCloud Specific)- Excellent technical documentation and reporting skills- Effective written and oral communication skills- Experience working in an Agile environment- Certification(s) in relevant technologies- Active Top Secret clearance#NSS#ARCompany OverviewGovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.Posted Pay RangeThe posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an ""at-will position"" and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.Posted Salary RangeUSD $155,000.00 - USD $172,000.00 /Yr.Submit a referral to this job (https://careers-govcio.icims.com/jobs/4626/technical-data-scientist-etl-engineer/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)Location US-RemoteID 2024-4626Category IT Infrastructure & Network Engineering & OperationsPosition Type Full-Time","HS Diploma with 9+ years experience/professional; Clearance: Secret; Will accept a SECRET clearance but must be able to obtain a Top Secret clearance; Must have or obtain IAT level II/III certification (i.e., CompTIA Security+(CE)); Demonstrated experience or certifications in Linux and/or AWS GovCloud technologies; 2 more items(s)","Posted Pay Range; The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity; If hired, employee will be in an ""at-will position"" and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors; Posted Salary Range; USD $155,000.00 - USD $172,000.00 /Yr; 2 more items(s)","ETL, ELT, SAS, SPSS, Oracle, R, Python, Python SciKit, R Caret, PostgreSQL MADlib, Apache Spark MLlib, Tableau, Trifacta, CompTIA Security+(CE), Linux, AWS GovCloud"
"Senior Data Engineer, 3PX Private Pricing - Analytics & Insights",MSIT,"MSIT • Claremont, CA •  via OPTnation",106K–107K a year,"Full-time, Part-time, and Contractor",esign construction testing and deployment activities as established by departmental and organizational standards. Demonstrate collaborative skills working within a project team of diverse skills and will bring communication skills including oral written and presentation skills creativity and problem-solving skills to a challenging environment. Partner with other competency leads/ developers and support project planning technical design development and solution deployment functions. Identify opportunities in business processes system capabilities and delivery methodologies for continuous improvement as applicable. Lead mentor and develop development resources and provide project related directions. Act in a technical SME role support development QA and production support teams in SDLC and operational activities.Qualifications Undergraduate degree or equivalent experience. 5 years of experience with modern relational databases. 5 years of experience optimizing SQL statements. 5 years of experience,Qualifications Undergraduate degree or equivalent experience; 5 years of experience with modern relational databases; 5 years of experience optimizing SQL statements; 5 years of experience; 1 more items(s),Responsibilities Research evaluate identify alternative approaches recommend design and code efficient and effective solutions for challenging problems ranging from small to large work efforts for low to high complexity problems; Develop test deploy and schedule complex ETL/ELT solutions to integrate multiple data asset across the organization; Comply with standards and guidelines related to the design construction testing and deployment activities as established by departmental and organizational standards; Demonstrate collaborative skills working within a project team of diverse skills and will bring communication skills including oral written and presentation skills creativity and problem-solving skills to a challenging environment; Partner with other competency leads/ developers and support project planning technical design development and solution deployment functions; Identify opportunities in business processes system capabilities and delivery methodologies for continuous improvement as applicable; Lead mentor and develop development resources and provide project related directions; Act in a technical SME role support development QA and production support teams in SDLC and operational activities; 5 more items(s),SQL
Senior Data Engineer - Tech Lead,Amazon Web Services (AWS),"Amazon Web Services (AWS) • Santa Clara, CA •  via LinkedIn",16 days ago,Full-time,"(GDSP) organization is responsible for the Private Pricing Program. The Private Pricing Analytics and Insights (PPA&I) team owns building scalable analytical solutions that enable the GDSP organization with actionable insights to make data-driven decisions. This role will focus on Data Engineering, and Analytics related to the Private Pricing Program, requiring deep technical skills, strong business acumen and a deep analytical background to provide actionable data-driven insights and decision support.As a Senior Data Engineer on this team, you will drive efficiency in data handling processes, setup product analytics, drive advanced analysis and build new metrics which are key inputs to improve the health, scale and growth of the Private Pricing Program. You will work collaboratively with the business leaders (including the VP of Private Pricing), operations, and engineering teams on many non-standard and unique business problems and support business initiatives by collecting required and related data from external/internal sources.The ideal candidate should have strong communication skills and ability to prioritize effectively to ensure timelines are met. You will own projects E2E, from engaging with customers to production and delivery of a suite of tools used by the organization to make key business decisions. You should be a self-starter, comfortable with ambiguity, able to think big and be creative (while still paying careful attention to detail). You think in terms of architecture, not just code. You proactively work to improve the consistency and integration between your team’s BI solutions and any related systems or artifacts.Key job responsibilitiesOwning the design, development, and maintenance of scalable solutions for ongoing metrics, reports, analyses, dashboards, etc. to support analytical and business needs• Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using AWS services and internal tools• Build and deliver high quality data sets to support data scientists and customer reporting needs.• Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers• Translate basic business problem statements into analysis requirements.• Use analytical and statistical rigor to answer business questions and drive business decisions.• Find and create ways to measure the customer experience to drive business outcomes.• Develop queries and visualizations for ad-hoc requests and projects, as well as ongoing reporting.• Write queries and output efficiently, and have in-depth knowledge of the available in area of expertise. Pull the needed with standard query syntax; periodically identify more advanced methods of query optimization. Convert to make it analysis-ready.• Recognize and adopt best practices in reporting and analysis: integrity, design, analysis, validation, and documentation.• Troubleshoot operational quality issues.• Review and audit existing jobs and queries.• Recommending improvements to back-end sources for increased accuracy and simplicity.About The TeamDiverse Experiences:AWS values diverse experiences. Even if you do not meet all of the qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn’t followed a traditional path, or includes alternative experiences, don’t let it stop you from applying.Why AWS?Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating — that’s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.Inclusive Team CultureHere at AWS, it’s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness.Mentorship & Career GrowthWe’re continuously raising our performance bar as we strive to become Earth’s Best Employer. That’s why you’ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.Work/Life BalanceWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why flexible work hours and arrangements are part of our culture. When we feel supported in the workplace and at home, there’s nothing we can’t achieve in the cloud.Basic Qualifications• 5+ years of data engineering experience• Experience with data modeling, warehousing and building ETL pipelines• Experience with SQL• Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS• Experience mentoring team members on best practicesPreferred Qualifications• Experience operating large data warehouses• Experience developing, deploying, and managing production data pipelines in AWS• Experience deploying and managing machine learning models deployed into productionAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.Los Angeles County applicants: Job duties for this position include: work safely and cooperatively with other employees, supervisors, and staff; adhere to standards of excellence despite stressful conditions; communicate effectively and respectfully with employees, supervisors, and staff to ensure exceptional customer service; and follow all federal, state, and local laws and Company policies. Criminal history may have a direct, adverse, and negative relationship with some of the material job duties of this position. These include the duties and responsibilities listed above, as well as the abilities to adhere to company policies, exercise sound judgment, effectively manage stress and work safely and respectfully with others, exhibit trustworthiness and professionalism, and safeguard business operations and the Company’s reputation. Pursuant to the Los Angeles County Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $139,100/year in our lowest geographic market up to $240,500/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Company - Amazon Web Services, Inc.Job ID: A2717021","The ideal candidate should have strong communication skills and ability to prioritize effectively to ensure timelines are met; You should be a self-starter, comfortable with ambiguity, able to think big and be creative (while still paying careful attention to detail); You think in terms of architecture, not just code; 5+ years of data engineering experience; Experience with data modeling, warehousing and building ETL pipelines; Experience with SQL; Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS; Experience mentoring team members on best practices; Los Angeles County applicants: Job duties for this position include: work safely and cooperatively with other employees, supervisors, and staff; adhere to standards of excellence despite stressful conditions; communicate effectively and respectfully with employees, supervisors, and staff to ensure exceptional customer service; and follow all federal, state, and local laws and Company policies; These include the duties and responsibilities listed above, as well as the abilities to adhere to company policies, exercise sound judgment, effectively manage stress and work safely and respectfully with others, exhibit trustworthiness and professionalism, and safeguard business operations and the Company’s reputation; 7 more items(s)","Work/Life Balance; We value work-life harmony; The base pay for this position ranges from $139,100/year in our lowest geographic market up to $240,500/year in our highest geographic market; Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits; 1 more items(s)","AWS, SQL, Python, Java, Scala, NodeJS"
"Cloud data engineer at Empower Professionals Sunnyvale, CA",Moss Adams LLP,"Moss Adams LLP • Medford, OR •  via Adzuna",2 days ago,Full-time,"ure make this a reality. Join a values-driven firm where you'll have fun while solving complex and interesting business challenges.Introduction to the Team:The Senior Data Engineer on the Data Platform Team is primarily responsible for the development, testing and maintenance of the firm's analytical data assets to support DW/BI applications, workflows, reporting, integrations and other solutions. This role collaborates with data scientists, report writers, product group customers, stakeholders and other IT resources to define business needs and build scalable data processing pipelines. This individual is expected to provide solution design leadership, as well as mentor junior engineers and team members. Expertise in SQL and Python, solid experience and understanding on Snowflake data cloud and Azure data assets, and a strong understanding of data integration, data warehousing and dimensional modeling are required. Experience with SQL Server 2019 is desired to support migration from on-prem SQL Server to Snowflake. Strong analytical skills as well as good customer service is preferred.Individuals who thrive at Moss Adams exhibit the following success skills - Collaboration, Critical Thinking, Emotional Intelligence, Executive Presence, Growth Mindset, Intellectual Curiosity, and Results Focus.Responsibilities:- In collaboration with the data analytics & reporting team, manage and support Azure Machine Learning environment- Develop ETL using data factory, SSIS tools- Design, construct, and manage the data lake environment including: Data ingestion, staging, data quality monitoring, and business modeling- Develop, construct, test and maintain architectures (databases and large-scale processing systems)- Design, develop and implement technical solutions in SQL environment (databases, SQL, data marts, integrations)- Develop visual reports in Power BI- Maintain and develop queries and stored procedures using T-SQL- Support data integrations- Analyze data and identify gaps, data quality and integrity issues- Work in all aspects of development, design, reviews, troubleshooting and post-production issue resolution- Follow SDLC and create and maintain required documentation of newly developed and refactored applicationsQualifications:- Bachelor's degree or equivalent experience required; emphasis in Computer Science, Computer Engineering or related field preferred- Minimum of 3 years of related experience required; experience in a professional services environment preferred- Highly skilled in SQL programming in T-SQL building stored procedures and jobs- Strong functional understanding of Azure Machine Learning- Ability to manage large volume data integration/ETL; experience with SQL Server Integration Services (SSIS) including ability to configure SQL Server agent- Expertise in identifying and mapping data elements across multiple systems- Knowledge of Power BI report visualizations, Data Analysis Expressions (DAX), Power Query (M), and Power BI data modeling- Hands-on experience in writing DAX queries and custom calculations- Strong understanding of data integration, quality and validation- Strong understanding of MDM, Data Warehouse/Data mart and relational database design- Excellent written and verbal communication skills and able to interact with multiple IT teams and business partners both formally and informally- Should possess sound knowledge of SDLC using Azure DevOps- Self-motivated with the ability to work both independently and in a team environment- Effective time management, ability to multi-task with changing priorities while continuing to provide exceptional client service- Ability to travel as needed, approximately 5%Moss Adams is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation, gender identity or any other characteristic protected by law.Moss Adams complies with federal and state disability laws and makes reasonable accommodations for applicants and employees with disabilities. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact careers@mossadams.com.Some local/state regulations require employers to disclose the pay range in job postings. While this is the typical range of pay for the position, actual pay may vary based on internal equity, knowledge, experience, skillset, and geographic location among other factors. It's uncommon for an individual to be hired at the top end of the pay range. This position may be eligible for an annual discretionary bonus. For more information about our benefit offerings and other total rewards, visit our careers page.#LI-AC1Compensation Range (Denver Market ONLY): Compensation range for California: $106,000 -$152,000 Compensation range for Washington: $99,000 -$133,000Primary Location Tacoma, WAOther Locations Pasadena, CA, Woodland Hills, CA, Spokane, WA, San Diego, CA, Bellingham, WA, Orange County, CA, Eugene, OR, Walnut Creek, CA, El Segundo, CA, Wenatchee, WA, San Francisco, CA, Everett, WA, Medford, OR, Yakima, WA, Tri-Cities, WA, Stockton, CA, Seattle, WA, Santa Rosa, CA, Silicon Valley, CA, Sacramento, CA, Portland, OR, Fresno, CAEmployee Status: RegularSchedule: Full TimeReq ID: 27392","Expertise in SQL and Python, solid experience and understanding on Snowflake data cloud and Azure data assets, and a strong understanding of data integration, data warehousing and dimensional modeling are required; Individuals who thrive at Moss Adams exhibit the following success skills - Collaboration, Critical Thinking, Emotional Intelligence, Executive Presence, Growth Mindset, Intellectual Curiosity, and Results Focus; Highly skilled in SQL programming in T-SQL building stored procedures and jobs; Strong functional understanding of Azure Machine Learning; Ability to manage large volume data integration/ETL; experience with SQL Server Integration Services (SSIS) including ability to configure SQL Server agent; Expertise in identifying and mapping data elements across multiple systems; Knowledge of Power BI report visualizations, Data Analysis Expressions (DAX), Power Query (M), and Power BI data modeling; Hands-on experience in writing DAX queries and custom calculations; Strong understanding of data integration, quality and validation; Strong understanding of MDM, Data Warehouse/Data mart and relational database design; Excellent written and verbal communication skills and able to interact with multiple IT teams and business partners both formally and informally; Should possess sound knowledge of SDLC using Azure DevOps; Self-motivated with the ability to work both independently and in a team environment; Effective time management, ability to multi-task with changing priorities while continuing to provide exceptional client service; Ability to travel as needed, approximately 5%; 12 more items(s)","It's uncommon for an individual to be hired at the top end of the pay range; This position may be eligible for an annual discretionary bonus; For more information about our benefit offerings and other total rewards, visit our careers page; Compensation Range (Denver Market ONLY): Compensation range for California: $106,000 -$152,000 Compensation range for Washington: $99,000 -$133,000; 1 more items(s)","SQL, Python, Snowflake, Azure, SQL Server 2019, Azure Machine Learning, SSIS, Power BI, T-SQL, DAX, Power Query (M), Azure DevOps"
Software Data Engineer,Kern Energy CA,"Kern Energy CA • Bakersfield, CA •  via LinkedIn",2 days ago,Full-time,"ta infrastructure. Additionally, the engineer will optimize the information flow within the company to ensure data is accessible and reliable for analysis. The ideal candidate will embody Kern’s core values: Teamwork, Safety, Excellence, Integrity, and Connection.We are pleased to offer this job opportunity with flexible options, allowing you to choose between remote, hybrid, or onsite work arrangements.Job Function• Design and implement robust, scalable, and efficient data pipelines and architectures.• Work closely with IT Architect, project champions, and end users to understand data requirements and translate them into technical specifications.• Develop and maintain data integration processes to collect, process, and transfer data between systems.• Implement ETL (Extract, Transform, Load) processes for efficient data movement.• Manage and optimize databases for performance, reliability, and scalability.• Implement and maintain database schemas, indexes, and queries.• Implement data quality standards and governance processes to ensure the accuracy and consistency of data.• Monitor and troubleshoot data issues, implementing corrective actions as needed.• Implement and maintain security protocols for data storage, transmission, and access.• Ensure compliance with data protection and privacy regulations.• Collaborate with cross-functional teams including data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.• Identify and implement optimizations to improve the performance of data processing and analytics.• Document data architecture, processes, and configurations for future reference and knowledge sharing.Knowledge Skills• Strong proficiency in programming languages such as Python, R, DAX.• Expertise in working with databases (e.g., SQL, NoSQL) and data warehousing, data lakes, and/or data bricks, and/or Snowflake.• Experience with big data technologies like Hadoop, Spark, or Kafka.• Experience with data visualization tools (e.g., Tableau, Power BI).• Knowledge of cloud platforms such as Azure, AWS, or Google Cloud.• Knowledge of machine learning concepts.• Familiarity with data modeling and schema design.• Strong attention to detail and commitment to delivering high-quality solutions.• Demonstrated ability to manage time effectively and meet deadlines collaboratively in a team and independently.• Experience with unstructured data and correlating into relational data.• Manage projects with minimal supervision.• Excellent verbal, written, and social skills to communicate effectively with all levels of the organization.• Excellent problem-solving skills.• Experience with cloud-based presentation services and on-prem reporting tools.Education/ Experience• B.S. in Computer Science Engineering, Information Technology, or relevant degree• 2+ years of proven experience as a Data Engineer or in a similar role.• Background in data warehouse design and data mining, including an in-depth understanding of Relational database design, is preferred.Kern Energy is an equal-opportunity employer.All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws and will not be discriminated against on the basis of disability.","Strong proficiency in programming languages such as Python, R, DAX; Expertise in working with databases (e.g., SQL, NoSQL) and data warehousing, data lakes, and/or data bricks, and/or Snowflake; Experience with big data technologies like Hadoop, Spark, or Kafka; Experience with data visualization tools (e.g., Tableau, Power BI); Knowledge of cloud platforms such as Azure, AWS, or Google Cloud; Knowledge of machine learning concepts; Familiarity with data modeling and schema design; Strong attention to detail and commitment to delivering high-quality solutions; Demonstrated ability to manage time effectively and meet deadlines collaboratively in a team and independently; Experience with unstructured data and correlating into relational data; Manage projects with minimal supervision; Excellent verbal, written, and social skills to communicate effectively with all levels of the organization; Excellent problem-solving skills; Experience with cloud-based presentation services and on-prem reporting tools; B.S. in Computer Science Engineering, Information Technology, or relevant degree; 2+ years of proven experience as a Data Engineer or in a similar role; 13 more items(s)","Collaborating with the IT Architect, business analysts, and other stakeholders, the engineer will understand data requirements and implement solutions to support the organization's needs; This role is crucial for ensuring the availability, reliability, and performance of the data infrastructure; Additionally, the engineer will optimize the information flow within the company to ensure data is accessible and reliable for analysis; Design and implement robust, scalable, and efficient data pipelines and architectures; Work closely with IT Architect, project champions, and end users to understand data requirements and translate them into technical specifications; Develop and maintain data integration processes to collect, process, and transfer data between systems; Implement ETL (Extract, Transform, Load) processes for efficient data movement; Manage and optimize databases for performance, reliability, and scalability; Implement and maintain database schemas, indexes, and queries; Implement data quality standards and governance processes to ensure the accuracy and consistency of data; Monitor and troubleshoot data issues, implementing corrective actions as needed; Implement and maintain security protocols for data storage, transmission, and access; Ensure compliance with data protection and privacy regulations; Collaborate with cross-functional teams including data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions; Identify and implement optimizations to improve the performance of data processing and analytics; Document data architecture, processes, and configurations for future reference and knowledge sharing; 13 more items(s)","Python, R, DAX, SQL, NoSQL, Hadoop, Spark, Kafka, Tableau, Power BI, Azure, AWS, Google Cloud"
"Senior Data Engineer, PV Core Tech WW",AT&T,"AT&T • El Segundo, CA •  via JobzMall",108K–133K a year,Full-time,"ining the health of the pipelines ensuring data flow is efficient and managed. Candidate thrives in high-paced environments working across multiple projects and efforts. They will have a deep technical understanding, hands-on experience in distributed computing, big data, ETL, dimensional modeling , columnar databases and data visualization.AT&T is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.","They will have a deep technical understanding, hands-on experience in distributed computing, big data, ETL, dimensional modeling , columnar databases and data visualization","The candidate will demonstrate the ability to design, build, scale and maintain the data platform components for customers; Responsible for helping our customers integrate large amounts of complex data into their systems and maintaining the health of the pipelines ensuring data flow is efficient and managed; Candidate thrives in high-paced environments working across multiple projects and efforts","ETL, dimensional modeling, columnar databases, data visualization"
Lead data engineer,Amazon.com Services LLC,"Amazon.com Services LLC • Culver City, CA •  via Indeed",139K–240K a year,Full-time,"e you passionate about shaping the future of entertainment?Do you want to define the next generation of how and what Amazon customers are watching? Are you excited by the challenge of building, owning, and operating large-scale systems to solve complex problems in the digital media supply chain? If you want to work with a team of highly skilled software engineers, this Data Engineer position with Amazon Prime Video could be the perfect fit for you.Thrive in a Fast-Paced EnvironmentThis role is ideal for someone who enjoys owning massive data pipelines, building highly available stacks and services, and has strong problem-solving skills. You'll need a solid foundation in computer science, experience with backend design patterns and APIs, and a passion for operational excellence.Make an Impact on Millions of ViewersYou'll join a team that collaborates with products, marketing, finance, analytics, machine learning, and technology teams to deliver real-time data processing solutions. These solutions empower Amazon leadership, marketers, and product managers with timely, flexible, and structured access to customer insights. The team is responsible for building end-to-end pipelines and systems using the latest AWS technologies and software development principles. As a Data Engineer, you'll play a key role in shaping the future of Prime Video's measurement and attribution systems.Key job responsibilities• Lead the architecture, design, and development of Prime Video's data, metrics, and reporting platform.• Architect and implement new and automated Business Intelligence solutions, including big data and new analytical capabilities.• Partner with business leaders to drive strategy and prioritize projects and feature sets.• Write and review use cases and drive the development process from design to release.• Provide technical leadership and mentoring for a team of software engineers.• Own design and execution of end-to-end projects.• Establish key relationships with Amazon business units.• Implement data quality checks and monitoring procedures.• Automate data processing tasks.• Collaborate with cross-functional teams to understand data needs and design solutions.• Document data pipelines and processes.• Stay up-to-date with the latest trends and technologies in data engineering.• Experience with big data technologies such as: Hadoop, Hive, Spark, EMR• Experience operating large data warehouses• Bachelor's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent• Experience building data products incrementally and integrating and managing data sets from multiple sources• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets• Experience communicating with users, other technical teams, and management to collect requirements, describe data modeling decisions and data engineering strategyAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $139,100/year in our lowest geographic market up to $240,500/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.","7+ years of data engineering experience; Experience with data modeling, warehousing and building ETL pipelines; Experience with SQL; Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS; Experience mentoring team members on best practices; Experience with MPP databases such as Amazon Redshift; This role is ideal for someone who enjoys owning massive data pipelines, building highly available stacks and services, and has strong problem-solving skills; You'll need a solid foundation in computer science, experience with backend design patterns and APIs, and a passion for operational excellence; Experience with big data technologies such as: Hadoop, Hive, Spark, EMR; Experience operating large data warehouses; Bachelor's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent; Experience building data products incrementally and integrating and managing data sets from multiple sources; Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets; Experience communicating with users, other technical teams, and management to collect requirements, describe data modeling decisions and data engineering strategy; 11 more items(s)","The base pay for this position ranges from $139,100/year in our lowest geographic market up to $240,500/year in our highest geographic market; Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits","Hadoop, Hive, Spark, EMR, SQL, Python, Java, Scala, NodeJS, Amazon Redshift"
Data Engineer TECHM-JOB-28410,VirtualVocations,"VirtualVocations • Los Angeles, CA •  via Talent.com",5 days ago,Full-time,Last updated : 2024-09-03,Strong Python development and software design; Docker & Kubernetes; Object store (S3) & data lake concepts; Git; 1 more items(s),"They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products; The right candidate will be excited by the prospect of optimizing or even re-designing our data architecture to support our next generation of products and data initiatives; Strong python programming skills, expert level on using Python to process Big Data;; Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases; Extensive Experience on Databricks on Azure Cloud platform, deep understanding on Delta lake, Lake House Architecture; Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement; Strong analytic skills related to working with Data Visualization Dashboard, Metrics and etc, experience on Tableau, Power BI or Looker tools;; Build processes supporting data transformation, data structures, metadata, dependency and workload management; A successful history of manipulating, processing and extracting value from large disconnected datasets; Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores; Familiar with Deployment tool like Docker and building CI/CD pipelines; Experience supporting and working with cross-functional teams in a dynamic environment; 8+ years' experience in software development, Data engineering, and; Bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field; Python,; Advanced SQL and; Databricks; Must be strong and hands-on on Python and Databricks;; Required Skills : Python; Additional Skills : Python Developer; 17 more items(s)","Python, Docker, Kubernetes, S3, Git, SQL, Databricks, Azure Cloud, Delta lake, Tableau, Power BI, Looker"
"Data Engineer - (Python, Advanced SQL ,Databricks)Hybrid-Sunnyvale ,CA",Keylent Inc,"Keylent Inc • Sunnyvale, CA •  via LinkedIn",Full-time,No Degree Mentioned,as,Strong Python development and software design; Docker & Kubernetes; Object store (S3) & data lake concepts; Git; 1 more items(s),"They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products; The right candidate will be excited by the prospect of optimizing or even re-designing our data architecture to support our next generation of products and data initiatives; Strong python programming skills, expert level on using Python to process Big Data;; Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases; Extensive Experience on Databricks on Azure Cloud platform, deep understanding on Delta lake, Lake House Architecture; Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement; Strong analytic skills related to working with Data Visualization Dashboard, Metrics and etc, experience on Tableau, Power BI or Looker tools;; Build processes supporting data transformation, data structures, metadata, dependency and workload management; A successful history of manipulating, processing and extracting value from large disconnected datasets; Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores; Familiar with Deployment tool like Docker and building CI/CD pipelines; Experience supporting and working with cross-functional teams in a dynamic environment; 8+ years' experience in software development, Data engineering, and; Bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field; Python,; Advanced SQL and; Databricks; Must be strong and hands-on on Python and Databricks;; Required Skills : Python; Additional Skills : Python Developer; 17 more items(s)","Python, Docker, Kubernetes, S3, Git, SQL, Databricks, Azure Cloud, Delta lake, Lake House Architecture, Tableau, Power BI, Looker"
Senior Specialist - Data Engineering,General Motors,"General Motors • Carson City, NV •  via Ladders",7 days ago,Full-time,"The team is responsible for both advanced analytics strategy and applied, project-based solutions, focusing on the company’s most critical business areas.As a Data Engineer, you will build analytical data sets in support of Advanced Analytics projects. You will work closely with our forward-thinking Researchers and Data Scientists to deliver value to our vision for the future. Are you ready to join a future facing team?This role reports to the Product Owner of Marketing Activation Science, with leadership visibility to the greater Enterprise Data, Analytics, and Insights organization focused on democratizing data and decisions.What You'll DoCollaborate with internal and external stakeholders, including Activation Data Science to deliver on the portfolio.Drive the adoption of cloud-first technologies and industry-standard Data Engineering practices to accelerate and scale our engineering capabilities.Adhere to standards and processes to validate, monitor, and support data products that enable our decision science portfolio and data science capabilities.Collaborate with cross-functional teams, including data governance, security, data architecture, release management, Dev & ML Ops, and infrastructure, to ensure seamless integration and alignment of data engineering initiatives.Stay up to date with emerging trends and technologies in the field of Data Engineering, and proactively identify opportunities for improvement and innovation within our organization.Participate in a culture of continuous learning, knowledge sharing, and development within the team and the broader data engineering community.Additional Job DescriptionYour Skills & Abilities (Required Qualifications)7+ years of hands-on experience developing and implementing enterprise-scale data and analytics solutions using modern hybrid cloud technologies with a focus on data pipelines and reporting.Bachelor's degree (or equivalent work experience) in Computer Science, Data Science, Software Engineering, or a related field. Master's degree is a plus.Strong problem-solving and analytical skills with practical experience analyzing and curating large scale customer event data products from disparate data sources including 1st party and 3rd party data and map type data with variable structure.In-depth knowledge of industry-standard Data Engineering practices including data privacy & security, ETL/ELT, data architecture, data quality assurance, performance optimization, source code management, release management, and operations.Expert programming skills in data and analytics platforms, big data processing frameworks and languages, and development tools including Azure (or similar), Databricks, Spark, Python, SQL, and GitHub.Effective communication and people skills, with the ability to collaborate effectively with cross-functional technical teams and non-technical stakeholders.Naturally curious with the ability to work independently and proactively.Experience working in an Agile development environment.Experience deploying machine learning models and process automationThis role is based remotely but if you live within a 50-mile radius of Austin, TX, Roswell, GA, Mountain View, CA or Warren, MI, you are expected to report to that location three times a week, at minimum.This job is not eligible for relocation benefits. Any relocation costs would be the responsibility of the selected candidate.A company vehicle will be provided for this role with successful completion of a Motor Vehicle Report review.Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance.Benefits: GM offers a variety of health and wellbeing benefit programs. Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more.GM DOES NOT PROVIDE IMMIGRATION-RELATED SPONSORSHIP FOR THIS ROLE. DO NOT APPLY FOR THIS ROLE IF YOU WILL NEED GM IMMIGRATION SPONSORSHIP (e.g., H-1B, TN, STEM OPT, etc.) NOW OR IN THE FUTURE.#LI-CC1About GMOur vision is a world with Zero Crashes, Zero Emissions and Zero Congestion and we embrace the responsibility to lead the change that will make our world better, safer and more equitable for all.Why Join UsWe aspire to be the most inclusive company in the world. We believe we all must make a choice every day – individually and collectively – to drive meaningful change through our words, our deeds and our culture. Our Work Appropriately philosophy supports our foundation of inclusion and provides employees the flexibility to work where they can have the greatest impact on achieving our goals, dependent on role needs. Every day, we want every employee, no matter their background, ethnicity, preferences, or location, to feel they belong to one General Motors team.Benefits OverviewThe goal of the General Motors total rewards program is to support the health and well-being of you and your family. Our comprehensive compensation plan incudes, the following benefits, in addition to many others:• Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;• Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;• Company and matching contributions to 401K savings plan to help you save for retirement;• Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;• Tuition assistance and student loan refinancing;• Discount on GM vehicles for you, your family and friends.Diversity InformationGeneral Motors is committed to being a workplace that is not only free of discrimination, but one that genuinely fosters inclusion and belonging. We strongly believe that workforce diversity creates an environment in which our employees can thrive and develop better products for our customers. We understand and embrace the variety through which people gain experiences whether through professional, personal, educational, or volunteer opportunities. GM is proud to be an equal opportunity employer.We encourage interested candidates to review the key responsibilities and qualifications and apply for any positions that match your skills and capabilities.Equal Employment Opportunity StatementsGM is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. GM is committed to providing a work environment free from unlawful discrimination and advancing equal employment opportunities for all qualified individuals. As part of this commitment, all practices and decisions relating to terms and conditions of employment, including, but not limited to, recruiting, hiring, training, promotion, discipline, compensation, benefits, and termination of employment are made without regard to an individual's protected characteristics. For purposes of this policy, “protected characteristics"" include an individual's actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, weight, height, marital status, military service and veteran status, physical or mental disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state or local laws and ordinances. If you need a reasonable accommodation to assist with your job search or application for employment, email us at Careers.Accommodations@GM.com or call us at 800-865-7580. In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying.?We are leading the change to make our world better, safer and more equitable for all through our actions and how we behave. Learn more about:Our Company (https://search-careers.gm.com/en/working-at-gm/)Our CultureHow we hire??????? (https://search-careers.gm.com/en/how-we-hire/)Our diverse team of employees bring their collective passion for engineering, technology and design to deliver on our vision of a world with Zero Crashes, Zero Emissions and Zero Congestion. We are looking for adventure-seekers and imaginative thought leaders to help us transform mobility.Explore our global location sThe policy of General Motors is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's age, race, color, sex, religion, national origin, disability, sexual orientation, gender identity/expression or veteran status. Additionally, General Motors is committed to being an Equal Employment Opportunity Employer and offers opportunities to all job seekers including individuals with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, email us at Careers.Accommodations@GM.com .In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying.","7+ years of hands-on experience developing and implementing enterprise-scale data and analytics solutions using modern hybrid cloud technologies with a focus on data pipelines and reporting; Bachelor's degree (or equivalent work experience) in Computer Science, Data Science, Software Engineering, or a related field; Strong problem-solving and analytical skills with practical experience analyzing and curating large scale customer event data products from disparate data sources including 1st party and 3rd party data and map type data with variable structure; In-depth knowledge of industry-standard Data Engineering practices including data privacy & security, ETL/ELT, data architecture, data quality assurance, performance optimization, source code management, release management, and operations; Expert programming skills in data and analytics platforms, big data processing frameworks and languages, and development tools including Azure (or similar), Databricks, Spark, Python, SQL, and GitHub; Effective communication and people skills, with the ability to collaborate effectively with cross-functional technical teams and non-technical stakeholders; Naturally curious with the ability to work independently and proactively; Experience working in an Agile development environment; Experience deploying machine learning models and process automation; 6 more items(s)","This job is not eligible for relocation benefits; Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance; Benefits: GM offers a variety of health and wellbeing benefit programs; Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more; GM DOES NOT PROVIDE IMMIGRATION-RELATED SPONSORSHIP FOR THIS ROLE; The goal of the General Motors total rewards program is to support the health and well-being of you and your family; Our comprehensive compensation plan incudes, the following benefits, in addition to many others:; Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;; Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;; Company and matching contributions to 401K savings plan to help you save for retirement;; Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;; Tuition assistance and student loan refinancing;; Discount on GM vehicles for you, your family and friends; 10 more items(s)","Azure, Databricks, Spark, Python, SQL, GitHub"
Sr. Data Engineer (hybrid on-site in Los Angeles),LTIMindtree,"LTIMindtree • San Jose, CA •  via Indeed",4 days ago,110K–130K a year,"d transforming dataExposure to Cloud computing GCPAssist with production issues in Data Warehouses like reloading data transformations and translationsDevelop a Database Design and Reporting Design based on Business Intelligence and Reporting requirementsSkillsSnowflakeOther detailsBenefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):Benefits and Perks:• Comprehensive Medical Plan Covering Medical, Dental, Vision• Short Term and Long-Term Disability Coverage• 401(k) Plan with Company match• Life Insurance• Vacation Time, Sick Leave, Paid Holidays• Paid Paternity and Maternity LeaveThe range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.BenefitsCompensation range: $ 110,000.00 to 130,000.00 per year","Comprehensive Medical Plan Covering Medical, Dental, Vision; Short Term and Long-Term Disability Coverage; 401(k) Plan with Company match; Life Insurance; Vacation Time, Sick Leave, Paid Holidays; Paid Paternity and Maternity Leave; The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations; Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training; Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation; Compensation range: $ 110,000.00 to 130,000.00 per year; 7 more items(s)",Ability to Design Implement and optimize largescale data and analytics solutions on Snowflake Cloud Data Warehouse is essential; A Data Engineer at Snowflake is responsible for; Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL; Querying Snowflake using SQL; Development of scripts using Unix Python etc for loading extracting and transforming data; Exposure to Cloud computing GCP; Assist with production issues in Data Warehouses like reloading data transformations and translations; Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements; 5 more items(s),"Snowflake, Python, Snow SQL, Unix, GCP"
PLX Data Engineer,Jobot,"Jobot • Los Angeles, CA •  via LinkedIn",9 hours ago,110K–150K a year,"t About UsLeading provider of poultry and meat solutions throughout California, the Pacific Northwest, Arizona, Texas, Nevada and New Mexico. Our company was built with two principles in mind make a quality product and provide the best customer service possibleWhy join us?We grow our people to grow our business. We champion great people who bring ambition, curiosity, and high performance to the table as the guardians of our beloved and nostalgic brands. Good isn't good enough. We choose greatness every day by challenging the ordinary and making bold decisions. All while celebrating our wins - and our failures – as we work together to lead the future of food.Great Benefits And Compensation PackageGreat work culture and leadershipCompetitive health, dental, life, and 401-k benefits.Job DetailsJob DetailsOur company, a leading player in the manufacturing industry, is currently seeking an experienced and dynamic Data Engineer to join our team. The successful candidate will be responsible for developing, testing, and maintaining architectures such as databases and large-scale processing systems, ensuring our data remains secure, accessible, and optimized. This is a full-time, permanent position offering the opportunity to work with a dedicated team of professionals in a fast-paced, innovative environment.Key ResponsibilitiesData Infrastructure Development• Design, build, and maintain scalable and robust data pipelines to support data ingestion, processing, and storage.• Assist with the develop of ETL (Extract, Transform, Load) processes to ensure data is accurate, consistent, and available for analysis.• Collaborate with internal team members and analytics teams to integrate data from various sources, including databases, APIs, and third-party platforms.Data Quality and Governance• Implement data quality checks and validation processes to ensure data accuracy and completeness.• Develop and enforce data governance policies and best practices across the organization.• Maintain comprehensive documentation of data sources, structures, and processes.Collaboration and Support• Work closely with external data analysts, external data scientists, and internal business stakeholders to understand data needs and deliver actionable insights.• Provide technical support and guidance to team members and other departments as needed.• Stay updated with the latest industry trends and technologies, recommending and implementing improvements to our data infrastructure.Qualifications• Bachelor’s degree in Computer Science, Engineering, or a related field; Master’s degree preferred.• Proven experience as a Data Engineer or in a similar role, with a strong focus on data infrastructure and analytics.• Proficiency in SQL and experience with database management systems (e.g., PostgreSQL, MySQL, SQL Server).• Experience with data pipeline and ETL tools (e.g., Snowflake, Apache Nifi, Apache Airflow, Talend).• Familiarity with big data technologies (e.g., Hadoop, Spark) and cloud platforms (e.g., AWS, Azure, Google Cloud).• Strong programming skills in languages such as Python, Java, or Scala.• Excellent problem-solving skills and attention to detail.• Strong communication and collaboration abilities.Preferred Qualifications• Experience with data visualization tools (e.g., Tableau, Power BI).• Experience in the food industry or a similar sector.Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.Want to learn more about this role and Jobot?Click our Jobot logo and follow our LinkedIn page!","The successful candidate will be responsible for developing, testing, and maintaining architectures such as databases and large-scale processing systems, ensuring our data remains secure, accessible, and optimized; Proven experience as a Data Engineer or in a similar role, with a strong focus on data infrastructure and analytics; Proficiency in SQL and experience with database management systems (e.g., PostgreSQL, MySQL, SQL Server); Experience with data pipeline and ETL tools (e.g., Snowflake, Apache Nifi, Apache Airflow, Talend); Familiarity with big data technologies (e.g., Hadoop, Spark) and cloud platforms (e.g., AWS, Azure, Google Cloud); Strong programming skills in languages such as Python, Java, or Scala; Excellent problem-solving skills and attention to detail; Strong communication and collaboration abilities; 5 more items(s)","Salary $110,000 - $150,000 per year; Great Benefits And Compensation Package; Great work culture and leadership; Competitive health, dental, life, and 401-k benefits; 1 more items(s)","SQL, PostgreSQL, MySQL, SQL Server, Snowflake, Apache Nifi, Apache Airflow, Talend, Hadoop, Spark, AWS, Azure, Google Cloud, Python, Java, Scala, Tableau, Power BI"
Azure Data Engineer - West Region,"US Tech Solutions, Inc.","US Tech Solutions, Inc. • Mountain View, CA •  via ZipRecruiter",3 days ago,Full-time,"nalysis and profiling using relevant tools, leveraging the data infrastructure, and existing data models with minimal guidance.• Work with clients to identify their needs and clarify requirements and enable data-driven decision-making by collecting, transforming, and publishing data.• Follow and improve upon the technical best practices, including making data discoverable, thinking about the lifecycle of data, and managing master data well.Experience:• 3 years of experience in data engineering.• Experience with analyzing data, database query (e.g. SQL), and creating dashboards/reports.• Experience with design data pipelines and model data, for sync and async system integration, and implementation.Skills:• Ability to manage multiple projects with varying priorities.• Strong communication and presentation skills to deliver findings of analysis.• Organizational and project management skills.• Familiarity with Ads products.• Knowledge of Data Visualization and dashboard tool - PLX, Tableau, PowerBI, Looker.Education:• Bachelor's degree with quantitative focus or equivalent practical experience.About US Tech Solutions:US Tech Solutions is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. To know more about US Tech Solutions, please visit www.ustechsolutions.com.US Tech Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","3 years of experience in data engineering; Experience with analyzing data, database query (e.g. SQL), and creating dashboards/reports; Experience with design data pipelines and model data, for sync and async system integration, and implementation; Ability to manage multiple projects with varying priorities; Strong communication and presentation skills to deliver findings of analysis; Organizational and project management skills; Familiarity with Ads products; Knowledge of Data Visualization and dashboard tool - PLX, Tableau, PowerBI, Looker; Bachelor's degree with quantitative focus or equivalent practical experience; 6 more items(s)","The ideal candidate will work in developing and maintaining data models, pipelines, and SQL scripts to assist in the visualization, analysis, interpretation of data; Operate a robust, stable infrastructure, working closely with Engineering teams and partners to continuously drive process improvements across various cross functional partners; Perform data analysis and profiling using relevant tools, leveraging the data infrastructure, and existing data models with minimal guidance; Work with clients to identify their needs and clarify requirements and enable data-driven decision-making by collecting, transforming, and publishing data; Follow and improve upon the technical best practices, including making data discoverable, thinking about the lifecycle of data, and managing master data well; 2 more items(s)","SQL, PLX, Tableau, PowerBI, Looker"
Sr. Data Engineer/Developer,Avanade Inc.,"Avanade Inc. • Los Angeles, CA •  via Ladders",14 days ago,108K–184K a year,"look to us for innovation, which means you'll have early access to the newest Microsoft technologies so you can master them and stay ahead of the curve. Together we do what matters.Skills and experiencesSkills and experiences:• Demonstrable end-to-end experience in Data Engineering, including large-scale projects• Experience in working with the latest Azure technologies, such as Databricks, Synapse, Data Factory, Azure Data Lake Storage (Gen 2), Cosmos DB• Understanding of software engineering tools and concepts including experience in Python, Scala or PySpark• Confident communicator who can explain technical terms to non-technical audiences and mentor junior team members• Previous experience leading small development teams: track work, manage assignments, manage capacity, etc• Previous consulting experience is a huge plusAbout youAbout you:• Analytical, curious, agile• Team player and good communicator• Problem-solver, patient, quality-driven• Self-motivating• Innovative mindset• You might have a degree in Computer Science or another related fieldWhat you'll doWhat you'll do:• Architecture, design, development, and delivery of enterprise-grade analytics solutions based on Azure and Databricks technologies• Mentor more junior team members and supporting their personal development• Constantly develop technical skills in the latest Azure and Databricks technologies - achieving & maintaining relevant certifications• Work directly with high profile clients across a variety of sectors to understand their requirements and present solutions to customer sponsorsLearn moreLearn More:To learn more about the types of projects our Data Engineering team works on check out these case studies:• thyssenkrupp Materials Services uses data to help strike a delicate operational balance• What matters to SSE Renewables is innovating for a sustainable futureInterested in knowing what's going on inside Avanade? Check out our blogs:• Avanade Insights - exchange ideas that drive tomorrow's innovation• Inside Avanade - explore what life is like working at AvanadeCompensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in California, Colorado, New York, and Washington as set forth below.California: $108,000-$184,300Colorado: $93,600-$165,300Washington: $108,000-$184,300Enjoy your careerEnjoy your career:• Some of the best things about working at Avanade• Opportunity to work for Microsoft's Global Alliance Partner of the Year (14 years in a row), with exceptional development and training (minimum 80 hours per year for training and paid certifications)• Real-time access to technical and skilled resources globally• Dedicated career advisor to encourage your growth• Engaged and helpful coworkers genuinely interested in youFind out more about some of our benefits here .A great place to workAs you bring your skills and abilities to Avanade, you'll get distinctive experiences, limitless learning, and ambitious growth in return.As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You'll join a community of smart, supportive collaborators to lift, mentor, and guide you, and lean on your expertise. You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It's all here, so take a closer look! We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our [2] Inclusion & Diversity page. Create a future for our people that focuses on * Expanding your thinking * Experimenting courageously * Learning and pivoting Inspire greatness in our people by * Empowering every voice * Encouraging boldness * Celebrating progress Accelerate the impact of our people by * Amazing the client * Prioritizing what matters * Acting as one","Demonstrable end-to-end experience in Data Engineering, including large-scale projects; Experience in working with the latest Azure technologies, such as Databricks, Synapse, Data Factory, Azure Data Lake Storage (Gen 2), Cosmos DB; Understanding of software engineering tools and concepts including experience in Python, Scala or PySpark; Confident communicator who can explain technical terms to non-technical audiences and mentor junior team members; Previous experience leading small development teams: track work, manage assignments, manage capacity, etc; Previous consulting experience is a huge plus; Analytical, curious, agile; Team player and good communicator; Problem-solver, patient, quality-driven; You might have a degree in Computer Science or another related field; 7 more items(s)","Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience; Some of the best things about working at Avanade; Opportunity to work for Microsoft's Global Alliance Partner of the Year (14 years in a row), with exceptional development and training (minimum 80 hours per year for training and paid certifications); Real-time access to technical and skilled resources globally; Dedicated career advisor to encourage your growth; Engaged and helpful coworkers genuinely interested in you; 3 more items(s)","Databricks, Synapse, Data Factory, Azure Data Lake Storage (Gen 2), Cosmos DB, Python, Scala, PySpark"
Data Engineer (Informatica/SQL/Teradata),Intratek Computer Inc.,"Intratek Computer Inc. • Los Angeles, CA •  via LinkedIn",2 days ago,Contractor,"ware and software support, maintenance and repair, programming, professional staffing, networking, web design and development, and helpdesk implementation and management.Intratek Computer, Inc. is seeking a highly skilled and experienced Senior Data Engineer / Developer (see skills below) to support us in Los Angeles, CA 90012. The candidate should have demonstrated skills with WhereScape RED automation tools and the ability to design and implement fully operational solutions on Snowflake Data Warehouse. Additionally, the ideal candidate will have a strong background in delivering enterprise data warehouses, data lakes, with experience in designing and engineering end-to-end data analytics solutions.This is a remote position; however, they may have to travel on site or conferences on rare occasions.• Senior Data Engineer / Developer• 12 Months contract• Remote with occasional onsite• Los Angeles, CA 90012• Monday through Friday – day Shift• Pay rate depends on experience• Medical benefits• Paid vacation• Paid holidaysDuties and requirements:Required Skills• Proficiency in WhereScape RED for data warehouse automation, including designing, building, and managing data warehouses.• Expertise in Snowflake’s cloud data platform, including data loading, transformation, and querying using Snowflake SQL.• Experience with SQL-based development, optimization, and tuning for large-scale data processing.• Strong understanding of dimensional modeling concepts and experience in designing and implementing data models for analytics and reporting purposes.• Ability to optimize data pipelines and queries for performance and scalability.• Familiarity with Snowflake’s features such as virtual warehouses, data sharing, and data governance capabilities.• Knowledge of WhereScape scripting language (WSL) for customizing and extending automation processes.• Experience with data integration tools and techniques to ingest data from various sources into Snowflake.• Understanding of data governance principles and experience implementing data governance frameworks within Snowflake.• Ability to implement data quality checks and ensure data integrity within the data warehouse environment.• Strong SQL skills for data manipulation, optimization, and performance tuning.• Experience with data visualization tools such as Power BI.Equal Opportunity Employer:Intratek Computer Inc. is an equal opportunity employer. “All qualified applicants will receive consideration for employment without regard to their race, religion, ancestry, national origin, sex, sexual orientation, age, disability, marital status, domestic partner status, or medical condition.”Veterans Preference:Special preference will be given returning war veterans when hiring new employees in an attempt to recognize their service, sacrifice, and skillsApply NowTagged as: Sr. Data Engineer/Developer","Intratek Computer, Inc. is seeking a highly skilled and experienced Senior Data Engineer / Developer (see skills below) to support us in Los Angeles, CA 90012; The candidate should have demonstrated skills with WhereScape RED automation tools and the ability to design and implement fully operational solutions on Snowflake Data Warehouse; Additionally, the ideal candidate will have a strong background in delivering enterprise data warehouses, data lakes, with experience in designing and engineering end-to-end data analytics solutions; Proficiency in WhereScape RED for data warehouse automation, including designing, building, and managing data warehouses; Expertise in Snowflake’s cloud data platform, including data loading, transformation, and querying using Snowflake SQL; Experience with SQL-based development, optimization, and tuning for large-scale data processing; Strong understanding of dimensional modeling concepts and experience in designing and implementing data models for analytics and reporting purposes; Ability to optimize data pipelines and queries for performance and scalability; Familiarity with Snowflake’s features such as virtual warehouses, data sharing, and data governance capabilities; Knowledge of WhereScape scripting language (WSL) for customizing and extending automation processes; Experience with data integration tools and techniques to ingest data from various sources into Snowflake; Understanding of data governance principles and experience implementing data governance frameworks within Snowflake; Ability to implement data quality checks and ensure data integrity within the data warehouse environment; Strong SQL skills for data manipulation, optimization, and performance tuning; Experience with data visualization tools such as Power BI; 12 more items(s)",Medical benefits; Paid vacation; Paid holidays,"WhereScape RED, Snowflake, Snowflake SQL, Power BI"
Data & AI Engineering - Data Engineer,IDC Technologies,"IDC Technologies • Sunnyvale, CA •  via ZipRecruiter",7 days ago,Full-time,"gineers will write data collection scripts, aggregation algorithms, and implement storage technologies that power our internal business intelligence operations.Candidates are expected to be excellent engineers and will be interviewed by our existing engineering team. Please do not submit candidates who do not have strong knowledge of algorithms and data structures, and a track record of shipping high quality softwareQualificationsBachelorsAdditional InformationAll your information will be kept confidential according to EEO guidelines.","Please focus on spark/Pyspark with data experience; Must Have Skills : Python , Data Engineering; Nice to have : Docker , Kuebernetes; Looking for experienced Python engineers with a background in data engineering; Should be familiar with modern data platforms like Apache Presto or AWS Athena, and workflow tools like Apache Airflow; Please do not submit candidates who do not have strong knowledge of algorithms and data structures, and a track record of shipping high quality software; Bachelors; 4 more items(s)","University degree in Computer Related Sciences or similar; demonstrated ability as Data engineer in creating data pipelines for a cloud data warehouse; Strong python and SQL skills; Experience building data pipelines in python Experience with extracting data from Rest APIs and ingesting it to a cloud data warehouse; Experience working with S3 buckets and writing DAGs on Airfloow; Rigor in high code quality, automated testing, and other engineering standard processes; Effective communication, collaboration, and social skills; Result oriented approach; Good English (oral & written) and communication skills in general; Experience in AWS; 7 more items(s)","Python, Docker, Kubernetes, Apache Presto, AWS Athena, Apache Airflow, SQL, S3, AWS"
BI & Snowflake Data Engineer,SP Software Solutions,"SP Software Solutions • Mountain View, CA •  via ZipRecruiter",7 days ago,Contractor,"ions, and corrections to existing software.• Create technical documentation and procedures for installation and maintenance.• Write Unit Tests covering known use cases using appropriate tools• Integrate test frameworks in development process• Refactor existing solutions to make it reusable and scalable• Work with operations to get the solutions deployed• Take ownership of production deployment of code• Collaborating with and/or lead cross functional teams, build and launch applications and data platforms at scale, either for revenue generating or operational purposes• Come up with Coding and Design best practices• Thrive in self-motivated internal-innovation driven environment• Adapting fast to new application knowledge and changesQualificationsMaster/Bachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline; advanced degree preferred.Minimum of 2+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies.Minimum 1+ years of experience in Python, Pandas and SQLStrong knowledge of Database concepts and Python.Expert on SQL skills for data manipulation (DML) and validation (SQL Server, DB2, Oracle).Expertise in Object Oriented Programming Language - Java / Python.Experience using CI/CD Process, version control and bug tracking tools.Experience in handling very large data volume in low latency and batch mode.Experience with automation of job execution, validation and compares of the data files on Hadoop Env at the field level.Experience in leading a small team and be a team player.Strong communication skills with proven ability to present complex ideas and document in a clear and concise way.Quick learner; self-starter, detailed and in-depth.Additional InformationAll your information will be kept confidential according to EEO guidelines.","Minimum of 2+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies; Minimum 1+ years of experience in Python, Pandas and SQL; Strong knowledge of Database concepts and Python; Expert on SQL skills for data manipulation (DML) and validation (SQL Server, DB2, Oracle); Expertise in Object Oriented Programming Language - Java / Python; Experience using CI/CD Process, version control and bug tracking tools; Experience in handling very large data volume in low latency and batch mode; Experience with automation of job execution, validation and compares of the data files on Hadoop Env at the field level; Experience in leading a small team and be a team player; Strong communication skills with proven ability to present complex ideas and document in a clear and concise way; Quick learner; self-starter, detailed and in-depth; 8 more items(s)","Translate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data & Java etc; Work with business partners directly to seek clarity on requirements; Define solutions in terms of components, modules and algorithms; Design, develop, document and implement new programs and subprograms, as well as enhancements, modifications, and corrections to existing software; Create technical documentation and procedures for installation and maintenance; Write Unit Tests covering known use cases using appropriate tools; Integrate test frameworks in development process; Refactor existing solutions to make it reusable and scalable; Work with operations to get the solutions deployed; Take ownership of production deployment of code; Collaborating with and/or lead cross functional teams, build and launch applications and data platforms at scale, either for revenue generating or operational purposes; Come up with Coding and Design best practices; Thrive in self-motivated internal-innovation driven environment; Adapting fast to new application knowledge and changes; 11 more items(s)","Python, Pandas, SQL, SQL Server, DB2, Oracle, Java, Hadoop"
Data engineers - Graph DB,Infosys,"Infosys • Sunnyvale, CA •  via ZipRecruiter",4 days ago,Full-time,"nnovators, across the globe, is differentiated by the imagination, knowledge and experience, across industries and technologies, that we bring to every project we undertake.In the role of Technology Lead, you will interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation, Application Architecture definition and Design. You will play an important role in creating the high-level design artifacts. You will also deliver high quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.Required Qualifications• Candidate must be located within commuting distance of Sunnyvale, CA, or be willing to relocate to the area. This position may require travel in the US• Candidates authorized to work for any employer in the United States without employer-based visa sponsorship are welcome to apply. Infosys is unable to provide immigration sponsorship for this role at this time• Bachelor's degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education.• At least 4 years of experience with Information Technology.• At least 4 years of experience with BI and Data Warehousing• At least 2 years of experience with Snowflake database and ETL data pipelines• Proficient in writing complex SQLs, expertise in performance tuning of SQLs.• Strong skills in Python scripting and programming.Preferred Qualifications• Experience in designing end-to-end reporting and dashboarding applications.• Experience is working with client business users and other stakeholders to understand reporting / dashboarding requirements and then translating them to technical design and implement entire technical solution.• Hands on experience in database and table design.• Working experience with huge volumes of data would be an advantage.• Expected to lead offshore data engineers• Good communication skills. Should be able to communicate by telephone, email or face to face. Travel may be required as per the job requirements.The job entails sitting as well as working at a computer for extended periods of time.Estimated annual compensation range for candidates based in CA is as indicated below:California: $<90,751> to $<150,816>Along with competitive pay, as a full-time Infosys employee you are also eligible for the following benefits :-• Medical/Dental/Vision/Life Insurance• Long-term/Short-term Disability• Health and Dependent Care Reimbursement Accounts• Insurance (Accident, Critical Illness , Hospital Indemnity, Legal)• 401(k) plan and contributions dependent on salary level• Paid holidays plus Paid Time OffEEO/About UsAbout UsInfosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation. With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.Infosys provides equal employment opportunities to applicants and employees without regard to race; color; sex; gender identity; sexual orientation; religious practices and observances; national origin; pregnancy, childbirth, or related medical conditions; status as a protected veteran or spouse/family member of a protected veteran; or disability.","Candidate must be located within commuting distance of Sunnyvale, CA, or be willing to relocate to the area; This position may require travel in the US; Candidates authorized to work for any employer in the United States without employer-based visa sponsorship are welcome to apply; Bachelor's degree or foreign equivalent required from an accredited institution; Will also consider three years of progressive experience in the specialty in lieu of every year of education; At least 4 years of experience with Information Technology; At least 4 years of experience with BI and Data Warehousing; At least 2 years of experience with Snowflake database and ETL data pipelines; Proficient in writing complex SQLs, expertise in performance tuning of SQLs; Strong skills in Python scripting and programming; Should be able to communicate by telephone, email or face to face; Travel may be required as per the job requirements; 9 more items(s)","California: $<90,751> to $<150,816>; Along with competitive pay, as a full-time Infosys employee you are also eligible for the following benefits :-; Medical/Dental/Vision/Life Insurance; Long-term/Short-term Disability; Health and Dependent Care Reimbursement Accounts; Insurance (Accident, Critical Illness , Hospital Indemnity, Legal); 401(k) plan and contributions dependent on salary level; Paid holidays plus Paid Time Off; 5 more items(s)","Snowflake, Python, SQL"
Sr Azure Cloud Data Engineer,FlairTechSolutions,"FlairTechSolutions • Remote, OR •  via Indeed",4 days ago,50–55 an hour,Note: Mandatory to have 10+ yrs of experience. Don't share below experiencedresume.C2C-GCP Data Engineer-100% RemoteLinkedIn Created before 2014 and mandatory to share passport number for submission purpose.Job Type: ContractPay: $50.00 - $55.00 per hourExpected hours: 40 per weekSchedule:• Day shiftApplication Question(s):• LinkedIn IDWork Location: Remote,Note: Mandatory to have 10+ yrs of experience; LinkedIn Created before 2014 and mandatory to share passport number for submission purpose,Pay: $50.00 - $55.00 per hour; Expected hours: 40 per week,GCP
"Data Science Engineer permanent position at San Mateo, CA",Walmart,"Walmart • Sunnyvale, CA •  via Ladders",6 days ago,Full-time,"ertising data analytics related to demand, supply, and overall marketplace health. You will be responsible for extracting meaningful insights about campaign performance, marketplace efficiency, and gaps in systems and products, surfacing marketplace health metrics via dashboards, and working with cross-functional partners to move the needle in display advertising.What You Will DoBuilding data pipelines and dashboards to monitor display ads serving funnel and marketplace health.Leading analytics for display ads marketplace and presenting findings and recommendations to engineering teams and cross-functional partners.Collaborating with engineering and data science leads to improve data quality, building data warehouse, and delivering predictive models for marketplace insights and anomaly detection.Bringing data-driven culture to the engineering team to assure product and system quality.What You'll BringBachelor's degree in data science, computer science, statistics, operation research, or related fields.6 years' experience of building data pipelines, extracting signals from noisy data, and establishing metrics for monitoring.Proficiency in data analysis tools (e.g., Python, R, SQL) and data visualization tools (e.g., Tableau, Superset, Looker).Analytical thinking and detail-oriented mindset. Familiar with SQL and Non-SQL databases.Strong communication skills. Ability to convey complex ideas and findings to non-technical stakeholders.Knowledge of A/B testing.Preferred QualificationsAdvanced degree in data science, computer science, statistics, operation research, or related fields.6+ years' experience in programmatic advertising, online content distribution, or e-commerce.Knowledge of optimization, auction, and ML applications.People management experience is a big plus.About Walmart Global TechImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That is what we do at Walmart Global Tech. We are a team of software engineers, data scientists, cybersecurity experts, and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity and those looking for the opportunity to define their career. Here, you can kickstart a distinguished career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.Flexible, Hybrid WorkWe use a hybrid way of working that is primarily in the office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.BenefitsBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.Equal Opportunity EmployerWalmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity - unique styles, experiences, identities, ideas, and opinions - while being inclusive of all people.Minimum QualificationsOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.Option 1: Bachelor's degree in Computer Science and 5 years' experience in software engineering or related field.Option 2: 7 years' experience in software engineering or related field.Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.4 years' experience in data engineering, database engineering, business intelligence, or business analytics.Preferred QualificationsOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud.Master's degree in Computer Science or related field and 5 years' experience in software engineering or related field.We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture.Primary Location840 W CALIFORNIA AVE, SUNNYVALE, CA 94086-4828, United States of America#J-18808-Ljbffr","Proficiency in data analysis tools (e.g., Python, R, SQL) and data visualization tools (e.g., Tableau, Superset, Looker); Analytical thinking and detail-oriented mindset; Familiar with SQL and Non-SQL databases; Strong communication skills; Ability to convey complex ideas and findings to non-technical stakeholders; Knowledge of A/B testing; Knowledge of optimization, auction, and ML applications; People management experience is a big plus; That means understanding, respecting and valuing diversity - unique styles, experiences, identities, ideas, and opinions - while being inclusive of all people; Option 1: Bachelor's degree in Computer Science and 5 years' experience in software engineering or related field; Option 2: 7 years' experience in software engineering or related field; Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.4 years' experience in data engineering, database engineering, business intelligence, or business analytics; Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud; Master's degree in Computer Science or related field and 5 years' experience in software engineering or related field.We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly; The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture; 12 more items(s)","BenefitsBeyond our great compensation package, you can receive incentive awards for your performance; Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more","Python, R, SQL, Tableau, Superset, Looker"
Marketing Technology Data Engineer,MIT RESOURCE,"MIT RESOURCE • San Mateo, CA •  via ZipRecruiter",7 days ago,Full-time,"der groups to more accurately and efficiently determine member risk scores for appropriate value-based reimbursement.Fast growing and fast moving startup cultureValues great teamworkValues and celebrates great workValues fun alwaysValues learning and sharing of experiencesQualificationsRequired qualification:1--2 yrs of Scala2- 5 yrs of one of Hadoop, YARN, or Spark2-5 yrs of one of Java or PythonMechanical engineerAdditional InformationPlease help pass along to colleagues or associates below position who are looking for new role if you are not available.For further details contact me at chaitanya AT mitresource DOT com","1--2 yrs of Scala; 2- 5 yrs of one of Hadoop, YARN, or Spark; 2-5 yrs of one of Java or Python","5+ years working in a data engineering role supporting product, analytics and data science teams; 4+ years of Python (or similar experience) writing efficient, testable, and readable code; Experience building and optimizing real-time and batch processing solutions, ensuring high availability and low latency, allowing for timely insights and actions; Skilled in designing end-to-end data pipelines in cloud frameworks (GCP, AWS, Azure) with multi-stakeholder requirements; Familiarity with Google Cloud (GCP) tools (e.g., Cloud Run, Cloud Functions, Vertex AI, App Engine, Cloud Storage, IAM); Experience with CI/CD pipelines for data processing (Docker, CircleCI, dbt, git); Proficient in Infrastructure as Code (Terraform or Pulumi) and data orchestration tools (e.g., Apache Airflow; 4 more items(s)","Scala, Hadoop, YARN, Spark, Java, Python, GCP, AWS, Azure, Cloud Run, Cloud Functions, Vertex AI, App Engine, Cloud Storage, IAM, Docker, CircleCI, dbt, git, Terraform, Pulumi, Apache Airflow"
Data Visualization Engineer,Diverse Lynx,"Diverse Lynx • Sunnyvale, CA •  via LinkedIn",Contractor,No Degree Mentioned,"ent, and presentation skills• Create and maintain reports, create and manage data models, leverage data across complex hierarchies using multiple data sources.• Experience in building ETL pipelines.• Leverage process improvement techniques to drive improvements in data quality.• Perform testing to support system implementations and upgradesDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.",Ability to rapidly learn and adapt to business changes; Experience in building ETL pipelines,"Fluency in Advanced SQL (complex joins, stored procedures, subqueries, window functions, performance optimization, etc.), Snowflake, Python; Provide analytical reporting and analytics to the operations team and external partners; Ability to operate in a fast paced, rapidly changing environment; Excellent communication, project management, and presentation skills; Create and maintain reports, create and manage data models, leverage data across complex hierarchies using multiple data sources; Leverage process improvement techniques to drive improvements in data quality; Perform testing to support system implementations and upgrades; 4 more items(s)","Advanced SQL, Snowflake, Python"
Data Warehouse QA Engineer (Only Locals),Keylent,"Keylent • San Ramon, CA •  via ZipRecruiter",Full-time,No Degree Mentioned,ogic Apps• Implement row level security in Azure SQL DB and Azure Analysis Services• Implement CI/CD via Azure DevOps• Ingest data into a data lake from an Azure Service Bus• Create Databricks jobs to operationalize data flows• Schedule automatic refresh and scheduling refresh of AAS models• Write Ansible playbooks to deploy into Azure resources,Corp-Corp or 3rd Parties: Yes; Azure Data Engineer with 2+ years' experience in consulting on data factory pipeline and transformation services,"Data engineer Azure data flows for CPF Plant Turnaround Analytics using Azure services: Data Factory, Databricks, Functions, Logic Apps; Implement row level security in Azure SQL DB and Azure Analysis Services; Implement CI/CD via Azure DevOps; Ingest data into a data lake from an Azure Service Bus; Create Databricks jobs to operationalize data flows; Schedule automatic refresh and scheduling refresh of AAS models; Write Ansible playbooks to deploy into Azure resources; 4 more items(s)","Azure DevOps, Azure Service Bus, Databricks, Azure SQL DB, Azure Analysis Services, Ansible, Azure Data Factory, Functions, Logic Apps"
Data Engineer - Python,ApTask,"ApTask • Cupertino, CA •  via LinkedIn",135K–150K a year,Full-time,". Client is known for its commitment to innovation and invests in research and development to stay at the forefront of technological advancements.It offers a comprehensive set of services, including:IT Services: Application development, maintenance, and testing.Consulting: Business consulting, IT strategy, and digital transformation.Business Process Outsourcing (BPO): Outsourcing of business processes to improve efficiency.Enterprise Solutions: Implementation and support of enterprise-level software solutions. Digital Services: Services related to digital technologies, such as analytics, cloud, and IoT.Salary Range: $135K-$150K/AnnumJob Description:• 8+ years of relevant experience as a Data Engineer• Snowflake (mandatory and preferably certified) - Strong implementation experience with a good understanding of Snowflake Architecture being able to design and implement solutions and having good experience in Snowflake performance optimization techniques.• Advanced SQL (mandatory and preferably certified) - Good understanding of the concept of Slowly Changing Dimensions (SCD), be able to write complex queries using Self Joins, Cursors also recursive, Views/Materialized, strong in PL/SQL etc.• Strong experience in SQL Performance tuning especially when dealing with large datasets in millions of data.• Understanding of Data semantics and data semantic models• Python (mandatory) - Good experience in Python• Expert in Singlestore procedure.• Experience with Cloud Computing (AWS or Google Cloud) for deployment purposes is nice to have.About ApTask:ApTask is a leading global provider of workforce solutions and talent acquisition services, dedicated to shaping the future of work. As an African American-owned and Veteran-certified company, ApTask offers a comprehensive suite of services, including staffing and recruitment solutions, managed services, IT consulting, and project management. With a focus on excellence, collaboration, and innovation, ApTask provides unparalleled opportunities for professional growth and development. As a member of the ApTask team, you will have the chance to connect businesses with top-tier professionals, optimize workforce performance, and drive success across diverse industries. Join us at ApTask and be part of our mission to empower organizations to thrive while fostering a diverse and inclusive work environment.Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.Candidate Data Collection Disclaimer:At ApTask, we prioritize safeguarding your privacy. As part of our recruitment process, certain Personally Identifiable Information (PII) may be requested by our clients for verification and application purposes. Rest assured, we strictly adhere to confidentiality standards and comply with all relevant data protection laws. Please note that we only collect the necessary information as specified by each client and do not request sensitive details during the initial stages of recruitment.If you have any concerns or queries about your personal information, please feel free to contact our compliance team at businessexcellence@aptask.com","8+ years of relevant experience as a Data Engineer; Snowflake (mandatory and preferably certified) - Strong implementation experience with a good understanding of Snowflake Architecture being able to design and implement solutions and having good experience in Snowflake performance optimization techniques; Advanced SQL (mandatory and preferably certified) - Good understanding of the concept of Slowly Changing Dimensions (SCD), be able to write complex queries using Self Joins, Cursors also recursive, Views/Materialized, strong in PL/SQL etc; Strong experience in SQL Performance tuning especially when dealing with large datasets in millions of data; Python (mandatory) - Good experience in Python; Expert in Singlestore procedure; Experience with Cloud Computing (AWS or Google Cloud) for deployment purposes is nice to have; 4 more items(s)",Salary Range: $135K-$150K/Annum,"Snowflake, SQL, Python, Singlestore, AWS, Google Cloud"
Staff Data Scientist - Data Science Platform,Betteromics,"Betteromics • Redwood City, CA •  via ZipRecruiter",Full-time,Dental insurance,"tasets using the best-in-class ML and data science practices.Our Values• Innovate Together: We recognize that the best innovation comes from embracing the uniqueness of others, unlocking the potential of the collective• Seek Understanding: We empathetically seek to understand others with the goal of providing contextually grounded solutions• Embody Selflessness: We leverage different perspectives and achieve breakthrough by building values for others; lifting us all• Prioritize Integrity: We operate ethically, honestly and holding ourselves to high standards of care and transparencyWhat we look for in our ideal Betteromics team member• Bachelor's Degree in Computer Science or related field• Insatiable thirst / hunger for learning and problem solving• Empathy and humility to know that we don't know everything but want to help• Self driven generalists who can become experts and do what is needed when neededJob Requirements:• 8+ years experience in the full lifecycle of software products from requirements, design, development, testing, deployment and maintenance• Backend systems including realtime search (elasticsearch), database (postgresql), Datawarehouse (snowflake,hive...), Cloud storage (s3, ...)• General purpose data science and bioinformatics specific compute engines (cromwell, nextflow, redun, flyte ...)• High Dimensional data interpretation and visualization tools (mode, tableau, nivo, plotly, jmp, flowjo, R ...)• AI/ML Experience• Machine Learning tools such as Scikit-learn, TensorFlow, Keras, PyTorch, Langchain• Experience building ML models, fine tuning, evaluation, prompt engineering, RAG in areas such as nlp, generative ai , deep learning neural nets• Working with vector databases and model embeddings.• Experience working with models such as OpenAI GPT, Anthropic Claude, Gemini, Llama, Mixtral, ...• Domain experience in life sciences• NGS (next generation sequencing ) data formats (e.g. Bam, Fastq,) and Data pipelines (WGS, Broad GATK, nf-core ...)• Genomics, Proteomics, Metabolomics, Lipidomics, Transcriptomics, Clinical data experienceYour impact:• Design, develop, optimize, and productionize software solutions that deliver a delightful and stress-free experience for customers.• Get clarity of problem statements from designers, product managers, eng leads, break features into meaningful tasks, collaborate with other engineers and deliver the project with metrics and data.• Be a humble and trusted colleague for both our hardworking team members and passionate leaders alike. Deal with a difference in opinions in a mature and fair way.Benefits & Perks• Culture - our top priority! We pride ourselves on creating a culture that is inclusive and mission-driven• Generous Medical Plans, Vision, Dental, 401K, Paid Maternity and Paternity Leave• Flexible time off, alongside monthly paid company holidays• Hybrid-first environment that allows you to both interact with your teammates and work flexibly• Family- friendly. We are committed to helping employees take necessary time to take care of themselves and their family membersThe estimated base salary for this role ranges from $170,000 - $215,000 USD, plus a competitive equity package. Actual compensation is based on factors such as the candidate's skills, qualifications and experience.At Betteromics, we love that you are you! We design for equity rather than one-size fits all. All individuals seeking employment at Betteromics receive consideration for employment without regard to race, color, religion or belief, nationality, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status. As an equal opportunity employer, we stay true to our mission by ensuring that our place can be anyone's place","Bachelor's Degree in Computer Science or related field; Insatiable thirst / hunger for learning and problem solving; Empathy and humility to know that we don't know everything but want to help; Self driven generalists who can become experts and do what is needed when needed; 8+ years experience in the full lifecycle of software products from requirements, design, development, testing, deployment and maintenance; Backend systems including realtime search (elasticsearch), database (postgresql), Datawarehouse (snowflake,hive...), Cloud storage (s3, ...); General purpose data science and bioinformatics specific compute engines (cromwell, nextflow, redun, flyte ...); High Dimensional data interpretation and visualization tools (mode, tableau, nivo, plotly, jmp, flowjo, R ...); AI/ML Experience; Machine Learning tools such as Scikit-learn, TensorFlow, Keras, PyTorch, Langchain; Experience building ML models, fine tuning, evaluation, prompt engineering, RAG in areas such as nlp, generative ai , deep learning neural nets; Working with vector databases and model embeddings; Experience working with models such as OpenAI GPT, Anthropic Claude, Gemini, Llama, Mixtral, ..; Domain experience in life sciences; NGS (next generation sequencing ) data formats (e.g. Bam, Fastq,) and Data pipelines (WGS, Broad GATK, nf-core ...); Genomics, Proteomics, Metabolomics, Lipidomics, Transcriptomics, Clinical data experience; Deal with a difference in opinions in a mature and fair way; 14 more items(s)","Benefits & Perks; Culture - our top priority!; Generous Medical Plans, Vision, Dental, 401K, Paid Maternity and Paternity Leave; Flexible time off, alongside monthly paid company holidays; Hybrid-first environment that allows you to both interact with your teammates and work flexibly; Family- friendly; The estimated base salary for this role ranges from $170,000 - $215,000 USD, plus a competitive equity package; 4 more items(s)","Elasticsearch, PostgreSQL, Snowflake, Hive, S3, Cromwell, Nextflow, Redun, Flyte, Mode, Tableau, Nivo, Plotly, JMP, Flowjo, R, Scikit-learn, TensorFlow, Keras, PyTorch, Langchain, OpenAI GPT, Anthropic Claude, Gemini, Llama, Mixtral, Bam, Fastq, Broad GATK, nf-core"
"Senior Data Engineer (Ads, BizOps & LLMs)",Synaptein Solutions inc,"Synaptein Solutions inc • Thousand Oaks, CA •  via ZipRecruiter",3 days ago,Contractor,"s Python, Ruby, and JavaScript, and build tools such as Jenkins, Maven, Ant, Gradle or IvyQualificationsProven proficiency with scripting languages such as Python, Ruby, and JavaScript, and build tools such as Jenkins, Maven, Ant, Gradle or IvyAdditional InformationAll your information will be kept confidential according to EEO guidelines.","Proven proficiency with scripting languages such as Python, Ruby, and JavaScript, and build tools such as Jenkins, Maven, Ant, Gradle or Ivy; Proven proficiency with scripting languages such as Python, Ruby, and JavaScript, and build tools such as Jenkins, Maven, Ant, Gradle or Ivy","Has developed/owned/maintained ETL pipelines for internal business systems (for example: Rippling, HubSpot, Monday.com, Quickbooks/NetSuite, and similar); 4 - 7 years of experience in engineering / data engineering; Expert-level knowledge of Python & SQL; Has built batch and streaming data pipelines; Proven track record of leading data projects end to end with multiple systems and stakeholders involved; Bachelor's degree or higher from a highly selective university, with a strong preference for Computer Science, Physics, or Engineering; Early stage startup experience (has done the 0-1 journey at least once, ideally at a YC company); Experience with Ad Platform APIs (Meta, TikTok, Snap, Google) and related terminology (CAC, ROAS, CPC, CVR, Attribution, Incrementality, etc); Working knowledge of LLM / ML-ops tooling (LangChain, Vellum or similar) and architectures (prompt chaining, evaluators, version control, etc); Experience working directly with non-technical folks to develop and refine complex analytics/reports; 4 - 7 years of experience in engineering / data engineering; Can do transfers, but must currently have right to work in US; 9 more items(s)","Python, Ruby, JavaScript, Jenkins, Maven, Ant, Gradle, Ivy, Rippling, HubSpot, Monday.com, Quickbooks, NetSuite, SQL, LangChain, Vellum"
Lead Data Engineer with Snowflake and Python,Muon Space,"Muon Space • Mountain View, CA •  via ZipRecruiter",4 days ago,Full-time,"ng data and machine learning pipelines. This role will collaborate closely with data and software engineers and geospatial, scientific, and machine learning product developers to incorporate scalable cloud resources and modern software engineering practices into Muon's product development process in order to meet Muon's goals in monitoring and addressing climate change.This role may be remote or located at our headquarters in Mountain View, California.Responsibilities• Build and maintain Muon's data platform container orchestration infrastructure• Work with product developers to build and maintain data and machine learning pipelines• Support continuous improvement of software engineering processes for building, deploying, and monitoring data and machine learning pipelines• Provide expert support on data platform usage and best practices to product developers and scientists• Integrate data platform resources with Muon's wider cloud infrastructure• Maintain expert knowledge of data platform solutionsQualifications• 5+ years of software engineering experience• Experience building and maintaining container orchestration software platform• Experience using CI/CD processes for software and GitOps processes for infrastructure• Experience using modern time series DB and Grafana for application and infrastructure monitoring• Experience interacting with internal and external customers and service providersPreferred Qualifications• Experience building APIs for data access• Experience with common geospatial data formats (HDF5, NIST, COG, ...)• Experience building data indexing and data lineage solutions• Experience working with a distributed team of developers• Experience working with SaaS providers of core infrastructure services• Experience developing products from geospatial data (including non-imagery data types)• Experience developing retrieval algorithms for or products from satellite Earth observation dataSalaryThe salary range for this role is $95K- $195K and will depend on a candidate's skills, qualifications, and experience as defined during the interview process.About Muon SpaceFounded in 2021, Muon Space is an end-to-end Space Systems Provider that designs, builds, and operates LEO satellite constellations delivering mission-critical data. Our revolutionary, integrated technology stack enables customers to optimize every dimension of their missions for faster time-to-orbit and superior constellation remote sensing performance. Our state-of-the-art facility in the heart of Silicon Valley is optimized for manufacturing spacecraft and rapid, flexible payload integration at scale. From climate monitoring to national security, Muon Space is dedicated to delivering Earth Intelligence for a safer and more resilient world.Taking Care of Our TeamAt Muon salary is only part of our total compensation package. In addition to salary, we provide equity compensation as well as benefits including medical, dental, and vision insurance, a 401k retirement plan, short & long term disability and life insurance. We also offer three weeks paid vacation for new employees, along with 12 paid holidays, unlimited sick time and paid parental leave.Our mission embraces the entire planet and we believe our team should too. Muon is dedicated to creating a diverse and dynamic company and workforce. We believe in equal employment opportunity regardless of race, color, ancestry, religion, sex, age, national origin, citizenship, sexual orientation, marital status, disability, or gender identity. We value diversity in the workplace, and that starts with our applicants. We encourage you to apply, even if you don't check all the boxes, and we look forward to reviewing your application! In addition, if you need a reasonable accommodation as part of your application for employment or interviews with us, please let us know.ITAR/EAR RequirementsThis position requires access to export controlled information. To conform to U.S. Government export regulations applicable to that information, applicant must either be (A) a U.S. person, defined as a (i) U.S. citizen or national, (ii) U.S. lawful, permanent resident (green card holder), (iii) refugee under 8 U.S.C. § 1157, or (iv) asylee under 8 U.S.C. § 1158, (B) eligible to access the export controlled information without a required export authorization, or (C) eligible and reasonably likely to obtain the required export authorization from the applicable U.S. government agency. The Company may, for legitimate business reasons, decline to pursue any export licensing process.","5+ years of software engineering experience; Experience building and maintaining container orchestration software platform; Experience using CI/CD processes for software and GitOps processes for infrastructure; Experience using modern time series DB and Grafana for application and infrastructure monitoring; Experience interacting with internal and external customers and service providers; This position requires access to export controlled information; To conform to U.S. Government export regulations applicable to that information, applicant must either be (A) a U.S. person, defined as a (i) U.S. citizen or national, (ii) U.S. lawful, permanent resident (green card holder), (iii) refugee under 8 U.S.C. § 1157, or (iv) asylee under 8 U.S.C. § 1158, (B) eligible to access the export controlled information without a required export authorization, or (C) eligible and reasonably likely to obtain the required export authorization from the applicable U.S. government agency; 4 more items(s)","The salary range for this role is $95K- $195K and will depend on a candidate's skills, qualifications, and experience as defined during the interview process; At Muon salary is only part of our total compensation package; In addition to salary, we provide equity compensation as well as benefits including medical, dental, and vision insurance, a 401k retirement plan, short & long term disability and life insurance; We also offer three weeks paid vacation for new employees, along with 12 paid holidays, unlimited sick time and paid parental leave; 1 more items(s)","CI/CD, GitOps, time series DB, Grafana"
Data Engineer with Java & Scala,Diverse Lynx,"Diverse Lynx • Sunnyvale, CA •  via LinkedIn",Contractor,No Degree Mentioned,"llowing qualifications:"" Minimum of 10+ year of experience designing and implementing a full-scale data warehouse solution based on Snowflake."" A minimum of three years experience in developing production-ready data ingestion and processing pipelines using Spark, Python."" Expertise and excellent proficiency with Snowflake internals and integration of Snowflake with other technologies for data processing and reporting."" A highly effective communicator, both orally and in writing"" Problem-solving and architecting skills in cases of unclear requirements.Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Experience: 10+ Year; Job Type: Full Time and Contract (W2 Only)- Visa sponsorship not workable; "" Good communication skill; "" Snowflake Data Engineers are required to have the following qualifications:; "" Minimum of 10+ year of experience designing and implementing a full-scale data warehouse solution based on Snowflake; "" A minimum of three years experience in developing production-ready data ingestion and processing pipelines using Spark, Python; "" Expertise and excellent proficiency with Snowflake internals and integration of Snowflake with other technologies for data processing and reporting; "" A highly effective communicator, both orally and in writing; "" Problem-solving and architecting skills in cases of unclear requirements; 6 more items(s)","8+ years exp in Java, Python and Scala; With Spark and Machine Learning (3+)","Snowflake, Spark, Python, Java, Scala"
"Data Engineer - SQL / Talend / GCP - ""Best Place to Work""",VirginPulse,"VirginPulse • Remote, OR •  via ZipRecruiter",15 hours ago,Full-time,"diverse inclusive community, where every voice matters. Together, we're shaping a healthier, more engaged future.ResponsibilitiesWho are you?Senior Data Engineers perform development activities within the data engineering team and provide mentorship, guidance, onboarding, and training of other Data Engineers. You will work closely with account management, ETL, data warehouse, business intelligence, and reporting teams as you develop data pipelines and enhancements and investigate and troubleshoot issues, create data models, set up and investigate new technologies, and research complex data. You have a deep understanding of data modeling, structures, and data pipelines.In this role you will wear many hats, but your knowledge will be essential in the following:• Perform selective data engineering updates and enable selective interoperability with the Cognizant QicLink claims platform• Examining, extracting, cleansing, and loading data.• Building data pipelines using SQL, Kafka, and other technologies.• Triage complex incoming bugs and incidents and resolving them.• Perform technical operation tasks.• Investigate and troubleshoot issues with data and data pipelines. Including interacting with US based data partners and vendors.• Participation in sprint refinement, planning, and kick-off to help estimate stories, raise awareness and additional implementation details.• Help monitor, maintain, and improve performance of the data pipeline.• Performing, maintaining, and implementing new quality assurance rules, methods, and tools to maintain consistent and accurate data.You'll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.QualificationsWhat you bring to the Personify Health team:In order to represent the best of what we have to offer you come to us with a multitude of positive attributes including:• 4+ years of experience in data engineering with US healthcare insurance claims data and familiarity with associated diagnostic and procedure codes.• Extensive experience with TPA (Third Party Administrator) business model including claims billing, processing, and reporting.• 3 to 4 years of experience managing data structures within the Cognizant QicLink claims platform to include reporting, modification, and maintenance.• SW certification or degree in IT related field• Deep understanding of modern relational and non-relational models and differences between them.• Expert in writing complex SQL, including pivots, window functions, and complex date calculations.• Able to lead and mentor more junior data engineers.• ETL experience preferred• Git experience preferredNo candidate will meet every single desired qualification. If your experience looks a little different from what we've identified and you think you can bring value to the role, we'd love to learn more about you!Personify Health is an equal opportunity organization and is committed to diversity, inclusion, equity, and social justice.In compliance with all states and cities that require transparency of pay, the base compensation for this position ranges from $100,000 to $124,000. Note that salary may vary based on location, skills, and experience. This position is eligible for 10% target bonus/variable compensation as well as health, dental, vision, mental health and other benefits.We strive to cultivate a work environment where differences are celebrated, and employees of all backgrounds are empowered to thrive. Personify Health is committed to driving Diversity, Equity, Inclusion and Belonging (DEIB) for all stakeholders: employees (at each organization level), members, clients and the communities in which we operate. Diversity is core to who we are and critical to our work in health and wellbeing.#WeAreHiring #PersonifyHealthBeware of Hiring Scams: Personify Health will never ask for payment or sensitive personal information such as social security numbers during the hiring process. All official communication will come from a verified company email address. If you receive suspicious requests or communications, please report them to talent@virginpulse.com. All of our legitimate openings can be found on the Personify Health Career Site.Employment Type: FULL_TIME","You have a deep understanding of data modeling, structures, and data pipelines; 4+ years of experience in data engineering with US healthcare insurance claims data and familiarity with associated diagnostic and procedure codes; Extensive experience with TPA (Third Party Administrator) business model including claims billing, processing, and reporting; 3 to 4 years of experience managing data structures within the Cognizant QicLink claims platform to include reporting, modification, and maintenance; SW certification or degree in IT related field; Deep understanding of modern relational and non-relational models and differences between them; Expert in writing complex SQL, including pivots, window functions, and complex date calculations; Able to lead and mentor more junior data engineers; 5 more items(s)","In compliance with all states and cities that require transparency of pay, the base compensation for this position ranges from $100,000 to $124,000; This position is eligible for 10% target bonus/variable compensation as well as health, dental, vision, mental health and other benefits","Cognizant QicLink, SQL, Kafka, Git"
Data Engineer (L5) - Content Machine Learning,RxCloud,"RxCloud • Remote, OR •  via ZipRecruiter",7 days ago,Full-time,"tation techniques• Knowledge of programming languages (e.g., Java and Python)• Hands-on experience with SQL database design• Great numerical and analytical skills• Degree in Computer Science, IT, or similar field; a masters is a plus.• Data engineering certification (e.g., IBM Certified Data Engineer) is a plus.• Big data technologies such as Hadoop, Spark, and Kafka.• Also be familiar with cloud platforms such as AWS, Google Cloud, and Azure.• Should be proficient in SQL, Excel, Tableau, or other BI tools. They should also have a good understanding of statistical analysis and modelling techniques, as well as business acumen.Responsibilities• Analyse and organize raw data.• Build data systems and pipelines.• Evaluate business needs and objectives.• Interpret trends and patterns.• Conduct complex data analysis and report on results.• Prepare data for prescriptive and predictive modelling.• Build algorithms and prototypes.• Combine raw information from different sources.• Explore ways to enhance data quality and reliability.• Identify opportunities for data acquisition.• Develop analytical tools and programs.• Collaborate with data scientists and architects on several projects.","Previous 7+ years of experience as a data engineer or in a similar role; Technical expertise with data models, data mining, and segmentation techniques; Knowledge of programming languages (e.g., Java and Python); Hands-on experience with SQL database design; Great numerical and analytical skills; Big data technologies such as Hadoop, Spark, and Kafka; Also be familiar with cloud platforms such as AWS, Google Cloud, and Azure; Should be proficient in SQL, Excel, Tableau, or other BI tools; They should also have a good understanding of statistical analysis and modelling techniques, as well as business acumen; 6 more items(s)","A Data Engineer is a data professional who uses their expertise in data engineering and programming to build systems that collect, manage, and convert raw data into usable information for business analysts; Analyse and organize raw data; Build data systems and pipelines; Evaluate business needs and objectives; Interpret trends and patterns; Conduct complex data analysis and report on results; Prepare data for prescriptive and predictive modelling; Build algorithms and prototypes; Combine raw information from different sources; Explore ways to enhance data quality and reliability; Identify opportunities for data acquisition; Develop analytical tools and programs; Collaborate with data scientists and architects on several projects; 10 more items(s)","Java, Python, SQL, Hadoop, Spark, Kafka, AWS, Google Cloud, Azure, Excel, Tableau"
"Senior Data Engineer-/LOS ANGELES, CA (Hybrid)- 12 months Contract",Varo Bank,"Varo Bank • Remote, OR •  via ZipRecruiter",12 days ago,140K–180K a year,"teams to design, build, and scale high leverage datasets that enable analytics, and experimentation. As a Senior Data Engineer you will contribute to the vision for data architecture and data quality across multiple company verticals. If you are interested in working with an impressive team of Data pros who collaborate and challenge each other, and want to solve interesting problems to propel the company's growth, apply now.What you'll be doing• Design, build and maintain Varo's data pipelines to process data into and out of Varo's Data Lake• Work with AWS big data technologies including EMR, Glue, S3, EKS, Lambda, Athena, RDS• Develop and maintain the data strategy for Varo in terms of capabilities and control mechanisms that support company responsibilities as a regulated national bank• Provide technical leadership in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing and analytics• Work with business partners on requirements, clarification and results• Participate in developing and enforcing data security & access control policies• Develop effective controls for a resilient data ingestion process• Support application data integration design and build efforts, including real-time capabilities• Conduct code reviews in accordance with team processes and standardsYou'll bring the following required skills and experiences• Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience• 6-8 years of data modeling, data pipelines, data lake, data warehouse experience• 6+ years programming experience in Python (will also consider Java, Kotlin, C, C++)• 6+ years' experience working within the AWS Big Data/Hadoop Ecosystem (EMR, Glue and Athena)• Experience with other AWS components like Cloudwatch, EKS, KMS, Lambdas, S3 is a strong plus• Experience with downstream consumption patterns (reports, dashboards) is a plus• Experience in Hadoop, HDFS, Hive, Presto, REST/SOAP API, Spark2, Airflow is a plus$140,000 - $180,000 a yearFor cash compensation, we set standard ranges for all US-based roles based on function, level, and geographic location, benchmarked against similar-stage growth companies. Per applicable law, the salary range for this role is $140,000 - $180,000. Final offer amounts are determined by multiple factors as well as candidate experience and expertise and may vary from the identified range.We recognize not everyone will have all of these requirements. If you meet most of the criteria above and you're excited about the opportunity and willing to learn, we'd love to hear from you!About VaroVaro launched in 2017 with the vision to bring the best of fintech into the regulated banking system. We're a new kind of bank - all-digital, mission-driven, FDIC-insured, and designed around the modern American consumer.As the first consumer fintech to be granted a national bank charter in 2020, we make financial inclusion and opportunity for all a reality by empowering everyone with the products, insights, and support they need to get ahead. Through our core product offerings and suite of customer-first features, we aim to address a broad range of consumer needs while profitably serving underserved communities that have been historically excluded from the traditional financial system.We are growing quickly in our hub locations of San Francisco, Salt Lake City, and Charlotte along with colleagues located across the country. We have been recognized among Fast Company's Most Innovative Companies, Forbes' Fintech 50, and earned the No. 7 spot on Inc. 5000's list of fastest-growing companies across the country.Varo. A bank for all of us.Our Core Values- Customers First- Take Ownership- Respect- Stay Curious- Make it BetterLearn more about Varo by following us:Facebook - https://www.facebook.com/varomoneyInstagram - www.instagram.com/varobankLinkedIn - https://www.linkedin.com/company/varobankTwitter - https://twitter.com/varobankEngineering Blog - https://medium.com/engineering-varoSoundCloud - https://soundcloud.com/varobankVaro is an equal opportunity employer. Varo embraces diversity and we are committed to building teams that represent a variety of backgrounds, perspectives, and skills. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.Beware of fraudulent job postings!Varo will never ask for payment to process documents, refer you to a third party to process applications or visas, or ask you to pay costs. Never send money to anyone suggesting they can provide work with Varo. If you suspect you have received a phony offer, please e-mail careers@varomoney.com with the pertinent information and contact information.CCPA Notice at Collection for California Employees and Applicants:https://varomoney.box.com/s/q7eockvma9nd2b0utwryruh4ze6gf8egApply for this job","Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience; 6-8 years of data modeling, data pipelines, data lake, data warehouse experience; 6+ years programming experience in Python (will also consider Java, Kotlin, C, C++); 6+ years' experience working within the AWS Big Data/Hadoop Ecosystem (EMR, Glue and Athena); Experience with other AWS components like Cloudwatch, EKS, KMS, Lambdas, S3 is a strong plus; 2 more items(s)","$140,000 - $180,000 a year; For cash compensation, we set standard ranges for all US-based roles based on function, level, and geographic location, benchmarked against similar-stage growth companies; Per applicable law, the salary range for this role is $140,000 - $180,000","Varo's Data Lake, AWS, EMR, Glue, S3, EKS, Lambda, Athena, RDS, Cloudwatch, KMS, Hadoop, HDFS, Hive, Presto, REST/SOAP API, Spark2, Airflow, Python, Java, Kotlin, C, C++"
"Lead Data Engineer – Hadoop Apps (Bangkok-Based, Relocation Provided)",CenCal Health,"CenCal Health • Santa Barbara, CA •  via ZipRecruiter",Full-time,Health insurance,"the delivery of analytics solutions and involves building and optimizing analytics data ecosystems from the ground up.The Sr. Data Engineer will enhance ETL processes through development, setting standards and oversight. Lead a team of analytics developers, data analysts, and data scientists on various data initiatives, the engineer will ensure optimal and consistent delivery of data architecture across all projects. This role demands a self-directed individual comfortable with Data Warehousing, ETL, and analytics technologies. The Sr. Data Engineer will contribute to the optimization or redesign of the company's analytics data management ecosystem, supporting current and future products and data initiatives. Often taking the lead on projects and development efforts, they will guide them through all stages of SDLC and review and recommend approval for code generated by other team members.Duties & Responsibilities• Creates and maintains efficient and effective data pipeline architecture and ETL.• Ability to gain/procure business requirements with all levels of stakeholders, and leverage expert knowledge to assemble large, complex data sets that serve as a foundation for analytics, reporting and insights.• Identify, design, and implement internal improvements: automating manual processes, optimizing data delivery, enhancing infrastructure for greater efficiency, extensibility, scalability, etc.• Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of clinical, operational and financial data sources.• Design and develop analytics tools to provide actionable insights into member outcomes, operational efficiency and other key performance indicators.• Cultivate strong relationships with stakeholders at all levels of the organization on data-related issues and needs. Key role in workgroups and committees as an expert analytics advisor.• Optimize data management to support descriptive and predictive analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.• Seen as a strong SME while advising data and analytics experts to unlock the potential of data managed by the Data Management team.• Adheres to department standards, practices and P&Ps• Other duties as assignedQualificationsKnowledge/Skills/Abilities• Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.• Working knowledge of health plan functions and related data required.• Strong analytic skills related to working with health plan data sources and creating health plan-oriented analytics data products..• Building processes supporting data transformation, quality, data structures, metadata, dependency and workload management.• Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.• Superior communication skills, both written and spoken.• Outstanding abilities to create, edit and publish documentation for technical products and business user consumption.• Strong project management and organizational skills.• Proficiency in SQL Server and/or Oracle products including (but not limited to): Oracle SQL*Plus, SQL, PL/SQL, Java, JavaScript, HTML, XML, Linux/UNIX shell scripting; knowledge of Oracle RDBMS database, .Net Core, C#, Internet, and Intranets; experience with personal computers (PCs), MS Windows operating system, and MS Office Suite.• Superior problem solving and troubleshooting capabilities• Excellent verbal and written communication skills• Ability to attend meetings and participate in small teams effectively• Must be able to budget time and meet deadlinesEducation & Experience• 6+ years experience in a Data Engineer or similar role• Health plan experience required.• Bachelors degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field, or equivalent work experience.• Experience with relational SQL and NoSQL databases.• 6+ years working in an Oracle environment and advanced knowledge of PL/SQL.• Must know basic Unix scripting and familiarity with cron jobs.• Experience building and optimizing 'big data' data pipelines, architectures and data sets, and ETL workflow management tools.• Experience with cloud service environments.• Experience with object-oriented/object function scripting languages.• Experience working with business users to understand their function, processes and goals and incorporate this knowledge into value-added data products.• Experience performing root cause analysis to identify opportunities for improvement.• A successful history of manipulating, processing and extracting value from large disconnected datasets.","Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases; Working knowledge of health plan functions and related data required; Strong analytic skills related to working with health plan data sources and creating health plan-oriented analytics data products.; Building processes supporting data transformation, quality, data structures, metadata, dependency and workload management; Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores; Superior communication skills, both written and spoken; Outstanding abilities to create, edit and publish documentation for technical products and business user consumption; Strong project management and organizational skills; Proficiency in SQL Server and/or Oracle products including (but not limited to): Oracle SQL*Plus, SQL, PL/SQL, Java, JavaScript, HTML, XML, Linux/UNIX shell scripting; knowledge of Oracle RDBMS database, .Net Core, C#, Internet, and Intranets; experience with personal computers (PCs), MS Windows operating system, and MS Office Suite; Superior problem solving and troubleshooting capabilities; Excellent verbal and written communication skills; Ability to attend meetings and participate in small teams effectively; Must be able to budget time and meet deadlines; 6+ years experience in a Data Engineer or similar role; Health plan experience required; Bachelors degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field, or equivalent work experience; Experience with relational SQL and NoSQL databases; 6+ years working in an Oracle environment and advanced knowledge of PL/SQL; Must know basic Unix scripting and familiarity with cron jobs; Experience building and optimizing 'big data' data pipelines, architectures and data sets, and ETL workflow management tools; Experience with cloud service environments; Experience with object-oriented/object function scripting languages; Experience working with business users to understand their function, processes and goals and incorporate this knowledge into value-added data products; Experience performing root cause analysis to identify opportunities for improvement; A successful history of manipulating, processing and extracting value from large disconnected datasets; 22 more items(s)","Salary Range: $101,007 - $151,510","SQL Server, Oracle SQL*Plus, SQL, PL/SQL, Java, JavaScript, HTML, XML, Linux/UNIX shell scripting, Oracle RDBMS database, .Net Core, C#, Internet, Intranets, MS Windows operating system, MS Office Suite, Unix scripting"
"Senior Engineering Manager, Core Data",Agoda,"Agoda • Los Angeles, CA •  via LinkedIn",18 days ago,Full-time,"hrough a culture of experimentation and ownership, enhancing the ability for our customers to experience the world.Our Purpose – Bridging the World Through TravelWe believe travel allows people to enjoy, learn and experience more of the amazing world we live in. It brings individuals and cultures closer together, fostering empathy, understanding and happiness.We are a skillful, driven and diverse team from across the globe, united by a passion to make an impact. Harnessing our innovative technologies and strong partnerships, we aim to make travel easy and rewarding for everyone.Get to Know Our TeamThe Data department oversees all of Agoda’s data-related requirements. Our ultimate goal is to enable and increase the use of data in the company through creative approaches and the implementation of powerful resources such as operational and analytical databases, queue systems, BI tools, and data science technology. We hire the brightest minds from around the world to take on this challenge and equip them with the knowledge and tools that contribute to their personal growth and success while supporting our company’s culture of diversity and experimentation. The role the Data team plays at Agoda is critical as business users, product managers, engineers, and many others rely on us to empower their decision making. We are equally dedicated to our customers by improving their search experience with faster results and protecting them from any fraudulent activities. Data is interesting only when you have enough of it, and we have plenty. This is what drives up the challenge as part of the Data department, but also the reward.Why Agoda Hadoop Apps Team?Joining this team, you will be solving some of the most difficult challenges out there today for the Hadoop ecosystem. We focus largely on building user tools and applications for other teams to use, most of these are built on top of Yarn utilizing Apache Spark as well as other cutting edge technologies.We are also the engine that drives a fully functional world class data warehouse on top of Hadoop, this means providing tools to define data cubes and syncing high load of data from a processing Hadoop to other systems using in-house built Applications.In this Role, you will get to• Lead the team technically in improving scalability, stability, accuracy, speed and efficiency of our existing Data Platform• Build, administer and scale Data Platform• Be comfortable navigating the following technology stack: S3, Kubernetes, KubeFlow, Spark, SQL, Impala, Scala, java, Python, scripting (Bash/Python) etc.• Work with experienced engineers to identify and build tools to automate many large-scale data platform managementWhat You’ll Need To Succeed• Bachelor’s degree in Computer Science /Information Systems/Engineering/related field• 6+ years of experience in Data Platform Administration eg. Hadoop, S3, AWS, Google Cloud etc.• 6+ years of experience in Kubernetes Administration in non cloud platform• Good experience in Apache Spark performance tuning and debugging• Good level understanding of JVM and either Java or Scala• Experience in WorkFlow Scheduler eg. KubeFlow, AirFlow, Oozie etc.• SQL experience eg. SparkSql, Impala, BigQuery, Presto/Trino, StarRocks etc.• Experience debugging and reasoning about production issues is desirable• Analytical problem-solving capabilities & experience.• Systems administration skills in LinuxIt’s great if you have• Experience working with Open-source products• Python/Shell scripting skills• Working in an agile environment using test driven methodologies#telaviv #jerusalem #IT #ENG #4 #sanfrancisco #sanjose #losangeles #sandiego #oakland #denver #miami #orlando #atlanta #chicago #boston #detroit #newyork #portland #philadelphia #dallas #houston #austin #seattle #sydney #melbourne #perth #toronto #vancouver #montreal #prague #Brno #Ostrava #cairo #alexandria #giza #estonia #paris #berlin #munich #hamburg #stuttgart #cologne #frankfurt #budapest #bali #dublin #telaviv #milan #rome #venice #florence #naples #turin #palermo #bologna #osaka #malta #amsterdam #oslo #warsaw #krakow #alrayyan #riyadh #jeddah #mecca #medina #singapore #seoul #barcelona #madrid #stockholm #zurich #taipei #tainan #taichung #kaohsiung #bangkok #Phuket #istanbul #london #manchester #edinburgh #hcmc #hanoi #lodz #wroclaw #poznan #katowice #rio #salvador #newdelhi #bangalore #bandung #yokohama #nagoya #okinawa #fukuoka #jerusalem #IT #4 #bangalore #delhi #hyderabad #pune #singapore #beijing #shanghai #shenzhen #tokyo #seoul #hongkong #taipei #kualalumpur #jakarta #hochiminh #hochiminhcity #manila #instanbul #makati #dubai #riyadh #gurgaon #noidaEqual Opportunity EmployerAt Agoda, we pride ourselves on being a company represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Agoda is based solely on a person’s merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics.We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details please read our privacy policy .To all recruitment agencies: Agoda does not accept third party resumes. Please do not send resumes to our jobs alias, Agoda employees or any other organization location. Agoda is not responsible for any fees related to unsolicited resumes.","Proven track record as a hands-on technical leader, preferably at a Staff+ level in a similar industry; 2+ years of experience managing teams of 10+ software engineers; Strong background in backend development; experience with web apps and mobile is advantageous; Ability to craft and execute strategic engineering roadmaps that align with business objectives; Highly structured and proactive communication capabilities; Experience in staffing, mentoring, and leading engineering teams to success; Experience working with product managers, EM peers, and senior ICs; Excellent written and verbal communication skills, effective cross-functional leadership; 5 more items(s)","As a Senior Engineering Manager for the Core Data Engineering team at Instacart, you will be leading the engineering team that is responsible for building and maintaining the most critical company-wide datasets; In this role, you will also be interfacing with many engineering, product, data science, and analytics leaders across the company to design and develop high quality, trustworthy data; Define, communicate, and drive the technical strategy for the team that supports the product strategy and key bets for the company, including new technical investments; Drive high-quality execution of key business outcomes of the team by overseeing the roadmap and enabling technical and product decisions; Create and ensure the adoption of best practices and engineering standards to build highly scalable and reliable engineering systems; Foster strong collaboration and working model with peers and different stakeholder teams across Instacart including Product, Engineering Data Science, and Finance; Mentor and support team members for personal and professional growth; Lead the recruitment and hiring processes to build a high-performing team; 5 more items(s)","Hadoop, Yarn, Apache Spark, S3, Kubernetes, KubeFlow, SQL, Impala, Scala, Java, Python, Bash, AWS, Google Cloud, AirFlow, Oozie, SparkSql, BigQuery, Presto/Trino, StarRocks, Linux"
Solutions Data Engineer,Futran Solutions,"Futran Solutions • Los Angeles, CA •  via LinkedIn",Full-time,4 days ago,"ing Google cloud technologies such as BigQuery, DataFlow, etc.Design, develop, and deploy machine learning modelsPerform data cleaning, feature engineering, and data transformation tasks to prepare data for analysis and modeling.Implement and maintain data engineering best practices, ensuring data quality, reliability, and performance.Conduct code reviews, provide constructive feedback, and mentor junior team members to enhance their technical skills.Stay up to date with the latest advancements in machine learning, data engineering, and related technologies, and evaluate their potential impact on our products and solutions.Skills/RequirementsBachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related field with 5+ years experienceSolid experience in developing and deploying machine learning models and data pipelines.Strong background in data engineering, including data preprocessing, feature engineering, and ETL pipelines.Proficiency In Programming Python (also Java Experience Preferred)Strong understanding of machine learning algorithms, statistical modeling, and data mining techniques.Experienced in Pandas, Numpy, Tensorflow/Keras, and other ML libraries in PythonFamiliarity with Apache Airflow workflow management toolKnowledge of Google cloud platform and services such as BigQuery, DataFlow,Familiarity with relational and NoSQL databases, SQL, and data warehousing concepts.Seeking a skilled and highly motivated Machine Learning/Data Engineer to join the data technology group. As a Senior Machine Learning/Data Engineer, you will play a critical role in designing, developing, and deploying scalable and robust machine learning models and data pipelines for large scale datasets. You will be responsible for leading a team of offshore consultants, providing technical guidance, and overseeing their deliverables","Bachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related field with 5+ years experience; Solid experience in developing and deploying machine learning models and data pipelines; Strong background in data engineering, including data preprocessing, feature engineering, and ETL pipelines; Strong understanding of machine learning algorithms, statistical modeling, and data mining techniques; Experienced in Pandas, Numpy, Tensorflow/Keras, and other ML libraries in Python; Familiarity with Apache Airflow workflow management tool; Knowledge of Google cloud platform and services such as BigQuery, DataFlow,; Familiarity with relational and NoSQL databases, SQL, and data warehousing concepts; Seeking a skilled and highly motivated Machine Learning/Data Engineer to join the data technology group; 6 more items(s)","Develop and implement robust, scalable, and efficient data pipelines for collecting, processing, and analyzing large volumes of structured and unstructured data using Google cloud technologies such as BigQuery, DataFlow, etc; Design, develop, and deploy machine learning models; Perform data cleaning, feature engineering, and data transformation tasks to prepare data for analysis and modeling; Implement and maintain data engineering best practices, ensuring data quality, reliability, and performance; Conduct code reviews, provide constructive feedback, and mentor junior team members to enhance their technical skills; Stay up to date with the latest advancements in machine learning, data engineering, and related technologies, and evaluate their potential impact on our products and solutions; As a Senior Machine Learning/Data Engineer, you will play a critical role in designing, developing, and deploying scalable and robust machine learning models and data pipelines for large scale datasets; You will be responsible for leading a team of offshore consultants, providing technical guidance, and overseeing their deliverables; 5 more items(s)","BigQuery, DataFlow, Python, Java, Pandas, Numpy, Tensorflow/Keras, Apache Airflow, SQL"
Data brick enginner,2100 NVIDIA USA,"2100 NVIDIA USA • Remote, OR •  via ZipRecruiter",3 days ago,Full-time,"on pipelines to ensure a continuous and reliable flow of AV data into the system.• Monitor data pipelines and services to ensure data availability and reliability.• Collaborate with multi-functional teams to improve data processing efficiency, reduce latency, and enhance overall system performance.• Implement validation and data quality checks to ensure the integrity and accuracy of ingested data.• Work closely with the AV development team to understand data requirements and contribute to the enhancement of data-driven solutions.What we need to see:• BS/MS/Phd in Computer Science, Engineering or other technical fields or equivalent experience• 5+ years of experience working in the industry• Proficiency in Golang and distributed systems.• Strong understanding of computer science principles and data science• You are extremely motivated, highly passionate, curious about and follow state-of-the-art technologies.• You take pride in your work and strive to achieve incredible results and possess excellent communication and planning skills.• Be willing to solicit and incorporate feedback and maintain a continuous improvement mindset.The base salary range is 148,000 USD - 339,250 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.You will also be eligible for equity and benefits. NVIDIA accepts applications on an ongoing basis.NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.","BS/MS/Phd in Computer Science, Engineering or other technical fields or equivalent experience; 5+ years of experience working in the industry; Proficiency in Golang and distributed systems; Strong understanding of computer science principles and data science; You are extremely motivated, highly passionate, curious about and follow state-of-the-art technologies; You take pride in your work and strive to achieve incredible results and possess excellent communication and planning skills; 3 more items(s)","Your base salary will be determined based on your location, experience, and the pay of employees in similar positions; You will also be eligible for equity and benefits",Golang
Lead -Data Engineer (Production Support),Diverse Lynx,"Diverse Lynx • San Jose, CA •  via LinkedIn",Contractor,No Degree Mentioned,"CSS, JavaScript, XML, and AngularJS• Strong Java coding skills.? Focus on Quality (Unit Tests, Integration tests, Code Coverage) - experience with tools for e2e testing• Experience with both relational, and NoSQL databases• Strong knowledge of infrastructure and application security• Experience with API and Web Services (REST/SOAP/Bulk)? Experience with a DevOps culture• Working knowledge of continuous integration systems? Strong RDMBS experience; JMS Knowledge• Hands-on experience and expertise in Advanced SQL, AWS and Other Data Engineering tools, SnapLogic and other open source tools.• Background in web services and systems integrations using HTTP and RESTful design principles• Solid understanding of software development models, web technologies, database concepts and modern service oriented architectures.? Understanding of version control systems like GitHub ? Understanding of build tools like Jenkins, Maven, and ANT• Preferred 2+ years? experience with the Agile methodology.? Preferred 2+ years?• experience with story/sprint tracking tools such as JIRA or equivalent? Preferred 1+ years• experience supporting marketing automation or lead generation? Excellent verbal and written communication skills. Able to clearly articulate complex problems and solutions in terms that others can understand? Strong understanding of integrations between hosted services and on-premise enterprise applications a plus.• strong understanding of API integration and lead management within Marketo? Experience in Databricks workspace, Databricks notebooks, Job cluster, Delta Lake, Databricks Lakehouse and Unity catalog? Experience on design and develop Data pipelines and ETL or ELT jobs to ingest and process data in data lake• Databricks certification is a plusDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","7+ years of experience in software development or IT; Proven experience using structured, disciplined approaches to solving technical, data, and logical problems at enterprise scale; Experience in designing, developing, testing, and deploying data integrations; Experience with client-side technologies using HTML, CSS, JavaScript, XML, and AngularJS; Strong Java coding skills.?; Focus on Quality (Unit Tests, Integration tests, Code Coverage) - experience with tools for e2e testing; Experience with both relational, and NoSQL databases; Strong knowledge of infrastructure and application security; Experience with API and Web Services (REST/SOAP/Bulk)?; Experience with a DevOps culture; Working knowledge of continuous integration systems?; Strong RDMBS experience; JMS Knowledge; Hands-on experience and expertise in Advanced SQL, AWS and Other Data Engineering tools, SnapLogic and other open source tools; Background in web services and systems integrations using HTTP and RESTful design principles; Solid understanding of software development models, web technologies, database concepts and modern service oriented architectures.?; Understanding of version control systems like GitHub ?; Understanding of build tools like Jenkins, Maven, and ANT; Excellent verbal and written communication skills; Able to clearly articulate complex problems and solutions in terms that others can understand?; strong understanding of API integration and lead management within Marketo?; Experience in Databricks workspace, Databricks notebooks, Job cluster, Delta Lake, Databricks Lakehouse and Unity catalog?; Experience on design and develop Data pipelines and ETL or ELT jobs to ingest and process data in data lake; 19 more items(s)","Strong in SQL skills with expertise in T-SQL / PLSQL, (complex queries, overall SQL knowledge),; Strong in SSRS and SSIS; Hands on experience required; Must have hands on Experience with any Visualization tools experience is needed to have (PowerBI or Tableau or ClickView); Strong working experience in Data warehouse L2, L3 type of production support engagements; Independently able to work with business users to understand changed required or understand issue or defect; 3 more items(s)","CSS, JavaScript, XML, AngularJS, Java, AWS, SnapLogic, GitHub, Jenkins, Maven, ANT, Marketo, Databricks, SQL, T-SQL, PLSQL, SSRS, SSIS, PowerBI, Tableau, ClickView"
Data Engineer Azure with Healthcare - W2,Databricks,"Databricks • Los Angeles, CA •  via LinkedIn",8 days ago,Full-time,"ience with Apache Spark™ and expertise in other data technologies. SSAs help customers through design and successful implementation of essential workloads while aligning their technical roadmap for expanding the usage of the Databricks Data Intelligence Platform. As a deep go-to-expert reporting to the Specialist Field Engineering Manager, you will continue to strengthen your technical skills through mentorship, learning, and internal training programs and establish yourself in an area of specialty - whether that be streaming, performance tuning, industry expertise, or more.The Impact You Will Have• Provide technical leadership to guide strategic customers to successful implementations on big data projects, ranging from architectural design to data engineering to model deployment• Architect production level data pipelines, including end-to-end pipeline load performance testing and optimization, as well as production level deployments and meeting necessary security and networking requirements• Become a technical expert in an area such as data lake technology, big data streaming, or big data ingestion and workflows, as well as cloud platforms, automation, security, networking, or identity management• Assist Solution Architects with more advanced aspects of the technical sale including custom proof of concept content, estimating workload sizing, and custom architectures• Provide tutorials and training to improve community adoption (including hackathons and conference presentations)• Contribute to the Databricks CommunityWhat We Look For• 7+ years experience in a technical role with expertise in the following:• Software Engineering/Data Engineering: data ingestion, streaming technologies - such as Spark Streaming and Kafka, performance tuning, troubleshooting, and debugging Spark or other big data solutions• Data Applications Engineering: Build use cases that use data - such as risk modeling, fraud detection, customer life-time value• Extensive experience building big data pipelines• Experience maintaining and extending production data systems to evolve with complex needs• Deep Specialty Expertise in the following areas:• Experience scaling big data workloads (such as ETL) that are performant and cost-effective• Experience migrating Hadoop workloads to the public cloud - AWS, Azure, or GCP• Experience with large scale data ingestion pipelines and data migrations - including CDC and streaming ingestion pipelines• Expert with cloud data lake technologies - such as Delta and Delta Live• Bachelor's degree in Computer Science, Information Systems, Engineering, or equivalent experience through work experience.• Production programming experience in SQL and Python, Scala, or Java.• 4 years professional experience with Big Data technologies (Ex: Spark, Hadoop, Kafka) and architectures• 4 years customer-facing experience in a pre-sales or post-sales role• Can meet expectations for technical training and role-specific outcomes within 6 months of hire• Can travel up to 30% when neededPay Range TransparencyDatabricks is committed to fair and equitable compensation practices. The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location. Based on the factors above, Databricks utilizes the full width of the range. The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above. For more information regarding which range your location is in visit our page here.Zone 1 Pay Range$169,000—$299,000 USDZone 2 Pay Range$169,000—$299,000 USDZone 3 Pay Range$169,000—$299,000 USDAbout DatabricksDatabricks is the data and AI company. More than 10,000 organizations worldwide — including Comcast, Condé Nast, Grammarly, and over 50% of the Fortune 500 — rely on the Databricks Data Intelligence Platform to unify and democratize data, analytics and AI. Databricks is headquartered in San Francisco, with offices around the globe and was founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow. To learn more, follow Databricks on Twitter, LinkedIn and Facebook.BenefitsAt Databricks, we strive to provide comprehensive benefits and perks that meet the needs of all of our employees. For specific details on the benefits offered in your region, please visit https://www.mybenefitsnow.com/databricks.Our Commitment to Diversity and InclusionAt Databricks, we are committed to fostering a diverse and inclusive culture where everyone can excel. We take great care to ensure that our hiring practices are inclusive and meet equal employment opportunity standards. Individuals looking for employment at Databricks are considered without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, veteran status, and other protected characteristics.ComplianceIf access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone.","7+ years experience in a technical role with expertise in the following:; Software Engineering/Data Engineering: data ingestion, streaming technologies - such as Spark Streaming and Kafka, performance tuning, troubleshooting, and debugging Spark or other big data solutions; Data Applications Engineering: Build use cases that use data - such as risk modeling, fraud detection, customer life-time value; Extensive experience building big data pipelines; Experience maintaining and extending production data systems to evolve with complex needs; Deep Specialty Expertise in the following areas:; Experience scaling big data workloads (such as ETL) that are performant and cost-effective; Experience migrating Hadoop workloads to the public cloud - AWS, Azure, or GCP; Experience with large scale data ingestion pipelines and data migrations - including CDC and streaming ingestion pipelines; Expert with cloud data lake technologies - such as Delta and Delta Live; Bachelor's degree in Computer Science, Information Systems, Engineering, or equivalent experience through work experience; Production programming experience in SQL and Python, Scala, or Java; 4 years professional experience with Big Data technologies (Ex: Spark, Hadoop, Kafka) and architectures; 4 years customer-facing experience in a pre-sales or post-sales role; Can meet expectations for technical training and role-specific outcomes within 6 months of hire; Can travel up to 30% when needed; If access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone; 14 more items(s)","Pay Range Transparency; Databricks is committed to fair and equitable compensation practices; The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles; Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location; The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above; $169,000—$299,000 USD; Zone 2 Pay Range; $169,000—$299,000 USD; $169,000—$299,000 USD; At Databricks, we strive to provide comprehensive benefits and perks that meet the needs of all of our employees; For specific details on the benefits offered in your region, please visit https://www.mybenefitsnow.com/databricks; 8 more items(s)","Apache Spark, Spark Streaming, Kafka, AWS, Azure, GCP, Delta, Delta Live, SQL, Python, Scala, Java, Hadoop"
